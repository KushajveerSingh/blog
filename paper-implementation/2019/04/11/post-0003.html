<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Weight Standardization: A new normalization in town | Kushajveer Singh</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Weight Standardization: A new normalization in town" />
<meta name="author" content="Kushajveer Singh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data." />
<meta property="og:description" content="Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data." />
<link rel="canonical" href="https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html" />
<meta property="og:url" content="https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html" />
<meta property="og:site_name" content="Kushajveer Singh" />
<meta property="og:image" content="https://kushajveersingh.github.io/blog/images/preview/post_003.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-11T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Kushajveer Singh"},"description":"Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data.","@type":"BlogPosting","headline":"Weight Standardization: A new normalization in town","url":"https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html","datePublished":"2019-04-11T00:00:00-05:00","dateModified":"2019-04-11T00:00:00-05:00","image":"https://kushajveersingh.github.io/blog/images/preview/post_003.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kushajveersingh.github.io/blog/feed.xml" title="Kushajveer Singh" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-123402359-3','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Weight Standardization: A new normalization in town | Kushajveer Singh</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Weight Standardization: A new normalization in town" />
<meta name="author" content="Kushajveer Singh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data." />
<meta property="og:description" content="Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data." />
<link rel="canonical" href="https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html" />
<meta property="og:url" content="https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html" />
<meta property="og:site_name" content="Kushajveer Singh" />
<meta property="og:image" content="https://kushajveersingh.github.io/blog/images/preview/post_003.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-11T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Kushajveer Singh"},"description":"Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data.","@type":"BlogPosting","headline":"Weight Standardization: A new normalization in town","url":"https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html","datePublished":"2019-04-11T00:00:00-05:00","dateModified":"2019-04-11T00:00:00-05:00","image":"https://kushajveersingh.github.io/blog/images/preview/post_003.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kushajveersingh.github.io/blog/feed.xml" title="Kushajveer Singh" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-123402359-3','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Kushajveer Singh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/notebooks/">Notebooks</a><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Weight Standardization: A new normalization in town</h1><p class="page-description">Weight Standardization to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU has 1-2 batches of data.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-04-11T00:00:00-05:00" itemprop="datePublished">
        Apr 11, 2019
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Kushajveer Singh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#paper-implementation">paper-implementation</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#What-is-wrong-with-BN-and-GB?">What is wrong with BN and GB? </a></li>
<li class="toc-entry toc-h2"><a href="#How-these-normalization-actually-help?">How these normalization actually help? </a></li>
<li class="toc-entry toc-h2"><a href="#Weight-Standardization">Weight Standardization </a></li>
<li class="toc-entry toc-h2"><a href="#Enough-talk,-let's-go-to-experiments">Enough talk, let&#39;s go to experiments </a>
<ul>
<li class="toc-entry toc-h3"><a href="#How-to-implement-WS?">How to implement WS? </a></li>
<li class="toc-entry toc-h3"><a href="#First,-let's-try-out-at-batch-size-=-64">First, let&#39;s try out at batch size = 64 </a></li>
<li class="toc-entry toc-h3"><a href="#Try-at-bs=2">Try at bs=2 </a></li>
<li class="toc-entry toc-h3"><a href="#Trying-bs=256">Trying bs=256 </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-04-11-post-0003.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Link to <a href="https://github.com/KushajveerSingh/deep_learning/blob/master/deep_learning/paper_implementations/Weight%20Standardization:%20A%20New%20Normalization%20in%20town/Weight%20Standardization%20on%20CIFAR-10.ipynb">jupyter notebook</a>, <a href="https://arxiv.org/abs/1903.10520">paper</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recently a new normalization technique is proposed not for the activations but for the weights themselves in the paper <a href="https://arxiv.org/abs/1903.10520">Weight Standardization</a>.</p>
<p>In short, to get new state of the art results, they combined Batch Normalization and Weight Standardization. So in this post, I discuss what is weight standardization and how it helps in the training process, and I will show my own experiments on CIFAR-10 which you can also follow along.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/01.jpeg" alt="" title="Figure 1. Taken from the paper. Shows a clear comparison of all normalizations."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For my experiments, I will use cyclic learning. As the paper discusses training with constant learning rates, I would use cyclic LR as presented by Leslie N. Smith in his report.</p>
<p>To make things cleaner I would use this notation:-</p>
<ul>
<li>BN -&gt; Batch Normalization</li>
<li>GN -&gt; Group Normalization</li>
<li>WS -&gt; Weight Standardization</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-wrong-with-BN-and-GB?">
<a class="anchor" href="#What-is-wrong-with-BN-and-GB?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is wrong with BN and GB?<a class="anchor-link" href="#What-is-wrong-with-BN-and-GB?"> </a>
</h2>
<p>Ideally, nothing is wrong with them. But to get the most benefit out of BN we have to use a large batch size. And when we have smaller batch sizes we prefer to use GN. (By smaller I mean 1–2 images/GPU).</p>
<p>Why is this so?</p>
<p>To understand it we have to see how BN works. To make things simple, consider we have only one-channel on which we want to apply BN and we have 2 images as our batch size.</p>
<p>Now we would compute the mean and variance using the 2 images and then normalize the one-channel of the 2 images. So we used 2 images to compute mean and variance. This is the problem.</p>
<p>By increasing batch size, we are able to sample the value of mean and variance from a larger population, which means that the computed mean and variance would be closer to their real values.</p>
<p>GN was introduced for cases of small batch sizes but it was not able to meet the results that BN was able to achieve using larger batch sizes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-these-normalization-actually-help?">
<a class="anchor" href="#How-these-normalization-actually-help?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How these normalization actually help?<a class="anchor-link" href="#How-these-normalization-actually-help?"> </a>
</h2>
<p>It is one of the leading areas of research. But it was recently shown in the paper <a href="https://arxiv.org/abs/1901.09321">Fixup Initialization: Residual Learning without Normalization</a> the reason for the performance gains using BN.</p>
<p>In short, it helps make the loss surface smooth.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/02.jpeg" alt="" title="Figure 2. When we train NNs we are in millions of dimensions. Here I show an example of loss varying with only one parameter."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we make the loss surface smooth we can take longer steps, which means we can increase our learning rate. So using Batch Norm actually stabilizes our training and also makes it faster.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Weight-Standardization">
<a class="anchor" href="#Weight-Standardization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Standardization<a class="anchor-link" href="#Weight-Standardization"> </a>
</h2>
<p>Unlike BN and GN that we apply on activations i.e the output of the conv layer, we apply Weight Standardization on the weights of the conv layer itself. So we are applying WS to the kernels that our conv layer uses.</p>
<p>How does this help?</p>
<p>For the theoretical justification see the <a href="https://arxiv.org/abs/1903.10520">original paper</a> where they prove WS reduces the Lipschitz constants of the loss and the gradients.</p>
<p>But there are easier ways to understand it.</p>
<p>First, consider the optimizer we use. The role of the optimizer is to optimize the weights of our model, but when we apply normalization layers like BN, we do not normalize our weights, but instead, we normalize the activations which are optimizer does not even care about.</p>
<p>By using WS we are essentially normalizing the gradients during the backpropagation.</p>
<p>The authors of the paper tested WS on various computer vision tasks and they were able to achieve better results with the combination of WS+GN and WS+BN. The tasks that they tested on included:</p>
<ol>
<li>Image Classification</li>
<li>Object Detection</li>
<li>Video Recognition</li>
<li>Semantic Segmentation</li>
<li>Point Cloud Classification</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Enough-talk,-let's-go-to-experiments">
<a class="anchor" href="#Enough-talk,-let's-go-to-experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Enough talk, let's go to experiments<a class="anchor-link" href="#Enough-talk,-let's-go-to-experiments"> </a>
</h2>
<p>The code is available in the <a href="https://github.com/KushajveerSingh/deep_learning/blob/master/deep_learning/paper_implementations/Weight%20Standardization:%20A%20New%20Normalization%20in%20town/Weight%20Standardization%20on%20CIFAR-10.ipynb">notebook</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-to-implement-WS?">
<a class="anchor" href="#How-to-implement-WS?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to implement WS?<a class="anchor-link" href="#How-to-implement-WS?"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_chan</span><span class="p">,</span> <span class="n">out_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_chan</span><span class="p">,</span> <span class="n">out_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> 
                         <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">weight_mean</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                  <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">weight_mean</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-5</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">/</span> <span class="n">std</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="First,-let's-try-out-at-batch-size-=-64">
<a class="anchor" href="#First,-let's-try-out-at-batch-size-=-64" aria-hidden="true"><span class="octicon octicon-link"></span></a>First, let's try out at batch size = 64<a class="anchor-link" href="#First,-let's-try-out-at-batch-size-=-64"> </a>
</h3>
<p>This will provide a baseline of what we should expect. For this, I create 2 resnet18 models:</p>
<ol>
<li>resnet18 $\rightarrow$ It uses the nn.Conv2d layers</li>
<li>resnet18_ws $\rightarrow$ It uses above Conv2d layer which uses weight standardization</li>
</ol>
<p>I change the head of resnet model, as CIFAR images are already 32 in size and I don’t want to half their size initially. The code can be found in the notebook. And for the CIFAR dataset, I use the official train and valid split.</p>
<p>First I plot the value of loss v/s learning rate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/03.jpeg" alt="" title="Figure 3. Learning rate finder result with bs=64."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For those not familiar with loss v/s learning_rate graph. We are looking for the maximum value of lr at which the loss value starts increasing.</p>
<p>In this case the max_lr is around 0.0005. So let’s try to train model for some steps and see. In case you wonder in the second case the graph is flatter around 1e-2, it is because the scale of the two graphs is different.</p>
<p>So now let’s train our model and see what happens. I am using the fit_one_cycle to train my model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/04.jpeg" alt="" title="Figure 4. Loss values with bs=64"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is not much difference between the two as valid loss almost remains the same.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Try-at-bs=2">
<a class="anchor" href="#Try-at-bs=2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Try at bs=2<a class="anchor-link" href="#Try-at-bs=2"> </a>
</h3>
<p>Now I take a batch size of 2 and train the models in a similar manner.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/05.jpeg" alt="" title="Figure 5. Learning rate finder result with bs=2"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One thing that I should add here, is the loss diverged quickly when I used only BN, after around 40 iterations, while in the case of WS+BN the loss did not diverge.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/06.jpeg" alt="" title="Figure 6. Loss values with bs=2"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is not much difference in the loss values, but the time to run each cycle increased very much.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Trying-bs=256">
<a class="anchor" href="#Trying-bs=256" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trying bs=256<a class="anchor-link" href="#Trying-bs=256"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, I run some more experiments where I used a batch size of 256. Although, I could use a larger learning rate but the time taken to complete the cycle increased. The results are shown below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/07.jpeg" alt="" title="Figure 7. Learning rate finder result with bs=256"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_003/08.jpeg" alt="" title="Figure 8. Loss values with bs=256"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, in the graph we see we can use a larger learning rate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>From the above experiments, I think I would prefer not to use Weight Standardization when I am using cyclic learning. For large batch sizes, it even gave worse performance and for smaller batch sizes, it gave almost similar results, but using weight standardization we added a lot of time to our computation, which we could have used to train our model with Batch Norm alone.</p>
<p>For constant learning rate, I think weight standardization still makes sense as there we do not change our learning rate in the training process, so we must benefit from the smoother loss function. But in the case of cyclic learning, it does not offer us a benefit.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="KushajveerSingh/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/paper-implementation/2019/04/11/post-0003.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Deep Learning Researcher with interest in Computer Vision, Natural Language Processing and Reinforcement Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/KushajveerSingh" title="KushajveerSingh"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Kkushaj" title="Kkushaj"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
