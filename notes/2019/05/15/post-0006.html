<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to become an expert in NLP in 2019 | Kushajveer Singh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="How to become an expert in NLP in 2019" />
<meta name="author" content="Kushajveer Singh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Complete list of resources that will provide you with the all the theoretical background of the latest NLP research and techniques." />
<meta property="og:description" content="Complete list of resources that will provide you with the all the theoretical background of the latest NLP research and techniques." />
<link rel="canonical" href="https://kushajveersingh.github.io/blog/notes/2019/05/15/post-0006.html" />
<meta property="og:url" content="https://kushajveersingh.github.io/blog/notes/2019/05/15/post-0006.html" />
<meta property="og:site_name" content="Kushajveer Singh" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-15T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://kushajveersingh.github.io/blog/notes/2019/05/15/post-0006.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kushajveersingh.github.io/blog/notes/2019/05/15/post-0006.html"},"headline":"How to become an expert in NLP in 2019","dateModified":"2019-05-15T00:00:00-05:00","datePublished":"2019-05-15T00:00:00-05:00","author":{"@type":"Person","name":"Kushajveer Singh"},"description":"Complete list of resources that will provide you with the all the theoretical background of the latest NLP research and techniques.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kushajveersingh.github.io/blog/feed.xml" title="Kushajveer Singh" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-123402359-3','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Kushajveer Singh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/graphml/">Graph Machine Learning</a><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to become an expert in NLP in 2019</h1><p class="page-description">Complete list of resources that will provide you with the all the theoretical background of the latest NLP research and techniques.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-05-15T00:00:00-05:00" itemprop="datePublished">
        May 15, 2019
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Kushajveer Singh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#notes">notes</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-05-15-post-0006.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post, I would focus on all of the theoretical knowledge you need for the latest trends in NLP. I made this reading list as I learned new concepts. For the resources, I include papers, blogs, videos.</p>
<p>It is not necessary to read most of the stuff. Your main goal should be to understand that in this paper this thing was introduced and do I understand how it works, how it compares it with state of the art.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_006/01.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Trend</strong>: Use bigger transformer based models and solve multi-task learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-error">
    <svg class="octicon octicon-alert octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg>
    <strong>Warning: </strong>Warning: It is an increasing trend in NLP that if you have a new idea in NLP during reading any of the papers, you will have to use massive compute power to get any reasonable results. So you are limited by the open-source models.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>
<p><strong>fastai</strong>:- I had already watched the videos, so I thought I should add it to the top of the list.</p>
<ul>
<li>
<a href="https://course.fast.ai/videos/?lesson=4">Lesson 4</a> Practical Deep Learning for Coders. It will get you up with how to implement a language model in fastai.</li>
<li>
<a href="https://course.fast.ai/videos/?lesson=12">Lesson 12</a> Deep Learning from the Foundations. Goes further into ULMFit training.</li>
</ul>
</li>
<li>
<p><strong>LSTM</strong>:- Although transformers are mainly used nowadays, in some cases you can still use <em>LSTM</em> and it was the first successful model to get good results. You should use <em>AWD_LSTM</em> now if you want.</p>
<ul>
<li>
<em>Long Short-Term Memory</em> <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">paper</a>. A quick skim of the paper is sufficient.</li>
<li>
<em>Understanding LSTM Networks</em> <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">blog</a>. It explains all the details of the LSTM network graphically.</li>
</ul>
</li>
<li>
<p><strong>AWD_LSTM</strong>:- It was proposed to overcome the shortcoming of <em>LSTM</em> by introducing dropout between hidden layers, embedding dropout, weight tying. You should use <em>AWS_LSTM</em> instead of LSTM.</p>
<ul>
<li>
<em>Regularizing and Optimizing LSTM Language Models</em> <a href="https://arxiv.org/abs/1708.02182">paper</a>. AWD_LSTM paper</li>
<li>Official <a href="https://github.com/salesforce/awd-lstm-lm">code</a> by Salesforce</li>
<li>fastai <a href="https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py">implementation</a>
</li>
</ul>
</li>
<li>
<p><strong>Pointer Models</strong>:- Although not necessary, it is a good read. You can think of it as pre-attention theory.</p>
<ul>
<li>
<em>Pointer Sentinel Mixture Models</em> <a href="https://arxiv.org/abs/1609.07843">paper</a>
</li>
<li>Official <a href="https://www.youtube.com/watch?v=Ibt8ZpbX3D8">video</a> of above paper.</li>
<li>
<em>Improving Neural Language Models with a continuous cache</em> <a href="https://openreview.net/pdf?id=B184E5qee">paper</a>
</li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>What is the difference between weight decay and regularization? In weight decay, you directly add something to the update rule while in regularization it is added to the loss function. Why bring this up? Most probably the DL libraries are using weight_decay instead of regularization under the hood.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>In some of the papers, you would see that the authors preferred SGD over Adam, citing that Adam does not give good performance. The reason for that is (maybe) PyTorch/Tensorflow are doing the above mistake. This thing is explained in detail in this post.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>
<strong>Attention</strong>:- Remember <strong>Attention is not all you need</strong>.<ul>
<li>CS224n <a href="https://www.youtube.com/watch?v=XXtpJxZBa2c">video</a> explaining attention. Attention starts from 1:00:55 hours.</li>
<li>
<em>Attention is all you need</em> <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">paper</a>. This paper also introduces the Transformer which is nothing but a stack of encoder and decoder blocks. The magic is how these blocks are made and connected.</li>
<li>Read an <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">annotated version</a> of the above paper in PyTorch.</li>
<li>Official <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">video</a> explaining Attention</li>
<li>Google <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">blog</a> for Transformer</li>
<li>If you are interested in video you can check these <a href="https://www.youtube.com/watch?v=iDulhoQ2pro">link1</a>, <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">link2</a>.</li>
<li>
<em>Transformer-XL: Attentive Language Models Beyond a Fixed Length Context</em> <a href="https://arxiv.org/abs/1901.02860">paper</a>. Better version of Transformer but BERT does not use this.</li>
<li>Google <a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">blog</a> for Transformer-XL</li>
<li>
<em>Transformer-XL — Combining Transformers and RNNs Into a State-of-the-art Language Model</em> <a href="https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/">blog</a>
</li>
<li>For video check this <a href="https://www.youtube.com/watch?v=cXZ9YBqH3m0">link</a>.</li>
<li>
<em>The Illustrated Transformer</em> <a href="http://jalammar.github.io/illustrated-transformer/">blog</a>
</li>
<li>
<em>Attention and Memory in Deep Learning and NLP</em> <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">blog</a>.</li>
<li>
<em>Attention and Augmented Recurrent Neural Networks</em> <a href="https://distill.pub/2016/augmented-rnns/">blog</a>.</li>
<li>
<em>Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 1</em> <a href="https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8">blog</a>.</li>
<li>
<em>Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 2</em> <a href="https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-ii-c85bf8fd145">blog</a>.</li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is a lot of research going on to make better transformers, maybe I will read more papers on this in the future. Some other transformers include the <a href="https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html">Universal Transformer</a> and <a href="https://www.lyrn.ai/2019/03/12/the-evolved-transformer/">Evolved Transformer</a> which used AutoML to come up with Transformer architecture.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>
<p><strong>Random resources</strong></p>
<ul>
<li>
<em>Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</em> [blog](<a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_">http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_</a>
</li>
<li>
<em>Character-Level Language Modeling with Deeper Self-Attention</em> <a href="https://arxiv.org/abs/1808.04444">paper</a>.</li>
<li>
<em>Using the output embedding to Improve Langauge Models</em> <a href="https://arxiv.org/abs/1608.05859">paper</a>.</li>
<li>
<em>Quasi-Recurrent Neural Networks</em> <a href="https://arxiv.org/abs/1611.01576">paper</a>. A very fast version of LSTM. It uses convolution layers to make LSTM computations parallel. Code can be found in the <a href="https://github.com/fastai/fastai/blob/master/fastai/text/models/qrnn.py">fastai_library</a> or <a href="https://github.com/salesforce/pytorch-qrnn">official_code</a>.</li>
<li>
<em>Deep Learning for NLP Best Practices</em> <a href="http://ruder.io/deep-learning-nlp-best-practices/">blog</a> by Sebastian Ruder. A collection of best practices to be used when training LSTM models.</li>
<li>
<em>Notes on the state of the art techniques for language modeling</em> <a href="https://www.fast.ai/2017/08/25/language-modeling-sota/">blog</a>. A quick summary where Jeremy Howard summarizes some of his tricks which he uses in fastai library.</li>
<li>
<em>Language Modes and Contextualized Word Embeddings</em> <a href="http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/">blog</a>. Gives a quick overview of ELMo, BERT, and other models.</li>
<li>
<em>The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</em> <a href="http://jalammar.github.io/illustrated-bert/">blog</a>.</li>
</ul>
</li>
<li>
<p><strong>Multi-task Learning</strong>:- I am really excited about this. In this case, you train a single model for multiple tasks (more than 10 if you want). So your data looks like “translate to english some_text_in_german”. Your model actually learns to use the initial information to choose the task that it should perform.</p>
<ul>
<li>
<em>An overview of Multi-Task Learning in deep neural networks</em> <a href="http://jalammar.github.io/illustrated-bert/">paper</a>.</li>
<li>
<em>The Natural Language Decathlon: Multitask Learning as Question Answering</em> <a href="https://arxiv.org/abs/1806.08730">paper</a>.</li>
<li>
<em>Multi-Task Deep Neural Networks for Natural Language Understanding</em> <a href="https://arxiv.org/pdf/1901.11504.pdf">paper</a>.</li>
<li>OpenAI GPT is an example of this.</li>
</ul>
</li>
<li>
<p><strong>PyTorch</strong>:- Pytorch provide good <a href="https://pytorch.org/tutorials/#text">tutorials</a> giving you good references on how to code up most of the stuff in NLP.</p>
</li>
<li>
<p><strong>ELMo</strong>:- The first prominent research done where we moved from pretrained word-embeddings to using pretrained-models for getting the word-embeddings. So you use the input sentence to get the embeddings for the tokens present in the sentence.</p>
<ul>
<li>
<em>Deep Contextualized word representations</em> <a href="https://arxiv.org/abs/1802.05365">paper</a>, <a href="https://vimeo.com/277672840">video</a>
</li>
</ul>
</li>
<li>
<p><strong>ULMFit</strong>:- Is this better than BERT maybe not, but still in Kaggle competitions and external competitions ULMFiT gets the first place.</p>
<ul>
<li>
<em>Universal Language Model Fine-tuning for Text Classification</em> <a href="https://arxiv.org/abs/1801.06146">paper</a>.</li>
<li>Jeremy Howard <a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html">blog post</a> announcing ULMFiT.</li>
</ul>
</li>
<li>
<p><strong>OpenAI GPT</strong>:- I have not compared BERT with GPT2, but you work on some kind on ensemble if you want. Do not use GPT1 as BERT was made to overcome the limitations of GPT1.</p>
<ul>
<li>
<em>GPT1</em> <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">paper</a>, <a href="https://openai.com/blog/language-unsupervised/">blog</a>, <a href="https://github.com/openai/finetune-transformer-lm">code</a>
</li>
<li>
<em>GPT2</em>  <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">paper</a>, <a href="https://openai.com/blog/better-language-models/">blog</a>, <a href="https://github.com/openai/gpt-2">code</a>
</li>
<li>Check <a href="https://www.youtube.com/watch?v=T0I88NhR_9M">video</a> by openai on GPT2</li>
</ul>
</li>
<li>
<p><strong>BERT</strong>:- The most successful language model right now (as of May 2019).</p>
<ul>
<li>
<em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> <a href="https://arxiv.org/abs/1810.04805">paper</a>.</li>
<li>Google <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">blog</a> on BERT</li>
<li>
<em>Dissecting BERT Part 1: The Encoder</em> <a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3">blog</a>
</li>
<li>
<em>Understanding BERT Part 2: BERT Specifics</em> <a href="https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73">blog</a>
</li>
<li>
<em>Dissecting BERT Appendix: The Decoder</em> <a href="https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f">blog</a>
</li>
</ul>
</li>
</ol>
<p>To use all these models in PyTorch/Tensorflow you can use <a href="https://github.com/huggingface/transformers">hugginface/transformers</a> which gives complete implementations along with pretrained models for BERT, GPT1, GPT2, TransformerXL.</p>
<p>Congrats you made it to the end. You now have most of the theoretical knowledge needed to practice NLP using the latest models and techniques.</p>
<p>What to do now? You only learned the theory, now practice as much as you can.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="KushajveerSingh/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/notes/2019/05/15/post-0006.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/KushajveerSingh" title="KushajveerSingh"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Kkushaj" title="Kkushaj"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
