---
keywords: fastai
description: A quick summary of probabilistic math used in machine learning.
title: "All you need for Photorealistic Style Transfer in PyTorch"
toc: true
comments: true
author: Kushajveer Singh
categories: [paper-implementation]
badges: false
nb_path: _notebooks/2019-05-06-post-005.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-05-06-post-005.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Link to <a href="https://github.com/KushajveerSingh/Photorealistic-Style-Transfer/tree/master">jupyter notebook</a>, <a href="https://arxiv.org/abs/1904.11617">paper</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-style-transfer?">What is style transfer?<a class="anchor-link" href="#What-is-style-transfer?"> </a></h2><p>We have two images as input one is content image and the other is style image.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/01.jpeg" alt="" title="Figure 1. [left] content image, [right] style image"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our aim is to transfer the style from style image to the content image. This looks something like this.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/02.jpeg" alt="" title="Figure 2. Output image"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-another-paper?">Why another paper?<a class="anchor-link" href="#Why-another-paper?"> </a></h2><p>Earlier work on style transfer although successful was not able to maintain the structure of the content image. For instance, see Fig2 and then see the original content image in Fig1. As you can see the curves and structure of the content image are not distorted and the output image has the same structure as content image.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/03.jpeg" alt="" title="Figure 3. The results from this paper are shown in [e] and [j] and  a comparison is done with other methods as well."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gram-Matrix">Gram Matrix<a class="anchor-link" href="#Gram-Matrix"> </a></h2><p>The main idea behind the paper is using Gram Matrix for style transfer. It was shown in these 2 papers that Gram Matrix in feature map of convolutional neural network (CNN) can represent the style of an image and propose the neural style transfer algorithm for image stylization.</p>
<ol>
<li><a href="https://arxiv.org/abs/1505.07376">Texture Synthesis Using Convolution Neural Networks</a> by Gatys et al. 2015</li>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">Image Style Transfer Using Convolutional Neural Networks</a> by Gatys et al. 2016</li>
</ol>
<p>Details about gram matrix can be found on wikipedia. Mathematically, given a vector V gram matrix is computed as
{% raw %}
$$G=V^TV$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="High-Resolution-Models">High-Resolution Models<a class="anchor-link" href="#High-Resolution-Models"> </a></h2><p>It is a recent research paper accepted at CVPR 2019 paper. So generally what happens in CNNs is we first decrease the image size while increasing the number of filters and then increase the size of the image back to the original size.</p>
<p>Now this forces our model to generate output images from a very small resolution and this results in loss of finer details and structure. To counter this fact High-Res model was introduced.</p>
<p>High-resolution network is designed to maintain high-resolution representations through the whole process and continuously receive information from low-resolution networks. So we train our models on the original resolution.</p>
<p>Example of this model would be covered below. You can refer to the original <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">papers</a> for more details on this. I will cover this topic in detail in my next week blog post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Style-transfer-details">Style transfer details<a class="anchor-link" href="#Style-transfer-details"> </a></h2><p>The general architecture of modern deep learning style transfer algorithms looks something like this.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/04.jpeg" alt="" title="Figure 4. Model architecture for style transfer in the deep learning era."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are three things that style transfer model needs</p>
<ol>
<li><strong>Generating model</strong>:- It would generate the output images. In Fig4 this is ‘Hi-Res Generation Network’</li>
<li><strong>Loss function</strong>:- Correct choice of loss functions is very important in case you want to achieve good results.</li>
<li><strong>Loss Network</strong>:- You need a CNN model that is pretrained and can extract good features from the images. In our case, it is VGG19 pretrained on ImageNet.</li>
</ol>
<p>So we load VGG model. The complete code is available at my <a href="https://github.com/KushajveerSingh/Photorealistic-Style-Transfer">GitHub repo</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;GPU is not available&#39;</span><span class="p">)</span>
    
<span class="c1"># Load VGG19 features. We do not need the last linear layers,</span>
<span class="c1"># only CNN layers are needed</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">features</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">vgg</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># We don&#39;t want to train VGG</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">vgg</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we load our images from disk. My images are stored as <em>src/imgs/content.png and src/imgs/style.png</em>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="n">content_img</span> <span class="o">=</span> <span class="n">load_image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">img_root</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">content_img</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">content_img</span> <span class="o">=</span> <span class="n">content_img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">style_img</span> <span class="o">=</span> <span class="n">load_image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">img_root</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">style_img</span><span class="p">))</span>
<span class="n">style_img</span> <span class="o">=</span> <span class="n">style_img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Show content and style image</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im_convert</span><span class="p">(</span><span class="n">content_img</span><span class="p">))</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im_convert</span><span class="p">(</span><span class="n">style_img</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Utility functions</span>
<span class="k">def</span> <span class="nf">im_convert</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert img from pytorch tensor to numpy array, so we can plot it.</span>
<span class="sd">    It follows the standard method of denormalizing the img and clipping</span>
<span class="sd">    the outputs</span>
<span class="sd">    </span>
<span class="sd">    Input:</span>
<span class="sd">        img :- (batch, channel, height, width)</span>
<span class="sd">    Output:</span>
<span class="sd">        img :- (height, width, channel)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resize img to size, size should be int and also normalize the</span>
<span class="sd">    image using imagenet_stats</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
    
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">))</span>
    <span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Detail</strong>:- When we load our images, what sizes should we use? Your content image size should be divisible by 4, as our model would downsample images 2 times. For style images, do not resize them. Use their original resolution. Size of content image is (500x500x3) and size of style image is (800x800x3).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hi-Res-Generation-Network">Hi-Res Generation Network<a class="anchor-link" href="#Hi-Res-Generation-Network"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/05.jpeg" alt="" title="Figure 5. The structure of high-resolution generation network. When we fuse feature maps with different resolution, we directly concatenate these feature images like the inception module, for example, the feature map 4, is concatenated by the feature map 2 and the feature map 3. We use bottleneck residual to ensure that our network can be trained well and speedup the training while preserving good visual effects."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model is quite simple we start with 500x500x3 images and maintain this resolution for the complete model. We downsample to 250x250 and 125x125 and then fuse these back together with 500x500 images.</p>
<p><strong>Details</strong>:-</p>
<ol>
<li>No pooling is used (as pooling causes loss of information). Instead strided convolution (i.e. stride=2) are used.</li>
<li>No dropout is used. But if you need regularization you can use weight decay.</li>
<li>3x3 conv kernels are used everywhere with padding=1.</li>
<li>Zero padding is only used. Reflex padding was tested but the results were not good.</li>
<li>For upsampling,’bilinear’ mode is used.</li>
<li>For downsampling, conv layers are used.</li>
<li>InstanceNorm is used.</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="c1"># Downsampling function</span>
<span class="k">def</span> <span class="nf">conv_down</span><span class="p">(</span><span class="n">in_c</span><span class="p">,</span> <span class="n">out_c</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_c</span><span class="p">,</span> <span class="n">out_c</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Upsampling function</span>
<span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implementation-code">Implementation code<a class="anchor-link" href="#Implementation-code"> </a></h2><p>Residual connections are used between every block. We use BottleNeck layer from the ResNet architecture. (In Fig5 all the horizontal arrows are bottleneck layers).</p>
<p>Refresher on bottleneck layer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/06.jpeg" alt="" title="Figure 6. Architecture of BottleneckModule."></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="c1"># Helper class for BottleneckBlock</span>
<span class="k">class</span> <span class="nc">ConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># We have to keep the size of images same, so choose padding accordingly</span>
        <span class="n">num_pad</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">num_pad</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">BottleneckBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used</span>
<span class="sd">    instead of BatchNorm because when we want to generate images, we normalize</span>
<span class="sd">    all the images independently. </span>
<span class="sd">    </span>
<span class="sd">    (In batch norm you compute mean and std over complete batch, while in instance </span>
<span class="sd">    norm you compute mean and std for each image channel independently). The reason for </span>
<span class="sd">    doing this is, the generated images are independent of each other, so we should</span>
<span class="sd">    not normalize them using a common statistic.</span>
<span class="sd">    </span>
<span class="sd">    If you confused about the bottleneck architecture refer to the official pytorch</span>
<span class="sd">    resnet implementation and paper.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_c</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_c</span> <span class="o">=</span> <span class="n">out_channels</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">identity_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">ConvLayer</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">ConvLayer</span><span class="p">(</span><span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">ConvLayer</span><span class="p">(</span><span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">ConvLayer</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_c</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_c</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we are ready to implement our style_transfer model, which we call HRNet (based on the paper). Use the Fig5 as reference.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="k">class</span> <span class="nc">HRNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf.</span>
<span class="sd">    </span>
<span class="sd">    Naming convention used.</span>
<span class="sd">    I refer to vertical layers as a single layer, so from left to right we have 8 layers</span>
<span class="sd">    excluding the input image.</span>
<span class="sd">    E.g. layer 1 contains the 500x500x16 block</span>
<span class="sd">         layer 2 contains 500x500x32 and 250x250x32 blocks and so on</span>
<span class="sd">    </span>
<span class="sd">    self.layer{x}_{y}:</span>
<span class="sd">        x :- the layer number, as explained above</span>
<span class="sd">        y :- the index number for that function starting from 1. So if layer 3 has two</span>
<span class="sd">             downsample functions I write them as `downsample3_1`, `downsample3_2`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample2_1</span> <span class="o">=</span> <span class="n">conv_down</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3_2</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample3_1</span> <span class="o">=</span> <span class="n">conv_down</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample3_2</span> <span class="o">=</span> <span class="n">conv_down</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample3_3</span> <span class="o">=</span> <span class="n">conv_down</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer5_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer6_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer7_1</span> <span class="o">=</span> <span class="n">BottleneckBlock</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer8_1</span> <span class="o">=</span> <span class="n">conv_down</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Needed conv layer so reused conv_down function</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">map1_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">map2_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2_1</span><span class="p">(</span><span class="n">map1_1</span><span class="p">)</span>
        <span class="n">map2_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample2_1</span><span class="p">(</span><span class="n">map1_1</span><span class="p">)</span>
        
        <span class="n">map3_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer3_1</span><span class="p">(</span><span class="n">map2_1</span><span class="p">),</span> <span class="n">upsample</span><span class="p">(</span><span class="n">map2_2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">map3_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">downsample3_1</span><span class="p">(</span><span class="n">map2_1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3_2</span><span class="p">(</span><span class="n">map2_2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">map3_3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">downsample3_2</span><span class="p">(</span><span class="n">map2_1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample3_3</span><span class="p">(</span><span class="n">map2_2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">map4_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer4_1</span><span class="p">(</span><span class="n">map3_1</span><span class="p">),</span> <span class="n">upsample</span><span class="p">(</span><span class="n">map3_2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">upsample</span><span class="p">(</span><span class="n">map3_3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer5_1</span><span class="p">(</span><span class="n">map4_1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer6_1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer7_1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer8_1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-functions">Loss functions<a class="anchor-link" href="#Loss-functions"> </a></h2><p>In style transfer we use feature extraction, to calculate the value of losses. Feature extraction put in simple terms, means you take a pretrained imagenet model and pass your images through it and store the intermediate layer outputs. Generally, VGG model is used for such tasks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_005/07.jpeg" alt="" title="Figure 7. Architecture of VGG model."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So you take the outputs from the conv layers. Like for the above fig, you can take the output from the second 3x3 conv 64 layer and then 3x3 conv 128.</p>
<p>To extract features from VGG we use the following code.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="k">def</span> <span class="nf">get_features</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use VGG19 to extract features from the intermediate layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;0&#39;</span> <span class="p">:</span> <span class="s1">&#39;conv1_1&#39;</span><span class="p">,</span>  <span class="c1"># style layer</span>
            <span class="s1">&#39;5&#39;</span> <span class="p">:</span> <span class="s1">&#39;conv2_1&#39;</span><span class="p">,</span>  <span class="c1"># style layer</span>
            <span class="s1">&#39;10&#39;</span><span class="p">:</span> <span class="s1">&#39;conv3_1&#39;</span><span class="p">,</span>  <span class="c1"># style layer</span>
            <span class="s1">&#39;19&#39;</span><span class="p">:</span> <span class="s1">&#39;conv4_1&#39;</span><span class="p">,</span>  <span class="c1"># style layer</span>
            <span class="s1">&#39;28&#39;</span><span class="p">:</span> <span class="s1">&#39;conv5_1&#39;</span><span class="p">,</span>  <span class="c1"># style layer</span>
            
            <span class="s1">&#39;21&#39;</span><span class="p">:</span> <span class="s1">&#39;conv4_2&#39;</span>   <span class="c1"># content layer</span>
        <span class="p">}</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">img</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">features</span><span class="p">[</span><span class="n">layers</span><span class="p">[</span><span class="n">name</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x</span>
            
    <span class="k">return</span> <span class="n">features</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use 5 layers in total for feature extraction. Only conv4_2 is used as layer for content loss.</p>
<p>Refer to Fig4, we pass our output image from HRNet and the original content and style image through VGG.</p>
<p>There are two losses</p>
<ol>
<li>Content Loss</li>
<li>Style Loss</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Content-Loss">Content Loss<a class="anchor-link" href="#Content-Loss"> </a></h3><p>Content image and the output image should have a similar feature representation as computed by loss network VGG. Because we are only changing the style without any changes to the structure of the image. For the content loss, we use Euclidean distance as shown by the formula</p>
<p>{% raw %}
$$l_{content}^{\phi,j}(y,\hat{y})=\frac{1}{C_jJ_jW_j}\left\|\phi_j(\hat{y}=\phi_j(y)\right\|^2$$
{% endraw %}</p>
<p>$\phi_j$ means we are referring to the activations of the j-th layer of loss network. In code it looks like this.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>style_net = HRNet().to(device)</p>
<p>target = style_net(content_img).to(device)
target.requires<em>grad</em>(True)</p>
<p>target_features = get_features(target, vgg)
content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Style-Loss">Style Loss<a class="anchor-link" href="#Style-Loss"> </a></h3><p>We use gram matrix for this. So style of an image is given by its gram matrix. Our aim is to make style of two images close, so we compute the difference of gram matrix of style image and output image and then take their Frobenius norm.</p>
<p>{% raw %}
$$l_{style}^{\phi,j}(y,\hat{y})=\left\|G_j^{\phi}(y)-G_j^{\phi}(\hat{y})\right\|^2$$
{% endraw %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open>
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-show</span>
<span class="k">def</span> <span class="nf">get_gram_matrix</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gram matrix by converting to 2D tensor and doing dot product</span>
<span class="sd">    img: (batch, channel, height, width)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>
    <span class="n">gram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">gram</span>
    

<span class="c1"># There are 5 layers, and we compute style loss for each layer and sum them up</span>
<span class="n">style_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
    <span class="n">target_gram_matrix</span> <span class="o">=</span> <span class="n">get_gram_matrix</span><span class="p">(</span><span class="n">target_feature</span><span class="p">)</span>
    <span class="c1"># we already computed gram matrix for our style image</span>
    <span class="n">style_gram_matrix</span> <span class="o">=</span> <span class="n">style_gram_matrixs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>

    <span class="n">layer_style_loss</span> <span class="o">=</span> <span class="n">style_weights</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">target_gram_matrix</span> <span class="o">-</span> <span class="n">style_gram_matrix</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">target_feature</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">style_loss</span> <span class="o">+=</span> <span class="n">layer_style_loss</span> <span class="o">/</span> <span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Difficult-part">Difficult part<a class="anchor-link" href="#Difficult-part"> </a></h2><p>To compute our final losses, we multiply them with some weights.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">content_loss</span> <span class="o">=</span> <span class="n">content_weight</span> <span class="o">*</span> <span class="n">content_loss</span>  
<span class="n">style_loss</span> <span class="o">=</span> <span class="n">style_weight</span> <span class="o">*</span> <span class="n">style_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The difficulty comes in setting these values. If you want some desired output, then you would have to test different values before you get your desired result.</p>
<p>To build your own intuitions you can choose two images and try different range of values. I am working on providing like a summary of this. It will be available in my repo README.</p>
<p>Paper recommends content_weight = [50, 100] and style_weight = [1, 10].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>Well, congratulation made it to the end. You can now implement style transfer. Now read the paper for more details on style transfer.</p>
<p>Check out my repo <a href="https://github.com/KushajveerSingh/Photorealistic-Style-Transfer">README</a>, it will contain the complete instructions on how to use the code in the repo, along with complete steps on how to train your model.</p>

</div>
</div>
</div>
</div>
 

