---
title: "Exploring the issue of depth in GNNs and exploring ways of solving it"
description: "The post starts by exploring the question is depth a problem, then tries to find the reasons why depth is a problem and then the SOTA methods are discussed for solving this issue. Code for all experiments is available."
layout: post
toc: true
comments: true
hide: true
search_exclude: true
categories: [graph_machine_learning]
---

---
* [1] [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212)
* [2] [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)
[github](https://github.com/KushajveerSingh/gnn_depth)

> The code for all the experiments is available on [github](https://github.com/KushajveerSingh/gnn_depth). README contains all the details on how to reproduce everything in the blog. For easier access, I will refer to the *.py files in the blog whereever necessary. The code uses [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) and [Seaborn](https://seaborn.pydata.org/).

Graph neural networks (GNNs) have shown to be very useful for learning graph node representations over graph-structured data. The GNNs are trained using neural message passing framework [1] and Graph Convolutional Network (GCN) [2] is one of the most widely used GNN architecture. GNN architectures is the focus of my next blog. So for this post, I will use GCN for all the analysis.

So let's start.

## (Deep) Graph Neural Networks

Deep learning is known for training very deep models (the word *deep* is in the name). So training deep GNNs should be a no brainer to get better accuracy. 

Let's explore this idea a bit. Below I show the results of training a {2,4,8,16,32,64}-Layer GCN on Cora dataset.

![]({{ site.baseurl }}/images/post_0014/00.png "Figure 1: Performance degradation of GCNs is shown, where the accuracy of a deep GCN model drops by a large amount as depth increases.")

> Note: Use `python 01_deep_gcn.py` to reproduce results of Figure 1.

The results are a complete opposite of what deep learning is famous for. So why did deeper models not train well?

The most reasons are gradient vanishing and overfitting due to a large number of parameters.

## Gradient vanishing

Gradient vanishing occurs in deep models when small gradients (less than 1) keep multiplying. We can confirm this by checking the spread of the gradients over time ($mean \pm std$). Figure 2, shows the gradients of the second layer of a 32-Layer GCN model during training.

|Epoch|Mean                  |Std                   |
|:---:|:--------------------:|:--------------------:|
|0    |-6.438882849124639e-08|7.142710387597617e-07 |
|50   |-2.276356322406317e-14|9.411642472312298e-13 |
|100  |6.48658677992151e-25  |0.0                   |
|150  |-3.384510150852408e-33|0.0                   |
|200  |4.707241801359926e-41 |0.0                   |
|250  |0.0                   |0.0                   |
|300  |0.0                   |0.0                   |
|350  |0.0                   |0.0                   |
|400  |0.0                   |0.0                   |

> Note: Use `python 02_check_gradients.py` to reproduce results of above table.

We can see that gradients quickly vanish to 0, which is bad. In computer vision to solve this problem, the idea of residual (shortcut) connections was developed. The idea of residual connections is to add the original input to the output of a function, as shown below

$$
orig\_x = x
$$

$$
x = func(x)
$$

$$
x = orig\_x + x
$$

Roughly, during backpropagation 1 is added to the gradients of $x$ which prevents vanishing gradients (as gradients now become > 1).

Now let's add residual connections to the GCN models and check if the accuracy improves.




> Note: Use `python 03_deep_gcn_with_residual.py` to reproduce results of Figure 2 and above table.