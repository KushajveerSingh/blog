---
keywords: fastai
description: A quick summary of probabilistic math used in machine learning.
title: "Theoretical Machine Learning: Probabilistic and Statistical Math"
toc: true
comments: true
author: Kushajveer Singh
categories: [discussion]
image: images/preview/post_001.jpeg
badges: false
nb_path: _notebooks/2018-09-14-post-0001.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-09-14-post-0001.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_001/01.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as <strong>Polynomial Curve Fitting</strong> in ML.</p>
<p>In curve fitting, we usually use the <strong>maximum likelihood approach</strong> which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data).</p>
<p>To overcome overfitting we use <strong>regularization techniques</strong> by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quick-Probability-Overview">Quick Probability Overview<a class="anchor-link" href="#Quick-Probability-Overview"> </a></h2><p>Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as <em>X = Event that heads come and Y = Event that tails come</em>. Now P(X) = Probability of that particular event happening.</p>
<p>Takeaway, represent your probabilities as events of something happening.</p>
<p>In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable.</p>
<p>Next, we discuss <strong>expectation</strong>. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value?</p>
<p>Suppose x takes the value 10, the average of that value is the number itself.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_001/02.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we discuss <strong>variance</strong>. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is.</p>
<p>Another important concept in probabilistic maths is the concept of <strong>prior</strong>, <strong>posterior</strong> and <strong>likelihood</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_001/03.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Prior = P(X). It is probability available before we observing an event\</p>
<p>Posterior = P(X|Y). It is the probability of X after event Y has happened.\</p>
<p>Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X\</p>
<p>Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='Follow the resource list given at the end for further reading of the topic.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gaussian-Distribution">Gaussian Distribution<a class="anchor-link" href="#Gaussian-Distribution"> </a></h2><p>As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_001/04.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But before that let us discuss why we need to know about <strong>probability distributions</strong> in the first place?</p>
<p>When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output.</p>
<p>Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models?</p>
<p>After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial.</p>
<p>Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now I present a general way of approaching the problem of finding the values for the variables of a prior.</p>
<p>The Gaussian Equation is represented as follow</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% raw %}
$$\mathcal{N}(x|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}}exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution.</p>
<p>So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed <strong>N</strong> values as input. Now all these <strong>N</strong> values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% raw %}
$$p(x|\mu,\sigma^2)=\prod_{n=1}^{N}\mathcal{N}(x_n|\mu,\sigma^2)$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% raw %}
$$\ln p(x|\mu,\sigma^2)=-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2}\ln \sigma^2-\frac{N}{2}\ln (2\pi)$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now take the derivative w.r.t the variables and get their values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% raw %}
$$\mu_{ML}=\frac{1}{N}\prod_{n=1}^{N}x_n$$
{% endraw %}
{% raw %}
$$\sigma^2_{ML}=\frac{1}{N}\prod_{n=1}{N}(x_n-\mu_{ML})^2$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-was-all-this-important?">Why was all this important?<a class="anchor-link" href="#Why-was-all-this-important?"> </a></h2><p>All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior.</p>
<p>In cases where you have to take a prior like Poisson MSE would not be a good metric to consider.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-Prior-on-the-variables-also">Set Prior on the variables also<a class="anchor-link" href="#Set-Prior-on-the-variables-also"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_001/05.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution?</p>
<p>Ideally, you can choose any distribution. But practically you want to choose a <strong>conjugate prior</strong> for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors.</p>
<p>Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance.</p>
<p>Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition.</p>
<p>Here is the list of resources that I would highly recommend for learning ML:-</p>
<ol>
<li>CS229: Machine Learning by Stanford</li>
<li>Pattern Recognition and Machine Learning by Christopher M. Bishop</li>
<li>An Introduction to Statistical Learning: with Applications in R</li>
<li>The Elements of Statistical Learning Data Mining, inference and Prediction</li>
</ol>

</div>
</div>
</div>
</div>
 

