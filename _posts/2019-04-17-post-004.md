---
keywords: fastai
description: New state of the art method for generating colored images from segmentation masks. It uses a GAN to learn to produce photorealistic images.
title: "SPADE: State of the art in Image-to-Image Translation by Nvidia"
toc: true
comments: true
author: Kushajveer Singh
categories: [paper-implementation]
image: images/preview/post_004.png
badges: false
nb_path: _notebooks/2019-04-17-post-004.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-04-17-post-004.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Link to <a href="https://github.com/KushajveerSingh/SPADE-PyTorch">implementation code</a>, <a href="https://arxiv.org/abs/1903.07291">paper</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To give motivation for this paper, see the demo released by Nvidia.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include youtube.html content='<a href="https://youtu.be/MXWm6w4E5q0">https://youtu.be/MXWm6w4E5q0</a>' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-Semantic-Image-Synthesis?">What is Semantic Image Synthesis?<a class="anchor-link" href="#What-is-Semantic-Image-Synthesis?"> </a></h2><p>It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_004/01.jpeg" alt="" title="Figure 1. left segmentation map. right corresponding colored image of the segmentation map. Image taken from CityScapes Dataset."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="New-things-in-the-paper">New things in the paper<a class="anchor-link" href="#New-things-in-the-paper"> </a></h2><p>SPADE paper introduces a new normalization technique called <strong>spatially-adaptive normalization</strong>. Earlier models used the seg map only at the input layer but as seg map was only available in one layer the information contained in the seg map washed away in the deeper layers. SPADE solves this problem. In SPADE, we give seg map as input to all the intermediate layers.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-train-the-model?">How to train the model?<a class="anchor-link" href="#How-to-train-the-model?"> </a></h2><p>Before getting into the details of the model, I would discuss how models are trained for a task like Semantic Image Synthesis.</p>
<p>The core idea behind the model training is a GAN. Why GAN is needed? Because whenever we want to generate something that looks photorealistic or more technically closer to the output images, we have to use GANs.</p>
<p>So for GAN we need three things 1) Generator 2) Discriminator 3) Loss Function. For the Generator, we need to input some random values. Now you can either take random normal values. But if you want your output image to resemble some other image i.e. take the style of some image and add it your output image, you will also need an image encoder which would provide the mean and variance values for the random Gaussian distribution.</p>
<p>For the loss function, we would use the loss function used in pix2pixHD paper with some modifications. Also, I would discuss this technique where we extract features from the VGG model and then compute loss function (perceptual loss).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SPADE">SPADE<a class="anchor-link" href="#SPADE"> </a></h2><p>This is the basic block that we would use.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_004/02.jpeg" alt="" title="Figure 2. left shows the architecture of the model. right a 3D view of the model. Figure taken from paper."></p>

</div>
</div>
</div>
</div>
 

