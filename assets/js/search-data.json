{
  
    
        "post0": {
            "title": "Complete Walkthrough of Neural Network Initialization (Code + Maths)",
            "content": "import gzip, pickle, math from pathlib import Path from collections import defaultdict import numpy as np import matplotlib.pyplot as plt import seaborn as sns from fastai2.data.external import download_data # if you do not have fastai2, you can # download the dataset manually and place # in the extra folder import torch import torch.nn as nn import torch.nn.functional as F . Get Data . I use MNIST dataset. The dataset choice does not matter much as the things discussed in the notebook are general and apply to every dataset. . def get_data(): # if you do not have fastai, you can download the dataset directly from the given URL MNIST_URL = &#39;http://deeplearning.net/data/mnist/mnist.pkl.gz&#39; path = download_data(MNIST_URL, &#39;extra/mnist.pkl.gz&#39;) with gzip.open(path, &#39;rb&#39;) as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=&#39;latin-1&#39;) # By default these are numpy arrays, so convert them to pytorch tensors return map(torch.tensor, (x_train,y_train,x_valid,y_valid)) def normalize(x, m, s): return (x-m)/s . x_train, y_train, x_valid, y_valid = get_data() . # Normalize input data also train_mean, train_std = x_train.mean(), x_train.std() x_train = normalize(x_train, train_mean, train_std) x_valid = normalize(x_valid, train_mean, train_std) # use training stats for validation set also . x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . (torch.Size([50000, 784]), torch.Size([50000]), torch.Size([10000, 784]), torch.Size([10000])) . x_train.mean(), x_train.std(), x_valid.mean(), x_valid.std() . (tensor(-7.6999e-06), tensor(1.), tensor(-0.0059), tensor(0.9924)) . Now the inputs come from a distribution of $mean = 0$ and $std = 1$ (or $var = 1$ as $var = std^2$) . Note:- This fact will be used later, when we derive formula for variance. . Objective . We want the activations in our neural network to have 0.5 mean and standard deviation of 1. Now by activation I mean the output of activation_function(Linear(x, w, b)) layer which can be simplified as $max(x@w+b, 0)$. . Note:- Because we take the ReLU of activations, the mean gets shifted from 0 to 0.5. If we considered the activations after Linear layer, the mean would be 0. . But what about Batch Norm? The purpose of BatchNorm is to normalize the activations and then scale them by using a learned mean and standard deviation. So the idea of batch normalization is to have the same distribution over tie for the ease of training. Thus, BatchNorm reduces the strong dependence on initialization. However, initialization is still important because we can get into bad local minimas at the start of the training if we do not initialize out networks properly. (You can check my other notebook in which I discuss this in detail). . For this reason, I will not include BatchNorm in this discussion. . Kaiming Init (Code) . Kaiming Init (He Init) is the most commonly used initialization. It&#39;s implementation is as follows: . w1 = torch.randn(784, 50) * math.sqrt(2/784) b1 = torch.zeros(50) w2 = torch.randn(50,1) * math.sqrt(2/50) b2 = torch.zeros(1) . And that&#39;s it. You sample from a Normal Gaussian Distribution (mean=0, std=1) and then change it&#39;s standard deviation to math.sqrt(2/num_inputs). This preserves your activations std in the forward propagation. . In PyTorch you can implement this using nn.init.kaiming_normal_(...). The implementation details will be discussed in the end. . Now we have got the bigger picture that initialization is just multiplying by some value. Let&#39;s get into details of initialization. . Why Initialization is Important? . Example 1 . Pass the input through linear and relu layers and see the standard deviation of the outputs. . m = 784 nh = 10 #Num hidden units . # Get random weights w1 = torch.randn(m,nh) b1 = torch.zeros(nh) # bias initialized to 0 w2 = torch.randn(nh,1) b2 = torch.zeros(1) . def stats(x): # Utility function to print mean and std return x.mean(), x.std() . # Input is normalized to mean=0, std=1 stats(x_valid) . (tensor(-0.0059), tensor(0.9924)) . def lin(x, w, b): return x@w + b . t = lin(x_valid, w1, b1) stats(t) . (tensor(-2.8136), tensor(26.8307)) . The mean and std are way off from 0 and 1 respectively. Now we can take ReLU of this. . def relu(x): return x.clamp_min(0.) . t = relu(t) stats(t) . (tensor(9.3641), tensor(14.4010)) . We can see from this example, why initialization is important. After, just one layer(relu+linear) our mean and standard deviation have diverged from their initial value of 0 and 1. . Now imagine if we had 100 of these layers. Let&#39;s try that now. . Example 2 . To understand why initialization is important in a neural net, we&#39;ll focus on the basic operation you have there: matrix multipications. So let&#39;s just take a vector x, and a matrix a initialized randomly, then multiply them 100 times (as if we had 100 layers). . x = torch.randn(512) a = torch.randn(512, 512) . for i in range(100): x = a@x . stats(x) . (tensor(nan), tensor(nan)) . The problem we get is of activation explosion. Very soon our activations go to nan i.e. their values go to a large value that cannot be represented in memory. We can see when that happens. . x = torch.randn(512) a = torch.randn(512, 512) for i in range(100): x = a@x if x.mean() != x.mean(): break # nan != nan i . 27 . So it only took 27 multipications. Now only possible solution to mitigate this problem is to reduce the scale of the matrix a. . x = torch.randn(512) a = torch.randn(512,512)*0.01 for i in range(100): x = a@x . stats(x) . (tensor(0.), tensor(0.)) . Now we got the problem of vanishing gradients where our activations vanish to 0. . We can try Xavier initialization to solve this problem. (Xavier init is same as Kaiming init except in the numerator we have 1 instead of 2). . x = torch.randn(512) a = torch.randn(512,512)*math.sqrt(1/512) for i in range(100): x = a@x . stats(x) . (tensor(-0.1040), tensor(2.8382)) . It works. . We can try this experiment by adding relu also, in which case we would use Kaiming init. . The difference between Xavier and Kaiming init is . Xavier Init: Only Linear layer is considered | Kaiming Init: Linear + ReLU layer is considered | . x = torch.randn(512) a = torch.randn(512,512)*math.sqrt(2/512) for i in range(100): x = a@x x = x.clamp_min(0.) stats(x) . (tensor(1.2160), tensor(1.7869)) . And it works. The activations did not explode nor vanish. . Example 3 . Train a simple feedforward network on MNIST dataset and visualize the histogram of the activation values. . def lin_relu(c_in, c_out): return nn.Sequential( nn.Linear(c_in, c_out), nn.ReLU(inplace=True), ) class Net(nn.Module): def __init__(self): super().__init__() # Create a linear-relu model self.model = nn.Sequential( *lin_relu(784, 1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), nn.Linear(1000,10), # No need of softmax as it will be in the loss function ) self.initialize_model() def forward(self, x): return self.model(x) def initialize_model(self): # Before Xavier init, people used to sample weights from # a uniform distribution for layer in self.model: if isinstance(layer, nn.Linear): value = 1./math.sqrt(layer.weight.shape[1]) layer.weight.data.uniform_(-value, value) layer.bias.data.fill_(0) net = Net() net.model . Sequential( (0): Linear(in_features=784, out_features=1000, bias=True) (1): ReLU(inplace=True) (2): Linear(in_features=1000, out_features=1000, bias=True) (3): ReLU(inplace=True) (4): Linear(in_features=1000, out_features=1000, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=1000, out_features=1000, bias=True) (7): ReLU(inplace=True) (8): Linear(in_features=1000, out_features=1000, bias=True) (9): ReLU(inplace=True) (10): Linear(in_features=1000, out_features=10, bias=True) ) . bs = 128 device = torch.device(&#39;cuda&#39;) net = net.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) dataset = torch.utils.data.TensorDataset(x_train, y_train) dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) . # A quick training loop for epoch in range(70): running_loss = .0 for i, data in enumerate(dataloader,0): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch+1}: {running_loss/len(dataloader)}&#39;) . 1: 2.2928382635116575 2: 2.2065723312206758 3: 1.194048885657237 4: 0.5237417409817378 5: 0.3836347458072198 6: 0.3146293921730457 7: 0.2697077517135021 8: 0.2339217932942586 9: 0.20382011488844187 10: 0.17836485357047654 11: 0.1592684537745439 12: 0.14289972114448363 13: 0.1286666537133547 14: 0.11684789568758928 15: 0.10570534034990348 16: 0.09746135014754076 17: 0.08864978084770533 18: 0.08108058688827814 19: 0.07408739045166816 20: 0.06816183782349794 21: 0.06298899033512824 22: 0.05734651746849219 23: 0.05177393974497532 24: 0.047053486519517046 25: 0.04282796499438775 26: 0.039077873800236446 27: 0.03567600295138665 28: 0.03188744994023671 29: 0.029368048552901316 30: 0.02597655968215221 31: 0.023195109860255168 32: 0.020861454260272857 33: 0.018848492100070686 34: 0.017209559499930877 35: 0.015161331571065462 36: 0.013735617821415266 37: 0.012041004417607417 38: 0.010973294661977353 39: 0.010113576904703409 40: 0.009030632378581243 41: 0.008074290845065545 42: 0.007224243325300706 43: 0.006513816581513637 44: 0.0059723575384571 45: 0.00554538136586929 46: 0.004938531619233963 47: 0.004390296693413685 48: 0.004122716288727063 49: 0.0038293207541872294 50: 0.0034256023139907763 51: 0.003229047467884345 52: 0.002975824174399559 53: 0.002749748910084749 54: 0.002592976238483038 55: 0.0023717502848460124 56: 0.0021688310572734247 57: 0.002044256757467221 58: 0.0019431500003123895 59: 0.0017717176236403294 60: 0.001694328805957085 61: 0.0015888030234819804 62: 0.001510932248754379 63: 0.0014456520477930705 64: 0.0013670944441587496 65: 0.0013064238218924939 66: 0.0012553433672739909 67: 0.0011863003556544965 68: 0.0011301248310468135 69: 0.0010923029807133552 70: 0.001042095409371914 . Now visualize the activation values by using a test image. . torch.save(net.state_dict(), &#39;uniform.pt&#39;) . actvs = {} net = net.to(&#39;cpu&#39;) images = x_valid[:10000] out = images.clone() for i in range(len(net.model)): if i in (1,3,5,7,9): out = net.model[i](out) actvs[i] = out.squeeze(0).detach().mean(axis=0).numpy() else: out = net.model[i](out) . Visualize the activations using a kdeplot. It is similar to histogram, but gives a better view of the data. Alternatively, histogram can also be plotted to see the effect. . fig, ax = plt.subplots(figsize=(15,5)) ax.set_xlim(-1, 1) for k,v in actvs.items(): sns.kdeplot(v, ax=ax, label=f&#39;Layer {k}&#39;) . Test above example with Kaiming Init . def lin_relu(c_in, c_out): return nn.Sequential( nn.Linear(c_in, c_out), nn.ReLU(inplace=True), ) class Net(nn.Module): def __init__(self): super().__init__() # Create a linear-relu model self.model = nn.Sequential( *lin_relu(784, 1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), nn.Linear(1000,10), # No need of softmax as it will be in the loss function ) self.initialize_model() def forward(self, x): return self.model(x) def initialize_model(self): for layer in self.model: if isinstance(layer, nn.Linear): nn.init.kaiming_normal_(layer.weight.data, mode=&#39;fan_out&#39;) layer.bias.data.fill_(0) net = Net() . bs = 128 device = torch.device(&#39;cuda&#39;) net = net.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) dataset = torch.utils.data.TensorDataset(x_train, y_train) dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) . # A quick training loop for epoch in range(70): running_loss = .0 for i, data in enumerate(dataloader,0): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch+1}: {running_loss/len(dataloader)}&#39;) . 1: 0.7095098866484104 2: 0.0923736249263852 3: 0.03983800867333626 4: 0.01766058729531673 5: 0.009082728055998301 6: 0.005672304465984687 7: 0.004172981537591953 8: 0.003315979051284301 9: 0.0027411463121191047 10: 0.0023460902225894806 11: 0.0020474239801749204 12: 0.0018151276721022067 13: 0.0016298005023063758 14: 0.0014792730888495077 15: 0.0013441605111345267 16: 0.0012390905131514256 17: 0.001146641082297533 18: 0.0010655345013126348 19: 0.0009982335691650708 20: 0.000935702603787948 21: 0.0008791005668731837 22: 0.0008290120758689366 23: 0.0007855443713756708 24: 0.0007456283538769453 25: 0.0007100430006782214 26: 0.0006764667108654976 27: 0.0006458146258806571 28: 0.0006196549830910488 29: 0.0005923675229916206 30: 0.0005684418173936697 31: 0.0005472387879704817 32: 0.0005264173046900676 33: 0.0005066754296422004 34: 0.0004908624558876722 35: 0.00047273215575095934 36: 0.0004578854984197861 37: 0.00044289983044832183 38: 0.0004285678219718811 39: 0.00041579040579306774 40: 0.0004036462746369533 41: 0.00039161906983607855 42: 0.000380707326798867 43: 0.0003695801998942326 44: 0.00035994008947641423 45: 0.00035058893263339996 46: 0.000341620133855404 47: 0.0003329002513335301 48: 0.0003244552044914319 49: 0.0003169629149712049 50: 0.0003093158348630636 51: 0.0003023809108596582 52: 0.0002947433923299496 53: 0.00028860634909226346 54: 0.00028179665215504477 55: 0.00027597242823013894 56: 0.00027051137712521433 57: 0.0002648675336669653 58: 0.0002595099023519418 59: 0.0002543628406830323 60: 0.0002490593359256402 61: 0.0002443719846315873 62: 0.0002396430533665877 63: 0.00023537879953017602 64: 0.00023074357364422237 65: 0.00022677934895723293 66: 0.00022275162239869436 67: 0.00021877073897765234 68: 0.00021507212748894325 69: 0.0002110987806167358 70: 0.00020773537839070344 . torch.save(net.state_dict(), &#39;kaiming.pt&#39;) . actvs = {} net = net.to(&#39;cpu&#39;) images = x_valid[:10000] out = images.clone() for i in range(len(net.model)): if i in (1,3,5,7,9): out = net.model[i](out) actvs[i] = out.squeeze(0).detach().mean(axis=0).numpy() else: out = net.model[i](out) . fig, ax = plt.subplots(figsize=(15,5)) ax.set_xlim(-1, 1) for k,v in actvs.items(): sns.distplot(v, ax=ax, label=f&#39;Layer {k}&#39;, hist=False) . We can see clear improvement from the previous figure. The activation values for later layers is greater than those observed in the previous figure. This shows we made some improvement. . Initialization derivation for Mish activation . I have written a previous post on Mish activation function, you can check for details about the activation function. . In this part I will derive the initialization scheme when using Mish activation function. This is using the same method as described in the Kaiming Init paper. Deriving the initialization for Mish would provide variety on how you can use it for your own activation functions. . I will only derive the initialization formula for the forward pass. In practice we always set the initialization values so as to maintiain the mean, std for the forward pass. .",
            "url": "https://kushajveersingh.github.io/blog/2020/07/10/post-0010.html",
            "relUrl": "/2020/07/10/post-0010.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "STOC 2020: Session Notes",
            "content": "Session 3A - Walking Randomly, Massively, and Efficiently . by Jakub Lacki, Slobodan Mitrovic, Krzysztofof Onak, Piotr Sankowski video link . Overview of work . How to compute Random Walks? (in &quot;a small&quot; number of parallel steps) . Random Walks in Undirected Graphs . To generate a walk of length L from vertex v to x, we can follow the below procedure . Computer a random walk of length L/2 from $v rightarrow w$ and random walk of length L/2 from $w rightarrow x$. | Stitch the two above random walks to get the random walk of length L from $v rightarrow x$. | We can repeat the above procedure recursively by computing random walks of length L/2, L/2, ... . Now the problem is how many walks will pass through w (it follows stationary distribution). . Conclusion . Compute independent random walk of length L from each vertex . in poly log L steps undirected: $O( log L)$ rounds | directed: $O( log^2 log n+ log^21/ epsilon)$ | . | using $O(m^{1+o(1)})$ memory | . Session 7C - Does Learning Require Memorization? A Short Tale about a Long Tail . by Vitaly Feldman video link . Overview of problem . For state-of-the-art deep learning algorithms on text/images, we see this pattern . Training set error: 0-1% | (Typical) test set error: 10-30% | . This means the data distribution has a large number of points that the data distribution could not classify accurately. These misclassified points are generally outliers and misclassified labels. This same thing is true for training dataset also, which means the model is memorizing the labels for some inputs, otherwise it would not be able to achieve smaller training error rates. . An example was shown where Inception model was trained on random Imagenet labels, and yet it achieved 9% training error. Which means the model was memorizing the labels, as it is not possible to learn from random labels. . Defining label memorization . $mem(A,S,i)$ where . A = learning algorithm | S = dataset | i = $i^{th}$ example in dataset | . Memorization is defined as the difference between output of softmax of the model, first when i is part of training set and second when i is part of test set i.e. $$mem(A,S,i)=Pr(i in train)-Pr(i in test)$$ . We say an example is memorized if the above value is greater than some threshold (say 0.5). . . Tip: Memorization can be thought of as the difference between training and test error (i.e. generalization gap). If this value is large, we can say the model memorized more. . Why memorization is important? . . The four pictures in the training set are not much useful for learning what is a truck. Because in real life we would not see trucks like these. But if we memorize the first example, we get better result for the corresponding test image shown and same for the third image. . So memorization is essential in this case to perform better on the test set. But the problem is our model memorizes all the four images in the training set (some of which don&#39;t even benefit on the test set). And this is the reason why we see a difference in the training set and test set error rates. The model is memorizing useless examples. . . Note: In the above example it is assumed that test set is the real representation of real life i.e. we are not arguing that images in test set are not even real life images of truck. This is a separate problem of not making good test sets . So memorization is useful in some cases and in worst case is bad. . Conclusion . Label memorization is necessary for optimal generalization on long-tailed data distributions (not algorithm-specific). . Session 7C - Efficiently Learning Structured Distributions from Unstructured Batches . by Sitan Chen, Jerry Li, Ankur Moitra video link . Overview of problem . Inspired by Robust Learning. Can we design algorithms that can tolerate a constant fraction of corruptions in the data? These corruptions arise in . adversarial examples | data poisoning attacks on recommender systems | malware classifiers | . For this session, we deal with the problem where the data came from some crowdsource fashion. Like spell check app for mobile. We want to learn the distribution over misspellings of particular word. (This distribution is a discrete probability distribution over some words). . We have a central server where we collect the data from multiple users and aggregate the data to train our model. Now what if a constant fraction of users give adversarially chosen samples, to skew the model. We cannot distinguish between these adversarial and non-adversarial users. . We can only assume that as the number of batches increase for every user (a user will be sending multiple words to the server), the added redundancy will allow you to drive this error smaller and smaller. . In practical usage every user would have access to a some subset of the main distribution (if a person is interested in music, the words entered by him would resemble close to music, which will be different from a deep learning researcher). So our model should be able to tolerate these deviations from user to user (Federated Learning). . Session 8B - How to lose at Monte Carlo: a simple dynamical system . by C.Rojas, M.Yampolsky video link . In real life, all of the systems are non-deterministic because even the simplest model exhibit chaotic behavior (weather prediction is an example of this). Even the smallest errors in the calculation will blow up very fast so deterministic predictions are not possible. . Monte Carlo method(Non-deterministic approach) . Throw random darts to select a large number of initial values | Run the simulation for the desired duration for each of them; then statistically average the outcomes. | These averages are expected to reflect the true trajectory of the system. |",
            "url": "https://kushajveersingh.github.io/blog/notes/conference/2020/06/19/post-009.html",
            "relUrl": "/notes/conference/2020/06/19/post-009.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "STOC 2020: Session Notes",
            "content": "Session 3A - Walking Randomly, Massively, and Efficiently . by Jakub Lacki, Slobodan Mitrovic, Krzysztofof Onak, Piotr Sankowski video link . Overview of work . How to compute Random Walks? (in &quot;a small&quot; number of parallel steps) . Random Walks in Undirected Graphs . To generate a walk of length L from vertex v to x, we can follow the below procedure . Computer a random walk of length L/2 from $v rightarrow w$ and random walk of length L/2 from $w rightarrow x$. | Stitch the two above random walks to get the random walk of length L from $v rightarrow x$. | We can repeat the above procedure recursively by computing random walks of length L/2, L/2, ... . Now the problem is how many walks will pass through w (it follows stationary distribution). . Conclusion . Compute independent random walk of length L from each vertex . in poly log L steps undirected: $O( log L)$ rounds | directed: $O( log^2 log n+ log^21/ epsilon)$ | . | using $O(m^{1+o(1)})$ memory | . Session 7C - Does Learning Require Memorization? A Short Tale about a Long Tail . by Vitaly Feldman video link . Overview of problem . For state-of-the-art deep learning algorithms on text/images, we see this pattern . Training set error: 0-1% | (Typical) test set error: 10-30% | . This means the data distribution has a large number of points that the data distribution could not classify accurately. These misclassified points are generally outliers and misclassified labels. This same thing is true for training dataset also, which means the model is memorizing the labels for some inputs, otherwise it would not be able to achieve smaller training error rates. . An example was shown where Inception model was trained on random Imagenet labels, and yet it achieved 9% training error. Which means the model was memorizing the labels, as it is not possible to learn from random labels. . Defining label memorization . $mem(A,S,i)$ where . A = learning algorithm | S = dataset | i = $i^{th}$ example in dataset | . Memorization is defined as the difference between output of softmax of the model, first when i is part of training set and second when i is part of test set i.e. $$mem(A,S,i)=Pr(i in train)-Pr(i in test)$$ . We say an example is memorized if the above value is greater than some threshold (say 0.5). . . Tip: Memorization can be thought of as the difference between training and test error (i.e. generalization gap). If this value is large, we can say the model memorized more. . Why memorization is important? . . The four pictures in the training set are not much useful for learning what is a truck. Because in real life we would not see trucks like these. But if we memorize the first example, we get better result for the corresponding test image shown and same for the third image. . So memorization is essential in this case to perform better on the test set. But the problem is our model memorizes all the four images in the training set (some of which don&#39;t even benefit on the test set). And this is the reason why we see a difference in the training set and test set error rates. The model is memorizing useless examples. . . Note: In the above example it is assumed that test set is the real representation of real life i.e. we are not arguing that images in test set are not even real life images of truck. This is a separate problem of not making good test sets . So memorization is useful in some cases and in worst case is bad. . Conclusion . Label memorization is necessary for optimal generalization on long-tailed data distributions (not algorithm-specific). . Session 7C - Efficiently Learning Structured Distributions from Unstructured Batches . by Sitan Chen, Jerry Li, Ankur Moitra video link . Overview of problem . Inspired by Robust Learning. Can we design algorithms that can tolerate a constant fraction of corruptions in the data? These corruptions arise in . adversarial examples | data poisoning attacks on recommender systems | malware classifiers | . For this session, we deal with the problem where the data came from some crowdsource fashion. Like spell check app for mobile. We want to learn the distribution over misspellings of particular word. (This distribution is a discrete probability distribution over some words). . We have a central server where we collect the data from multiple users and aggregate the data to train our model. Now what if a constant fraction of users give adversarially chosen samples, to skew the model. We cannot distinguish between these adversarial and non-adversarial users. . We can only assume that as the number of batches increase for every user (a user will be sending multiple words to the server), the added redundancy will allow you to drive this error smaller and smaller. . In practical usage every user would have access to a some subset of the main distribution (if a person is interested in music, the words entered by him would resemble close to music, which will be different from a deep learning researcher). So our model should be able to tolerate these deviations from user to user (Federated Learning). . Session 8B - How to lose at Monte Carlo: a simple dynamical system . by C.Rojas, M.Yampolsky video link . In real life, all of the systems are non-deterministic because even the simplest model exhibit chaotic behavior (weather prediction is an example of this). Even the smallest errors in the calculation will blow up very fast so deterministic predictions are not possible. . Monte Carlo method(Non-deterministic approach) . Throw random darts to select a large number of initial values | Run the simulation for the desired duration for each of them; then statistically average the outcomes. | These averages are expected to reflect the true trajectory of the system. |",
            "url": "https://kushajveersingh.github.io/blog/notes/conference/2020/06/19/post-0009.html",
            "relUrl": "/notes/conference/2020/06/19/post-0009.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How to setup personal blog using Ghost and Github hosting",
            "content": ". Important: I have moved to fastpages. fastpages is the best option is you want to write jupyter notebook and share them as posts. All the details of setup are provided on the fastpages homepage. . The website looks as shown below: . . My system info . Ubuntu 20.04 LTS | Ghost 3.15.3 | Yarn 1.22.4 | Nodejs 12.16.3 | . Short summary of what we are going to do. . Install Ghost locally from source | Use default casper theme to make the website | Generate a static site using gssg | Host the static site on Github | Install Ghost locally and it&#39;s dependencies . Install NodeJS. (v12 is the recommended for Ghost). curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash sudo apt install -y nodejs . | Install Yarn. curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo &quot;deb https://dl.yarnpkg.com/debian/ stable main&quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update &amp;&amp; sudo apt install yarn . | Install Ghost from source. (The official setup guide can be found here Note: Replace KushajveerSingh with your Github username. git clone --recurse-submodules git@github.com:TryGhost/Ghost cd Ghost cd core/client cd ../../ . | This is a hack. If you follow the official setup guide then you would have forked the Ghost repo and added that as your upstream in the previous step. . I skipped that step, as I was having some problems with that. Instead I deleted the .git folder and initialized a new github repo for version control. . So go the main Ghost folder and delete .git folder. Then go to core/client and delete the .git folder and submodule file and you are done. . | Install dependencies (We are in home directory of Ghost) sudo npm install sudo npm install -g knex-migrator knex-migrator i . | Make your Ghost folder a github repo. Goto Github and create a new repo where you want to store the Ghost folder. | In the Ghost folder run these commands to push it to github. | | Create website using Ghost . Use npm start to start the Ghost server. This will open the server at http://localhost:2368. . Goto http://localhost:2368/ghost from where you can start creating your website. . Now create your website locally, and when you are done move to the next step. . Download ghost-static-site-generator . This is the tool that we will use to get a static site out of our Ghost site. You can check the official github repo of the package for more details on the usage. . To download the package run npm install -g ghost-static-site-generator. If you get errors run this command again. I ran this command twice and it worked. Maybe try sudo if it still not works. . Now you can create your static site using gssg --url=https://kushajveersingh.github.io/ and it will create a static folder in your current directory, from where you can copy the contents to your .github.io repo. . Automating the above process . To automate the complete process and ensure that my Ghost repo and .github.io repo are in sync, I created this script. . # Change git_folder, ghost_folder, url # git_folder -&gt; location of your .github.io repo # ghost_folder -&gt; location of the Ghost folder in which you are creating your site # url -&gt; The github address where your website will be published git_folder=&quot;/home/kushaj/Desktop/Github/KushajveerSingh.github.io&quot; ghost_folder=&quot;/home/kushaj/Desktop/Github/Ghost&quot; url=&quot;https://kushajveersingh.github.io/&quot; # Remove all contents of git_folder rm -r $git_folder/* # Generate static site using ghost-static-site-generator # The contents of the site are directly placed in the git_folder gssg --url $url --dest $git_folder # Commit the changes of git_folder cd $git_folder git add -A git commit -m &quot;$1&quot; git push origin master # Commit the changes of ghost_folder cd $ghost_folder git add -A git commit -m &quot;$1&quot; git push origin master . You need to change only git_folder, ghost_folder and url as per your requirements. . Usage . ./generate_script.sh &quot;initial commit&quot; . Your repositories will be pushed to Github with the provided commit message. .",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2020/05/13/post-008.html",
            "relUrl": "/tutorial/2020/05/13/post-008.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "How to setup personal blog using Ghost and Github hosting",
            "content": ". Important: I have moved to fastpages. fastpages is the best option is you want to write jupyter notebook and share them as posts. All the details of setup are provided on the fastpages homepage. . The website looks as shown below: . . My system info . Ubuntu 20.04 LTS | Ghost 3.15.3 | Yarn 1.22.4 | Nodejs 12.16.3 | . Short summary of what we are going to do. . Install Ghost locally from source | Use default casper theme to make the website | Generate a static site using gssg | Host the static site on Github | Install Ghost locally and it&#39;s dependencies . Install NodeJS. (v12 is the recommended for Ghost). curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash sudo apt install -y nodejs . | Install Yarn. curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo &quot;deb https://dl.yarnpkg.com/debian/ stable main&quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update &amp;&amp; sudo apt install yarn . | Install Ghost from source. (The official setup guide can be found here Note: Replace KushajveerSingh with your Github username. git clone --recurse-submodules git@github.com:TryGhost/Ghost cd Ghost cd core/client cd ../../ . | This is a hack. If you follow the official setup guide then you would have forked the Ghost repo and added that as your upstream in the previous step. . I skipped that step, as I was having some problems with that. Instead I deleted the .git folder and initialized a new github repo for version control. . So go the main Ghost folder and delete .git folder. Then go to core/client and delete the .git folder and submodule file and you are done. . | Install dependencies (We are in home directory of Ghost) sudo npm install sudo npm install -g knex-migrator knex-migrator i . | Make your Ghost folder a github repo. Goto Github and create a new repo where you want to store the Ghost folder. | In the Ghost folder run these commands to push it to github. | | Create website using Ghost . Use npm start to start the Ghost server. This will open the server at http://localhost:2368. . Goto http://localhost:2368/ghost from where you can start creating your website. . Now create your website locally, and when you are done move to the next step. . Download ghost-static-site-generator . This is the tool that we will use to get a static site out of our Ghost site. You can check the official github repo of the package for more details on the usage. . To download the package run npm install -g ghost-static-site-generator. If you get errors run this command again. I ran this command twice and it worked. Maybe try sudo if it still not works. . Now you can create your static site using gssg --url=https://kushajveersingh.github.io/ and it will create a static folder in your current directory, from where you can copy the contents to your .github.io repo. . Automating the above process . To automate the complete process and ensure that my Ghost repo and .github.io repo are in sync, I created this script. . # Change git_folder, ghost_folder, url # git_folder -&gt; location of your .github.io repo # ghost_folder -&gt; location of the Ghost folder in which you are creating your site # url -&gt; The github address where your website will be published git_folder=&quot;/home/kushaj/Desktop/Github/KushajveerSingh.github.io&quot; ghost_folder=&quot;/home/kushaj/Desktop/Github/Ghost&quot; url=&quot;https://kushajveersingh.github.io/&quot; # Remove all contents of git_folder rm -r $git_folder/* # Generate static site using ghost-static-site-generator # The contents of the site are directly placed in the git_folder gssg --url $url --dest $git_folder # Commit the changes of git_folder cd $git_folder git add -A git commit -m &quot;$1&quot; git push origin master # Commit the changes of ghost_folder cd $ghost_folder git add -A git commit -m &quot;$1&quot; git push origin master . You need to change only git_folder, ghost_folder and url as per your requirements. . Usage . ./generate_script.sh &quot;initial commit&quot; . Your repositories will be pushed to Github with the provided commit message. .",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2020/05/13/post-0008.html",
            "relUrl": "/tutorial/2020/05/13/post-0008.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Study of Mish activation function in transfer learning with code and discussion",
            "content": "Link to jupyter notebook, paper, fastai discussion thread . Mish activation function is proposed in Mish: A Self Regularized Non-Monotonic Neural Activation Function paper. The experiments conducted in the paper shows it achieves better accuracy than ReLU. Also, many experiments have been conducted by the fastai community and they were also able to achieve better results than ReLU. . Mish is defined as x * tanh(softplus(x)) or by this equation $x* tanh ( ln (1+e^x))$. . PyTorch implementation . # collapse-show class Mish(nn.Module): r&quot;&quot;&quot; Mish activation function is proposed in &quot;Mish: A Self Regularized Non-Monotonic Neural Activation Function&quot; paper, https://arxiv.org/abs/1908.08681. &quot;&quot;&quot; def __init__(self): super().__init__() def forward(self, x): return x * torch.tanh(F.softplus(x)) . . Plot mish function . To build upon this activation function let’s first see the plot of the function. . # collapse-show x = np.linspace(-7, 7, 700) y = x * np.tanh(np.log(1 + np.exp(x))) fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.spines[&#39;left&#39;].set_position(&#39;center&#39;) ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) plt.plot(x, y, &#39;b&#39;) plt.savefig(fname=&#39;/home/kushaj/Desktop/Temp/SOTA/images/mish_plot.png&#39;, dpi=1200) . . . Properties of mish . Unbounded Above:- Being unbounded above is a desired property of an activation function as it avoids saturation which causes training to slow down to near-zero gradients. | Bounded Below:- Being bounded below is desired because it results in strong regularization effects. | Non-monotonic:- This is the important factor in mish. We preserve small negative gradients and this allows the network to learn better and it also improves the gradient flow in the negative region as, unlike ReLU where all negative gradients become zero. | Continuity:- Mish’s first derivative is continuous over the entire domain which helps in effective optimization and generalization. Unlike ReLU which is discontinuous at zero. | To compute the first derivative expand the tanh(softplus(x)) term and you will get the following term and then do product rule of the derivative. . $$y=x* frac{e^{2x}+2e^x}{e^{2x}+2e^x+2}$$ . When using Mish against ReLU use a lower learning rate in the case of Mish. Range of around 1e-5 to 1e-1 showed good results. . Testing Mish against ReLU . Rather than training from scratch which is already done in the paper, I would test for transfer learning. When we use pretrained models for our own dataset we keep the CNN filter weights the same (we update them during finetuning) but we initialize the last fully-connected layers randomly (head of the model). So I would test for using ReLU and Mish in these fully-connected layers. . . Note: I would be using OneCycle training. In case you are unfamiliar with the training technique that I would use here, I have written a complete notebook summarizing them in fastai. You can check the notebook here. . I use CIFAR10 and CIFAR100 dataset to test a pretrained Resnet50 model. I would run the model for 10 epochs and then compare the results at the fifth and tenth epoch. Also, the results would be averaged across 3 runs using different learning rates (1e-2, 5e-3, 1e-3). The weighs of the CNN filters would not be updated, only the fully connected layers would be updated/trained. . For the fully connected layers, I would use the following architecture. In case of Mish, replace the ReLU with Mish. . # collapse-show # AdaptiveConcatPool2d is just combining AdaptiveAvgPool and AdaptiveMaxPool. head = nn.Sequential( AdaptiveConcatPool2d(), Flatten(), nn.BatchNorm1d(4096), nn.Dropout(p=0.25), nn.Linear(in_features=4096, out_features=512), nn.ReLU(inplace=True), nn.BatchNorm1d(512), nn.Dropout(p=0.5), nn.Linear(in_features=512, out_features=10) ) . . The final results are shown below. It was observed that Mish required training with a smaller learning rate otherwise it overfits quickly, thus suggesting that it requires stronger regularization than ReLU. It was consistent across multiple runs. Generally, you can get away with using a higher learning rate in the case of ReLU but when using Mish a higher learning rate always lead to overfitting. . Although the results are quite similar but by using Mish we can see some marginal improvements. This is a very limited test as only one Mish activation is used in the entire network and also the network has been run for only 10 epochs. . Visualization of output landscape . We would use a 5 layer randomly initialized fully connected neural network to visualize the output landscape of ReLU and Mish. The code and the results are given below. . # collapse-show from sklearn.preprocessing import MinMaxScaler from PIL import Image # The following code has been taken from # https://github.com/digantamisra98/Mish/blob/master/output_landscape.py def get_model(act_fn=&#39;relu&#39;): if act_fn is &#39;relu&#39;: fn = nn.ReLU(inplace=True) if act_fn is &#39;mish&#39;: fn = Mish() model = nn.Sequential( nn.Linear(2, 64), fn, nn.Linear(64, 32), fn, nn.Linear(32, 16), fn, nn.Linear(16, 1), fn ) return model # Main code relu_model = get_model(&#39;relu&#39;) mish_model = get_model(&#39;mish&#39;) x = np.linspace(0., 10., 100) y = np.linspace(0., 10., 100) grid = [torch.tensor([xi, yi]) for xi in x for yi in y] np_img_relu = np.array([relu_model(point).detach().numpy() for point in grid]).reshape(100, 100) np_img_mish = np.array([mish_model(point).detach().numpy() for point in grid]).reshape(100, 100) scaler = MinMaxScaler(feature_range=(0, 255)) np_img_relu = scaler.fit_transform(np_img_relu) np_img_mish = scaler.fit_transform(np_img_mish) plt.imsave(&#39;relu_land.png&#39;, np_img_relu) plt.imsave(&#39;mish_land.png&#39;, np_img_mish) . . From the above output landscapes, we can observe that the mish produces a smoother output landscape thus resulting is smoother loss functions which are easier to optimize and thus the network generalizes better. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/11/11/post-007.html",
            "relUrl": "/paper-implementation/2019/11/11/post-007.html",
            "date": " • Nov 11, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Study of Mish activation function in transfer learning with code and discussion",
            "content": "Link to jupyter notebook, paper, fastai discussion thread . Mish activation function is proposed in Mish: A Self Regularized Non-Monotonic Neural Activation Function paper. The experiments conducted in the paper shows it achieves better accuracy than ReLU. Also, many experiments have been conducted by the fastai community and they were also able to achieve better results than ReLU. . Mish is defined as x * tanh(softplus(x)) or by this equation $x* tanh ( ln (1+e^x))$. . PyTorch implementation . # collapse-show class Mish(nn.Module): r&quot;&quot;&quot; Mish activation function is proposed in &quot;Mish: A Self Regularized Non-Monotonic Neural Activation Function&quot; paper, https://arxiv.org/abs/1908.08681. &quot;&quot;&quot; def __init__(self): super().__init__() def forward(self, x): return x * torch.tanh(F.softplus(x)) . . Plot mish function . To build upon this activation function let’s first see the plot of the function. . # collapse-show x = np.linspace(-7, 7, 700) y = x * np.tanh(np.log(1 + np.exp(x))) fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.spines[&#39;left&#39;].set_position(&#39;center&#39;) ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) plt.plot(x, y, &#39;b&#39;) plt.savefig(fname=&#39;/home/kushaj/Desktop/Temp/SOTA/images/mish_plot.png&#39;, dpi=1200) . . . Properties of mish . Unbounded Above:- Being unbounded above is a desired property of an activation function as it avoids saturation which causes training to slow down to near-zero gradients. | Bounded Below:- Being bounded below is desired because it results in strong regularization effects. | Non-monotonic:- This is the important factor in mish. We preserve small negative gradients and this allows the network to learn better and it also improves the gradient flow in the negative region as, unlike ReLU where all negative gradients become zero. | Continuity:- Mish’s first derivative is continuous over the entire domain which helps in effective optimization and generalization. Unlike ReLU which is discontinuous at zero. | To compute the first derivative expand the tanh(softplus(x)) term and you will get the following term and then do product rule of the derivative. . $$y=x* frac{e^{2x}+2e^x}{e^{2x}+2e^x+2}$$ . When using Mish against ReLU use a lower learning rate in the case of Mish. Range of around 1e-5 to 1e-1 showed good results. . Testing Mish against ReLU . Rather than training from scratch which is already done in the paper, I would test for transfer learning. When we use pretrained models for our own dataset we keep the CNN filter weights the same (we update them during finetuning) but we initialize the last fully-connected layers randomly (head of the model). So I would test for using ReLU and Mish in these fully-connected layers. . . Note: I would be using OneCycle training. In case you are unfamiliar with the training technique that I would use here, I have written a complete notebook summarizing them in fastai. You can check the notebook here. . I use CIFAR10 and CIFAR100 dataset to test a pretrained Resnet50 model. I would run the model for 10 epochs and then compare the results at the fifth and tenth epoch. Also, the results would be averaged across 3 runs using different learning rates (1e-2, 5e-3, 1e-3). The weighs of the CNN filters would not be updated, only the fully connected layers would be updated/trained. . For the fully connected layers, I would use the following architecture. In case of Mish, replace the ReLU with Mish. . # collapse-show # AdaptiveConcatPool2d is just combining AdaptiveAvgPool and AdaptiveMaxPool. head = nn.Sequential( AdaptiveConcatPool2d(), Flatten(), nn.BatchNorm1d(4096), nn.Dropout(p=0.25), nn.Linear(in_features=4096, out_features=512), nn.ReLU(inplace=True), nn.BatchNorm1d(512), nn.Dropout(p=0.5), nn.Linear(in_features=512, out_features=10) ) . . The final results are shown below. It was observed that Mish required training with a smaller learning rate otherwise it overfits quickly, thus suggesting that it requires stronger regularization than ReLU. It was consistent across multiple runs. Generally, you can get away with using a higher learning rate in the case of ReLU but when using Mish a higher learning rate always lead to overfitting. . Although the results are quite similar but by using Mish we can see some marginal improvements. This is a very limited test as only one Mish activation is used in the entire network and also the network has been run for only 10 epochs. . Visualization of output landscape . We would use a 5 layer randomly initialized fully connected neural network to visualize the output landscape of ReLU and Mish. The code and the results are given below. . # collapse-show from sklearn.preprocessing import MinMaxScaler from PIL import Image # The following code has been taken from # https://github.com/digantamisra98/Mish/blob/master/output_landscape.py def get_model(act_fn=&#39;relu&#39;): if act_fn is &#39;relu&#39;: fn = nn.ReLU(inplace=True) if act_fn is &#39;mish&#39;: fn = Mish() model = nn.Sequential( nn.Linear(2, 64), fn, nn.Linear(64, 32), fn, nn.Linear(32, 16), fn, nn.Linear(16, 1), fn ) return model # Main code relu_model = get_model(&#39;relu&#39;) mish_model = get_model(&#39;mish&#39;) x = np.linspace(0., 10., 100) y = np.linspace(0., 10., 100) grid = [torch.tensor([xi, yi]) for xi in x for yi in y] np_img_relu = np.array([relu_model(point).detach().numpy() for point in grid]).reshape(100, 100) np_img_mish = np.array([mish_model(point).detach().numpy() for point in grid]).reshape(100, 100) scaler = MinMaxScaler(feature_range=(0, 255)) np_img_relu = scaler.fit_transform(np_img_relu) np_img_mish = scaler.fit_transform(np_img_mish) plt.imsave(&#39;relu_land.png&#39;, np_img_relu) plt.imsave(&#39;mish_land.png&#39;, np_img_mish) . . From the above output landscapes, we can observe that the mish produces a smoother output landscape thus resulting is smoother loss functions which are easier to optimize and thus the network generalizes better. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/11/11/post-0007.html",
            "relUrl": "/paper-implementation/2019/11/11/post-0007.html",
            "date": " • Nov 11, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "How to become an expert in NLP in 2019",
            "content": "In this post, I would focus on all of the theoretical knowledge you need for the latest trends in NLP. I made this reading list as I learned new concepts. For the resources, I include papers, blogs, videos. . It is not necessary to read most of the stuff. Your main goal should be to understand that in this paper this thing was introduced and do I understand how it works, how it compares it with state of the art. . . Trend: Use bigger transformer based models and solve multi-task learning. . . Warning: Warning: It is an increasing trend in NLP that if you have a new idea in NLP during reading any of the papers, you will have to use massive compute power to get any reasonable results. So you are limited by the open-source models. . fastai:- I had already watched the videos, so I thought I should add it to the top of the list. . Lesson 4 Practical Deep Learning for Coders. It will get you up with how to implement a language model in fastai. | Lesson 12 Deep Learning from the Foundations. Goes further into ULMFit training. | . | LSTM:- Although transformers are mainly used nowadays, in some cases you can still use LSTM and it was the first successful model to get good results. You should use AWD_LSTM now if you want. . Long Short-Term Memory paper. A quick skim of the paper is sufficient. | Understanding LSTM Networks blog. It explains all the details of the LSTM network graphically. | . | AWD_LSTM:- It was proposed to overcome the shortcoming of LSTM by introducing dropout between hidden layers, embedding dropout, weight tying. You should use AWS_LSTM instead of LSTM. . Regularizing and Optimizing LSTM Language Models paper. AWD_LSTM paper | Official code by Salesforce | fastai implementation | . | Pointer Models:- Although not necessary, it is a good read. You can think of it as pre-attention theory. . Pointer Sentinel Mixture Models paper | Official video of above paper. | Improving Neural Language Models with a continuous cache paper | . | . Tip: What is the difference between weight decay and regularization? In weight decay, you directly add something to the update rule while in regularization it is added to the loss function. Why bring this up? Most probably the DL libraries are using weight_decay instead of regularization under the hood. . . Note: In some of the papers, you would see that the authors preferred SGD over Adam, citing that Adam does not give good performance. The reason for that is (maybe) PyTorch/Tensorflow are doing the above mistake. This thing is explained in detail in this post. . Attention:- Remember Attention is not all you need. CS224n video explaining attention. Attention starts from 1:00:55 hours. | Attention is all you need paper. This paper also introduces the Transformer which is nothing but a stack of encoder and decoder blocks. The magic is how these blocks are made and connected. | Read an annotated version of the above paper in PyTorch. | Official video explaining Attention | Google blog for Transformer | If you are interested in video you can check these link1, link2. | Transformer-XL: Attentive Language Models Beyond a Fixed Length Context paper. Better version of Transformer but BERT does not use this. | Google blog for Transformer-XL | Transformer-XL — Combining Transformers and RNNs Into a State-of-the-art Language Model blog | For video check this link. | The Illustrated Transformer blog | Attention and Memory in Deep Learning and NLP blog. | Attention and Augmented Recurrent Neural Networks blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 1 blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 2 blog. | . | There is a lot of research going on to make better transformers, maybe I will read more papers on this in the future. Some other transformers include the Universal Transformer and Evolved Transformer which used AutoML to come up with Transformer architecture. . Random resources . Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) [blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_ | Character-Level Language Modeling with Deeper Self-Attention paper. | Using the output embedding to Improve Langauge Models paper. | Quasi-Recurrent Neural Networks paper. A very fast version of LSTM. It uses convolution layers to make LSTM computations parallel. Code can be found in the fastai_library or official_code. | Deep Learning for NLP Best Practices blog by Sebastian Ruder. A collection of best practices to be used when training LSTM models. | Notes on the state of the art techniques for language modeling blog. A quick summary where Jeremy Howard summarizes some of his tricks which he uses in fastai library. | Language Modes and Contextualized Word Embeddings blog. Gives a quick overview of ELMo, BERT, and other models. | The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) blog. | . | Multi-task Learning:- I am really excited about this. In this case, you train a single model for multiple tasks (more than 10 if you want). So your data looks like “translate to english some_text_in_german”. Your model actually learns to use the initial information to choose the task that it should perform. . An overview of Multi-Task Learning in deep neural networks paper. | The Natural Language Decathlon: Multitask Learning as Question Answering paper. | Multi-Task Deep Neural Networks for Natural Language Understanding paper. | OpenAI GPT is an example of this. | . | PyTorch:- Pytorch provide good tutorials giving you good references on how to code up most of the stuff in NLP. . | ELMo:- The first prominent research done where we moved from pretrained word-embeddings to using pretrained-models for getting the word-embeddings. So you use the input sentence to get the embeddings for the tokens present in the sentence. . Deep Contextualized word representations paper, video | . | ULMFit:- Is this better than BERT maybe not, but still in Kaggle competitions and external competitions ULMFiT gets the first place. . Universal Language Model Fine-tuning for Text Classification paper. | Jeremy Howard blog post announcing ULMFiT. | . | OpenAI GPT:- I have not compared BERT with GPT2, but you work on some kind on ensemble if you want. Do not use GPT1 as BERT was made to overcome the limitations of GPT1. . GPT1 paper, blog, code | GPT2 paper, blog, code | Check video by openai on GPT2 | . | BERT:- The most successful language model right now (as of May 2019). . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding paper. | Google blog on BERT | Dissecting BERT Part 1: The Encoder blog | Understanding BERT Part 2: BERT Specifics blog | Dissecting BERT Appendix: The Decoder blog | . | To use all these models in PyTorch/Tensorflow you can use hugginface/transformers which gives complete implementations along with pretrained models for BERT, GPT1, GPT2, TransformerXL. . Congrats you made it to the end. You now have most of the theoretical knowledge needed to practice NLP using the latest models and techniques. . What to do now? You only learned the theory, now practice as much as you can. .",
            "url": "https://kushajveersingh.github.io/blog/notes/2019/05/15/post-006.html",
            "relUrl": "/notes/2019/05/15/post-006.html",
            "date": " • May 15, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "How to become an expert in NLP in 2019",
            "content": "In this post, I would focus on all of the theoretical knowledge you need for the latest trends in NLP. I made this reading list as I learned new concepts. For the resources, I include papers, blogs, videos. . It is not necessary to read most of the stuff. Your main goal should be to understand that in this paper this thing was introduced and do I understand how it works, how it compares it with state of the art. . . Trend: Use bigger transformer based models and solve multi-task learning. . . Warning: Warning: It is an increasing trend in NLP that if you have a new idea in NLP during reading any of the papers, you will have to use massive compute power to get any reasonable results. So you are limited by the open-source models. . fastai:- I had already watched the videos, so I thought I should add it to the top of the list. . Lesson 4 Practical Deep Learning for Coders. It will get you up with how to implement a language model in fastai. | Lesson 12 Deep Learning from the Foundations. Goes further into ULMFit training. | . | LSTM:- Although transformers are mainly used nowadays, in some cases you can still use LSTM and it was the first successful model to get good results. You should use AWD_LSTM now if you want. . Long Short-Term Memory paper. A quick skim of the paper is sufficient. | Understanding LSTM Networks blog. It explains all the details of the LSTM network graphically. | . | AWD_LSTM:- It was proposed to overcome the shortcoming of LSTM by introducing dropout between hidden layers, embedding dropout, weight tying. You should use AWS_LSTM instead of LSTM. . Regularizing and Optimizing LSTM Language Models paper. AWD_LSTM paper | Official code by Salesforce | fastai implementation | . | Pointer Models:- Although not necessary, it is a good read. You can think of it as pre-attention theory. . Pointer Sentinel Mixture Models paper | Official video of above paper. | Improving Neural Language Models with a continuous cache paper | . | . Tip: What is the difference between weight decay and regularization? In weight decay, you directly add something to the update rule while in regularization it is added to the loss function. Why bring this up? Most probably the DL libraries are using weight_decay instead of regularization under the hood. . . Note: In some of the papers, you would see that the authors preferred SGD over Adam, citing that Adam does not give good performance. The reason for that is (maybe) PyTorch/Tensorflow are doing the above mistake. This thing is explained in detail in this post. . Attention:- Remember Attention is not all you need. CS224n video explaining attention. Attention starts from 1:00:55 hours. | Attention is all you need paper. This paper also introduces the Transformer which is nothing but a stack of encoder and decoder blocks. The magic is how these blocks are made and connected. | Read an annotated version of the above paper in PyTorch. | Official video explaining Attention | Google blog for Transformer | If you are interested in video you can check these link1, link2. | Transformer-XL: Attentive Language Models Beyond a Fixed Length Context paper. Better version of Transformer but BERT does not use this. | Google blog for Transformer-XL | Transformer-XL — Combining Transformers and RNNs Into a State-of-the-art Language Model blog | For video check this link. | The Illustrated Transformer blog | Attention and Memory in Deep Learning and NLP blog. | Attention and Augmented Recurrent Neural Networks blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 1 blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 2 blog. | . | There is a lot of research going on to make better transformers, maybe I will read more papers on this in the future. Some other transformers include the Universal Transformer and Evolved Transformer which used AutoML to come up with Transformer architecture. . Random resources . Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) [blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_ | Character-Level Language Modeling with Deeper Self-Attention paper. | Using the output embedding to Improve Langauge Models paper. | Quasi-Recurrent Neural Networks paper. A very fast version of LSTM. It uses convolution layers to make LSTM computations parallel. Code can be found in the fastai_library or official_code. | Deep Learning for NLP Best Practices blog by Sebastian Ruder. A collection of best practices to be used when training LSTM models. | Notes on the state of the art techniques for language modeling blog. A quick summary where Jeremy Howard summarizes some of his tricks which he uses in fastai library. | Language Modes and Contextualized Word Embeddings blog. Gives a quick overview of ELMo, BERT, and other models. | The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) blog. | . | Multi-task Learning:- I am really excited about this. In this case, you train a single model for multiple tasks (more than 10 if you want). So your data looks like “translate to english some_text_in_german”. Your model actually learns to use the initial information to choose the task that it should perform. . An overview of Multi-Task Learning in deep neural networks paper. | The Natural Language Decathlon: Multitask Learning as Question Answering paper. | Multi-Task Deep Neural Networks for Natural Language Understanding paper. | OpenAI GPT is an example of this. | . | PyTorch:- Pytorch provide good tutorials giving you good references on how to code up most of the stuff in NLP. . | ELMo:- The first prominent research done where we moved from pretrained word-embeddings to using pretrained-models for getting the word-embeddings. So you use the input sentence to get the embeddings for the tokens present in the sentence. . Deep Contextualized word representations paper, video | . | ULMFit:- Is this better than BERT maybe not, but still in Kaggle competitions and external competitions ULMFiT gets the first place. . Universal Language Model Fine-tuning for Text Classification paper. | Jeremy Howard blog post announcing ULMFiT. | . | OpenAI GPT:- I have not compared BERT with GPT2, but you work on some kind on ensemble if you want. Do not use GPT1 as BERT was made to overcome the limitations of GPT1. . GPT1 paper, blog, code | GPT2 paper, blog, code | Check video by openai on GPT2 | . | BERT:- The most successful language model right now (as of May 2019). . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding paper. | Google blog on BERT | Dissecting BERT Part 1: The Encoder blog | Understanding BERT Part 2: BERT Specifics blog | Dissecting BERT Appendix: The Decoder blog | . | To use all these models in PyTorch/Tensorflow you can use hugginface/transformers which gives complete implementations along with pretrained models for BERT, GPT1, GPT2, TransformerXL. . Congrats you made it to the end. You now have most of the theoretical knowledge needed to practice NLP using the latest models and techniques. . What to do now? You only learned the theory, now practice as much as you can. .",
            "url": "https://kushajveersingh.github.io/blog/notes/2019/05/15/post-0006.html",
            "relUrl": "/notes/2019/05/15/post-0006.html",
            "date": " • May 15, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "All you need for Photorealistic Style Transfer in PyTorch",
            "content": "Link to jupyter notebook, paper . What is style transfer? . We have two images as input one is content image and the other is style image. . . Our aim is to transfer the style from style image to the content image. This looks something like this. . . Why another paper? . Earlier work on style transfer although successful was not able to maintain the structure of the content image. For instance, see Fig2 and then see the original content image in Fig1. As you can see the curves and structure of the content image are not distorted and the output image has the same structure as content image. . . Gram Matrix . The main idea behind the paper is using Gram Matrix for style transfer. It was shown in these 2 papers that Gram Matrix in feature map of convolutional neural network (CNN) can represent the style of an image and propose the neural style transfer algorithm for image stylization. . Texture Synthesis Using Convolution Neural Networks by Gatys et al. 2015 | Image Style Transfer Using Convolutional Neural Networks by Gatys et al. 2016 | Details about gram matrix can be found on wikipedia. Mathematically, given a vector V gram matrix is computed as $$G=V^TV$$ . High-Resolution Models . It is a recent research paper accepted at CVPR 2019 paper. So generally what happens in CNNs is we first decrease the image size while increasing the number of filters and then increase the size of the image back to the original size. . Now this forces our model to generate output images from a very small resolution and this results in loss of finer details and structure. To counter this fact High-Res model was introduced. . High-resolution network is designed to maintain high-resolution representations through the whole process and continuously receive information from low-resolution networks. So we train our models on the original resolution. . Example of this model would be covered below. You can refer to the original papers for more details on this. I will cover this topic in detail in my next week blog post. . Style transfer details . The general architecture of modern deep learning style transfer algorithms looks something like this. . . There are three things that style transfer model needs . Generating model:- It would generate the output images. In Fig4 this is ‘Hi-Res Generation Network’ | Loss function:- Correct choice of loss functions is very important in case you want to achieve good results. | Loss Network:- You need a CNN model that is pretrained and can extract good features from the images. In our case, it is VGG19 pretrained on ImageNet. | So we load VGG model. The complete code is available at my GitHub repo. . # collapse-show if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: raise Exception(&#39;GPU is not available&#39;) # Load VGG19 features. We do not need the last linear layers, # only CNN layers are needed vgg = vgg19(pretrained=True).features vgg = vgg.to(device) # We don&#39;t want to train VGG for param in vgg.parameters(): param.requires_grad_(False) torch.backends.cudnn.benchmark = True . . Next we load our images from disk. My images are stored as src/imgs/content.png and src/imgs/style.png. . # collapse-show content_img = load_image(os.path.join(args.img_root, args.content_img), size=500) content_img = content_img.to(device) style_img = load_image(os.path.join(args.img_root, args.style_img)) style_img = style_img.to(device) # Show content and style image fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10)) ax1.imshow(im_convert(content_img)) ax2.imshow(im_convert(style_img)) plt.show() # Utility functions def im_convert(img): &quot;&quot;&quot; Convert img from pytorch tensor to numpy array, so we can plot it. It follows the standard method of denormalizing the img and clipping the outputs Input: img :- (batch, channel, height, width) Output: img :- (height, width, channel) &quot;&quot;&quot; img = img.to(&#39;cpu&#39;).clone().detach() img = img.numpy().squeeze(0) img = img.transpose(1, 2, 0) img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) img = img.clip(0, 1) return img def load_image(path, size=None): &quot;&quot;&quot; Resize img to size, size should be int and also normalize the image using imagenet_stats &quot;&quot;&quot; img = Image.open(path) if size is not None: img = img.resize((size, size)) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) img = transform(img).unsqueeze(0) return img . . Detail:- When we load our images, what sizes should we use? Your content image size should be divisible by 4, as our model would downsample images 2 times. For style images, do not resize them. Use their original resolution. Size of content image is (500x500x3) and size of style image is (800x800x3). . Hi-Res Generation Network . . The model is quite simple we start with 500x500x3 images and maintain this resolution for the complete model. We downsample to 250x250 and 125x125 and then fuse these back together with 500x500 images. . Details:- . No pooling is used (as pooling causes loss of information). Instead strided convolution (i.e. stride=2) are used. | No dropout is used. But if you need regularization you can use weight decay. | 3x3 conv kernels are used everywhere with padding=1. | Zero padding is only used. Reflex padding was tested but the results were not good. | For upsampling,’bilinear’ mode is used. | For downsampling, conv layers are used. | InstanceNorm is used. | # collapse-show # Downsampling function def conv_down(in_c, out_c, stride=2): return nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1) # Upsampling function def upsample(input, scale_factor): return F.interpolate(input=input, scale_factor=scale_factor, mode=&#39;bilinear&#39;, align_corners=False) . . Implementation code . Residual connections are used between every block. We use BottleNeck layer from the ResNet architecture. (In Fig5 all the horizontal arrows are bottleneck layers). . Refresher on bottleneck layer. . . # collapse-show # Helper class for BottleneckBlock class ConvLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1): super().__init__() # We have to keep the size of images same, so choose padding accordingly num_pad = int(np.floor(kernel_size / 2)) self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=num_pad) def forward(self, x): return self.conv(x) class BottleneckBlock(nn.Module): &quot;&quot;&quot; Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used instead of BatchNorm because when we want to generate images, we normalize all the images independently. (In batch norm you compute mean and std over complete batch, while in instance norm you compute mean and std for each image channel independently). The reason for doing this is, the generated images are independent of each other, so we should not normalize them using a common statistic. If you confused about the bottleneck architecture refer to the official pytorch resnet implementation and paper. &quot;&quot;&quot; def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): super().__init__() self.in_c = in_channels self.out_c = out_channels self.identity_block = nn.Sequential( ConvLayer(in_channels, out_channels//4, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels//4, kernel_size, stride=stride), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels), nn.ReLU(), ) self.shortcut = nn.Sequential( ConvLayer(in_channels, out_channels, 1, stride), nn.InstanceNorm2d(out_channels), ) def forward(self, x): out = self.identity_block(x) if self.in_c == self.out_c: residual = x else: residual = self.shortcut(x) out += residual out = F.relu(out) return out . . Now we are ready to implement our style_transfer model, which we call HRNet (based on the paper). Use the Fig5 as reference. . # collapse-show class HRNet(nn.Module): &quot;&quot;&quot; For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf. Naming convention used. I refer to vertical layers as a single layer, so from left to right we have 8 layers excluding the input image. E.g. layer 1 contains the 500x500x16 block layer 2 contains 500x500x32 and 250x250x32 blocks and so on self.layer{x}_{y}: x :- the layer number, as explained above y :- the index number for that function starting from 1. So if layer 3 has two downsample functions I write them as `downsample3_1`, `downsample3_2` &quot;&quot;&quot; def __init__(self): super().__init__() self.layer1_1 = BottleneckBlock(3, 16) self.layer2_1 = BottleneckBlock(16, 32) self.downsample2_1 = conv_down(16, 32) self.layer3_1 = BottleneckBlock(32, 32) self.layer3_2 = BottleneckBlock(32, 32) self.downsample3_1 = conv_down(32, 32) self.downsample3_2 = conv_down(32, 32, stride=4) self.downsample3_3 = conv_down(32, 32) self.layer4_1 = BottleneckBlock(64, 64) self.layer5_1 = BottleneckBlock(192, 64) self.layer6_1 = BottleneckBlock(64, 32) self.layer7_1 = BottleneckBlock(32, 16) self.layer8_1 = conv_down(16, 3, stride=1) # Needed conv layer so reused conv_down function def forward(self, x): map1_1 = self.layer1_1(x) map2_1 = self.layer2_1(map1_1) map2_2 = self.downsample2_1(map1_1) map3_1 = torch.cat((self.layer3_1(map2_1), upsample(map2_2, 2)), 1) map3_2 = torch.cat((self.downsample3_1(map2_1), self.layer3_2(map2_2)), 1) map3_3 = torch.cat((self.downsample3_2(map2_1), self.downsample3_3(map2_2)), 1) map4_1 = torch.cat((self.layer4_1(map3_1), upsample(map3_2, 2), upsample(map3_3, 4)), 1) out = self.layer5_1(map4_1) out = self.layer6_1(out) out = self.layer7_1(out) out = self.layer8_1(out) return out . . Loss functions . In style transfer we use feature extraction, to calculate the value of losses. Feature extraction put in simple terms, means you take a pretrained imagenet model and pass your images through it and store the intermediate layer outputs. Generally, VGG model is used for such tasks. . . So you take the outputs from the conv layers. Like for the above fig, you can take the output from the second 3x3 conv 64 layer and then 3x3 conv 128. . To extract features from VGG we use the following code. . # collapse-show def get_features(img, model, layers=None): &quot;&quot;&quot; Use VGG19 to extract features from the intermediate layers. &quot;&quot;&quot; if layers is None: layers = { &#39;0&#39; : &#39;conv1_1&#39;, # style layer &#39;5&#39; : &#39;conv2_1&#39;, # style layer &#39;10&#39;: &#39;conv3_1&#39;, # style layer &#39;19&#39;: &#39;conv4_1&#39;, # style layer &#39;28&#39;: &#39;conv5_1&#39;, # style layer &#39;21&#39;: &#39;conv4_2&#39; # content layer } features = {} x = img for name, layer in model._modules.items(): x = layer(x) if name in layers: features[layers[name]] = x return features . . We use 5 layers in total for feature extraction. Only conv4_2 is used as layer for content loss. . Refer to Fig4, we pass our output image from HRNet and the original content and style image through VGG. . There are two losses . Content Loss | Style Loss | Content Loss . Content image and the output image should have a similar feature representation as computed by loss network VGG. Because we are only changing the style without any changes to the structure of the image. For the content loss, we use Euclidean distance as shown by the formula . $$l_{content}^{ phi,j}(y, hat{y})= frac{1}{C_jJ_jW_j} left | phi_j( hat{y}= phi_j(y) right |^2$$ . $ phi_j$ means we are referring to the activations of the j-th layer of loss network. In code it looks like this. . style_net = HRNet().to(device) . target = style_net(content_img).to(device) target.requiresgrad(True) . target_features = get_features(target, vgg) content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;]) ** 2) . Style Loss . We use gram matrix for this. So style of an image is given by its gram matrix. Our aim is to make style of two images close, so we compute the difference of gram matrix of style image and output image and then take their Frobenius norm. . $$l_{style}^{ phi,j}(y, hat{y})= left |G_j^{ phi}(y)-G_j^{ phi}( hat{y}) right |^2$$ . # collapse-show def get_gram_matrix(img): &quot;&quot;&quot; Compute the gram matrix by converting to 2D tensor and doing dot product img: (batch, channel, height, width) &quot;&quot;&quot; b, c, h, w = img.size() img = img.view(b*c, h*w) gram = torch.mm(img, img.t()) return gram # There are 5 layers, and we compute style loss for each layer and sum them up style_loss = 0 for layer in layers: target_gram_matrix = get_gram_matrix(target_feature) # we already computed gram matrix for our style image style_gram_matrix = style_gram_matrixs[layer] layer_style_loss = style_weights[layer] * torch.mean((target_gram_matrix - style_gram_matrix) ** 2) b, c, h, w = target_feature.shape style_loss += layer_style_loss / (c*h*w) . . Difficult part . To compute our final losses, we multiply them with some weights. . content_loss = content_weight * content_loss style_loss = style_weight * style_loss . The difficulty comes in setting these values. If you want some desired output, then you would have to test different values before you get your desired result. . To build your own intuitions you can choose two images and try different range of values. I am working on providing like a summary of this. It will be available in my repo README. . Paper recommends content_weight = [50, 100] and style_weight = [1, 10]. . Conclusion . Well, congratulation made it to the end. You can now implement style transfer. Now read the paper for more details on style transfer. . Check out my repo README, it will contain the complete instructions on how to use the code in the repo, along with complete steps on how to train your model. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/05/06/post-005.html",
            "relUrl": "/paper-implementation/2019/05/06/post-005.html",
            "date": " • May 6, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "All you need for Photorealistic Style Transfer in PyTorch",
            "content": "Link to jupyter notebook, paper . What is style transfer? . We have two images as input one is content image and the other is style image. . . Our aim is to transfer the style from style image to the content image. This looks something like this. . . Why another paper? . Earlier work on style transfer although successful was not able to maintain the structure of the content image. For instance, see Fig2 and then see the original content image in Fig1. As you can see the curves and structure of the content image are not distorted and the output image has the same structure as content image. . . Gram Matrix . The main idea behind the paper is using Gram Matrix for style transfer. It was shown in these 2 papers that Gram Matrix in feature map of convolutional neural network (CNN) can represent the style of an image and propose the neural style transfer algorithm for image stylization. . Texture Synthesis Using Convolution Neural Networks by Gatys et al. 2015 | Image Style Transfer Using Convolutional Neural Networks by Gatys et al. 2016 | Details about gram matrix can be found on wikipedia. Mathematically, given a vector V gram matrix is computed as $$G=V^TV$$ . High-Resolution Models . It is a recent research paper accepted at CVPR 2019 paper. So generally what happens in CNNs is we first decrease the image size while increasing the number of filters and then increase the size of the image back to the original size. . Now this forces our model to generate output images from a very small resolution and this results in loss of finer details and structure. To counter this fact High-Res model was introduced. . High-resolution network is designed to maintain high-resolution representations through the whole process and continuously receive information from low-resolution networks. So we train our models on the original resolution. . Example of this model would be covered below. You can refer to the original papers for more details on this. I will cover this topic in detail in my next week blog post. . Style transfer details . The general architecture of modern deep learning style transfer algorithms looks something like this. . . There are three things that style transfer model needs . Generating model:- It would generate the output images. In Fig4 this is ‘Hi-Res Generation Network’ | Loss function:- Correct choice of loss functions is very important in case you want to achieve good results. | Loss Network:- You need a CNN model that is pretrained and can extract good features from the images. In our case, it is VGG19 pretrained on ImageNet. | So we load VGG model. The complete code is available at my GitHub repo. . # collapse-show if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: raise Exception(&#39;GPU is not available&#39;) # Load VGG19 features. We do not need the last linear layers, # only CNN layers are needed vgg = vgg19(pretrained=True).features vgg = vgg.to(device) # We don&#39;t want to train VGG for param in vgg.parameters(): param.requires_grad_(False) torch.backends.cudnn.benchmark = True . . Next we load our images from disk. My images are stored as src/imgs/content.png and src/imgs/style.png. . # collapse-show content_img = load_image(os.path.join(args.img_root, args.content_img), size=500) content_img = content_img.to(device) style_img = load_image(os.path.join(args.img_root, args.style_img)) style_img = style_img.to(device) # Show content and style image fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10)) ax1.imshow(im_convert(content_img)) ax2.imshow(im_convert(style_img)) plt.show() # Utility functions def im_convert(img): &quot;&quot;&quot; Convert img from pytorch tensor to numpy array, so we can plot it. It follows the standard method of denormalizing the img and clipping the outputs Input: img :- (batch, channel, height, width) Output: img :- (height, width, channel) &quot;&quot;&quot; img = img.to(&#39;cpu&#39;).clone().detach() img = img.numpy().squeeze(0) img = img.transpose(1, 2, 0) img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) img = img.clip(0, 1) return img def load_image(path, size=None): &quot;&quot;&quot; Resize img to size, size should be int and also normalize the image using imagenet_stats &quot;&quot;&quot; img = Image.open(path) if size is not None: img = img.resize((size, size)) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) img = transform(img).unsqueeze(0) return img . . Detail:- When we load our images, what sizes should we use? Your content image size should be divisible by 4, as our model would downsample images 2 times. For style images, do not resize them. Use their original resolution. Size of content image is (500x500x3) and size of style image is (800x800x3). . Hi-Res Generation Network . . The model is quite simple we start with 500x500x3 images and maintain this resolution for the complete model. We downsample to 250x250 and 125x125 and then fuse these back together with 500x500 images. . Details:- . No pooling is used (as pooling causes loss of information). Instead strided convolution (i.e. stride=2) are used. | No dropout is used. But if you need regularization you can use weight decay. | 3x3 conv kernels are used everywhere with padding=1. | Zero padding is only used. Reflex padding was tested but the results were not good. | For upsampling,’bilinear’ mode is used. | For downsampling, conv layers are used. | InstanceNorm is used. | # collapse-show # Downsampling function def conv_down(in_c, out_c, stride=2): return nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1) # Upsampling function def upsample(input, scale_factor): return F.interpolate(input=input, scale_factor=scale_factor, mode=&#39;bilinear&#39;, align_corners=False) . . Implementation code . Residual connections are used between every block. We use BottleNeck layer from the ResNet architecture. (In Fig5 all the horizontal arrows are bottleneck layers). . Refresher on bottleneck layer. . . # collapse-show # Helper class for BottleneckBlock class ConvLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1): super().__init__() # We have to keep the size of images same, so choose padding accordingly num_pad = int(np.floor(kernel_size / 2)) self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=num_pad) def forward(self, x): return self.conv(x) class BottleneckBlock(nn.Module): &quot;&quot;&quot; Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used instead of BatchNorm because when we want to generate images, we normalize all the images independently. (In batch norm you compute mean and std over complete batch, while in instance norm you compute mean and std for each image channel independently). The reason for doing this is, the generated images are independent of each other, so we should not normalize them using a common statistic. If you confused about the bottleneck architecture refer to the official pytorch resnet implementation and paper. &quot;&quot;&quot; def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): super().__init__() self.in_c = in_channels self.out_c = out_channels self.identity_block = nn.Sequential( ConvLayer(in_channels, out_channels//4, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels//4, kernel_size, stride=stride), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels), nn.ReLU(), ) self.shortcut = nn.Sequential( ConvLayer(in_channels, out_channels, 1, stride), nn.InstanceNorm2d(out_channels), ) def forward(self, x): out = self.identity_block(x) if self.in_c == self.out_c: residual = x else: residual = self.shortcut(x) out += residual out = F.relu(out) return out . . Now we are ready to implement our style_transfer model, which we call HRNet (based on the paper). Use the Fig5 as reference. . # collapse-show class HRNet(nn.Module): &quot;&quot;&quot; For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf. Naming convention used. I refer to vertical layers as a single layer, so from left to right we have 8 layers excluding the input image. E.g. layer 1 contains the 500x500x16 block layer 2 contains 500x500x32 and 250x250x32 blocks and so on self.layer{x}_{y}: x :- the layer number, as explained above y :- the index number for that function starting from 1. So if layer 3 has two downsample functions I write them as `downsample3_1`, `downsample3_2` &quot;&quot;&quot; def __init__(self): super().__init__() self.layer1_1 = BottleneckBlock(3, 16) self.layer2_1 = BottleneckBlock(16, 32) self.downsample2_1 = conv_down(16, 32) self.layer3_1 = BottleneckBlock(32, 32) self.layer3_2 = BottleneckBlock(32, 32) self.downsample3_1 = conv_down(32, 32) self.downsample3_2 = conv_down(32, 32, stride=4) self.downsample3_3 = conv_down(32, 32) self.layer4_1 = BottleneckBlock(64, 64) self.layer5_1 = BottleneckBlock(192, 64) self.layer6_1 = BottleneckBlock(64, 32) self.layer7_1 = BottleneckBlock(32, 16) self.layer8_1 = conv_down(16, 3, stride=1) # Needed conv layer so reused conv_down function def forward(self, x): map1_1 = self.layer1_1(x) map2_1 = self.layer2_1(map1_1) map2_2 = self.downsample2_1(map1_1) map3_1 = torch.cat((self.layer3_1(map2_1), upsample(map2_2, 2)), 1) map3_2 = torch.cat((self.downsample3_1(map2_1), self.layer3_2(map2_2)), 1) map3_3 = torch.cat((self.downsample3_2(map2_1), self.downsample3_3(map2_2)), 1) map4_1 = torch.cat((self.layer4_1(map3_1), upsample(map3_2, 2), upsample(map3_3, 4)), 1) out = self.layer5_1(map4_1) out = self.layer6_1(out) out = self.layer7_1(out) out = self.layer8_1(out) return out . . Loss functions . In style transfer we use feature extraction, to calculate the value of losses. Feature extraction put in simple terms, means you take a pretrained imagenet model and pass your images through it and store the intermediate layer outputs. Generally, VGG model is used for such tasks. . . So you take the outputs from the conv layers. Like for the above fig, you can take the output from the second 3x3 conv 64 layer and then 3x3 conv 128. . To extract features from VGG we use the following code. . # collapse-show def get_features(img, model, layers=None): &quot;&quot;&quot; Use VGG19 to extract features from the intermediate layers. &quot;&quot;&quot; if layers is None: layers = { &#39;0&#39; : &#39;conv1_1&#39;, # style layer &#39;5&#39; : &#39;conv2_1&#39;, # style layer &#39;10&#39;: &#39;conv3_1&#39;, # style layer &#39;19&#39;: &#39;conv4_1&#39;, # style layer &#39;28&#39;: &#39;conv5_1&#39;, # style layer &#39;21&#39;: &#39;conv4_2&#39; # content layer } features = {} x = img for name, layer in model._modules.items(): x = layer(x) if name in layers: features[layers[name]] = x return features . . We use 5 layers in total for feature extraction. Only conv4_2 is used as layer for content loss. . Refer to Fig4, we pass our output image from HRNet and the original content and style image through VGG. . There are two losses . Content Loss | Style Loss | Content Loss . Content image and the output image should have a similar feature representation as computed by loss network VGG. Because we are only changing the style without any changes to the structure of the image. For the content loss, we use Euclidean distance as shown by the formula . $$l_{content}^{ phi,j}(y, hat{y})= frac{1}{C_jJ_jW_j} left | phi_j( hat{y}= phi_j(y) right |^2$$ . $ phi_j$ means we are referring to the activations of the j-th layer of loss network. In code it looks like this. . style_net = HRNet().to(device) . target = style_net(content_img).to(device) target.requiresgrad(True) . target_features = get_features(target, vgg) content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;]) ** 2) . Style Loss . We use gram matrix for this. So style of an image is given by its gram matrix. Our aim is to make style of two images close, so we compute the difference of gram matrix of style image and output image and then take their Frobenius norm. . $$l_{style}^{ phi,j}(y, hat{y})= left |G_j^{ phi}(y)-G_j^{ phi}( hat{y}) right |^2$$ . # collapse-show def get_gram_matrix(img): &quot;&quot;&quot; Compute the gram matrix by converting to 2D tensor and doing dot product img: (batch, channel, height, width) &quot;&quot;&quot; b, c, h, w = img.size() img = img.view(b*c, h*w) gram = torch.mm(img, img.t()) return gram # There are 5 layers, and we compute style loss for each layer and sum them up style_loss = 0 for layer in layers: target_gram_matrix = get_gram_matrix(target_feature) # we already computed gram matrix for our style image style_gram_matrix = style_gram_matrixs[layer] layer_style_loss = style_weights[layer] * torch.mean((target_gram_matrix - style_gram_matrix) ** 2) b, c, h, w = target_feature.shape style_loss += layer_style_loss / (c*h*w) . . Difficult part . To compute our final losses, we multiply them with some weights. . content_loss = content_weight * content_loss style_loss = style_weight * style_loss . The difficulty comes in setting these values. If you want some desired output, then you would have to test different values before you get your desired result. . To build your own intuitions you can choose two images and try different range of values. I am working on providing like a summary of this. It will be available in my repo README. . Paper recommends content_weight = [50, 100] and style_weight = [1, 10]. . Conclusion . Well, congratulation made it to the end. You can now implement style transfer. Now read the paper for more details on style transfer. . Check out my repo README, it will contain the complete instructions on how to use the code in the repo, along with complete steps on how to train your model. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/05/06/post-0005.html",
            "relUrl": "/paper-implementation/2019/05/06/post-0005.html",
            "date": " • May 6, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "SPADE: State of the art in Image-to-Image Translation by Nvidia",
            "content": "Link to implementation code, paper . To give motivation for this paper, see the demo released by Nvidia. . . What is Semantic Image Synthesis? . It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class. . . New things in the paper . SPADE paper introduces a new normalization technique called spatially-adaptive normalization. Earlier models used the seg map only at the input layer but as seg map was only available in one layer the information contained in the seg map washed away in the deeper layers. SPADE solves this problem. In SPADE, we give seg map as input to all the intermediate layers. . How to train the model? . Before getting into the details of the model, I would discuss how models are trained for a task like Semantic Image Synthesis. . The core idea behind the model training is a GAN. Why GAN is needed? Because whenever we want to generate something that looks photorealistic or more technically closer to the output images, we have to use GANs. . So for GAN we need three things 1) Generator 2) Discriminator 3) Loss Function. For the Generator, we need to input some random values. Now you can either take random normal values. But if you want your output image to resemble some other image i.e. take the style of some image and add it your output image, you will also need an image encoder which would provide the mean and variance values for the random Gaussian distribution. . For the loss function, we would use the loss function used in pix2pixHD paper with some modifications. Also, I would discuss this technique where we extract features from the VGG model and then compute loss function (perceptual loss). . SPADE . This is the basic block that we would use. . . How to resize segmentation map? . Every pixel value in your seg map corresponds to a class and you cannot introduce new pixel values. When we use the defaults in various libraries for resizing, we do some form of interpolation like linear, which can change up the pixel values and result in values that were not there before. To solve this problem, whenever you have to resize your segmentation map use ‘nearest’ as the upsampling or downsampling method. . How we use it? Consider some layer in your model, you want to add the information from the segmentation map to the output of that layer. That will be done using SPADE. . SPADE first resizes your seg map to match the size of the features and then we apply a conv layer to the resized seg map to extract the features. To normalize our feature map, we first normalize our feature map using BatchNorm and then denormalize using the values we get from the seg map. . # collapse-show class SPADE(Module): def __init__(self, args, k): super().__init__() num_filters = args.spade_filter kernel_size = args.spade_kernel self.conv = spectral_norm(Conv2d(1, num_filters, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_gamma = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_beta = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) def forward(self, x, seg): N, C, H, W = x.size() sum_channel = torch.sum(x.reshape(N, C, H*W), dim=-1) mean = sum_channel / (N*H*W) std = torch.sqrt((sum_channel**2 - mean**2) / (N*H*W)) mean = torch.unsqueeze(torch.unsqueeze(mean, -1), -1) std = torch.unsqueeze(torch.unsqueeze(std, -1), -1) x = (x - mean) / std seg = F.interpolate(seg, size=(H,W), mode=&#39;nearest&#39;) seg = relu(self.conv(seg)) seg_gamma = self.conv_gamma(seg) seg_beta = self.conv_beta(seg) x = torch.matmul(seg_gamma, x) + seg_beta return x . . SPADERes Block . Just like Resnet where we combine conv layers into a ResNet Block, we combine SPADE into a SPADEResBlk. . . The idea is simple we are just extending the ResNet block. The skip-connection is important as it allows for training of deeper networks and we do not have to suffer from problems of vanishing gradients. . # collapse-show class SPADEResBlk(Module): def __init__(self, args, k, skip=False): super().__init__() kernel_size = args.spade_resblk_kernel self.skip = skip if self.skip: self.spade1 = SPADE(args, 2*k) self.conv1 = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade_skip = SPADE(args, 2*k) self.conv_skip = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) else: self.spade1 = SPADE(args, k) self.conv1 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade2 = SPADE(args, k) self.conv2 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) def forward(self, x, seg): x_skip = x x = relu(self.spade1(x, seg)) x = self.conv1(x) x = relu(self.spade2(x, seg)) x = self.conv2(x) if self.skip: x_skip = relu(self.spade_skip(x_skip, seg)) x_skip = self.conv_skip(x_skip) return x_skip + x . . Now we have our basic blocks, we start coding up our GAN. Again, the three things that we need for GAN are: . Generator | Discriminator | Loss Function | Generator . . # collapse-show class SPADEGenerator(nn.Module): def __init__(self, args): super().__init__() self.linear = Linear(args.gen_input_size, args.gen_hidden_size) self.spade_resblk1 = SPADEResBlk(args, 1024) self.spade_resblk2 = SPADEResBlk(args, 1024) self.spade_resblk3 = SPADEResBlk(args, 1024) self.spade_resblk4 = SPADEResBlk(args, 512) self.spade_resblk5 = SPADEResBlk(args, 256) self.spade_resblk6 = SPADEResBlk(args, 128) self.spade_resblk7 = SPADEResBlk(args, 64) self.conv = spectral_norm(Conv2d(64, 3, kernel_size=(3,3), padding=1)) def forward(self, x, seg): b, c, h, w = seg.size() x = self.linear(x) x = x.view(b, -1, 4, 4) x = interpolate(self.spade_resblk1(x, seg), size=(2*h, 2*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk2(x, seg), size=(4*h, 4*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk3(x, seg), size=(8*h, 8*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk4(x, seg), size=(16*h, 16*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk5(x, seg), size=(32*h, 32*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk6(x, seg), size=(64*h, 64*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk7(x, seg), size=(128*h, 128*w), mode=&#39;nearest&#39;) x = tanh(self.conv(x)) return x . . Discriminator . . # collapse-show def custom_model1(in_chan, out_chan): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=2, padding=1)), nn.LeakyReLU(inplace=True) ) def custom_model2(in_chan, out_chan, stride=2): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=stride, padding=1)), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEDiscriminator(nn.Module): def __init__(self, args): super().__init__() self.layer1 = custom_model1(4, 64) self.layer2 = custom_model2(64, 128) self.layer3 = custom_model2(128, 256) self.layer4 = custom_model2(256, 512, stride=1) self.inst_norm = nn.InstanceNorm2d(512) self.conv = spectral_norm(nn.Conv2d(512, 1, kernel_size=(4,4), padding=1)) def forward(self, img, seg): x = torch.cat((seg, img.detach()), dim=1) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = leaky_relu(self.inst_norm(x)) x = self.conv(x) return x . . Loss function . The most important piece for training a GAN. We are all familiar with the loss function of minimizing the Generator and maximizing the discriminator, where the objective function looks something like this. . $$ mathbb{E}_{( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})}[ log D( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})]+ mathbb{E}_{ boldsymbol{ mathrm{s}}}[ log (1-D( boldsymbol{ mathrm{s}},G( boldsymbol{ mathrm{s}})$$ . Now we extend this loss function to a feature matching loss. What do I mean? When we compute this loss function we are only computing the values on a fixed size of the image, but what if we compute the losses at different sizes of the image and then sum them all. . This loss would stabilize training as the generator has to produce natural statistics at multiple scales. To do so, we extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized images. This is done by taking features out of a pretrained VGG model. This is called perceptual loss. The code makes it easier to understand. . # collapse-show class VGGLoss(nn.Module): def __init__(self): super().__init__() self.vgg = VGG19().cuda() self.criterion = nn.L1Loss() self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0] def forward(self, x, y): x_vgg, y_vgg = self.vgg(x), self.vgg(y) loss = 0 for i in range(len(x_vgg)): loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach()) return loss . . So we take the two images, real and synthesized and pass it through VGG network. We compare the intermediate feature maps to compute the loss. We can also use ResNet, but VGG works pretty good and earlier layers of VGG are generally good at extracting the features of an image. . This is not the complete loss function. Below I show my implementation without the perceptual loss. I strongly recommend seeing the loss function implementation used by Nvidia themselves for this project as it combines the above loss also and it would also provide a general guideline on how to train GANs in 2019. . # collapse-show class GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0, tensor=torch.FloatTensor): super().__init__() self.real_label = target_real_label self.fake_label = target_fake_label self.real_label_var = None self.fake_label_var = None self.Tensor = tensor if use_lsgan: self.loss = nn.L1Loss() else: self.loss = nn.BCELoss() def get_target_tensor(self, input, target_is_real): target_tensor = None if target_is_real: create_label = ((self.real_label_var is None) or (self.real_label_var.numel() != input.numel())) if create_label: real_tensor = self.Tensor(input.size()).fill_(self.real_label) self.real_label_var = torch.tensor(real_tensor, requires_grad=False) target_tensor = self.real_label_var else: create_label = ((self.fake_label_var is None) or (self.fake_label_var.numel() != input.numel())) if create_label: fake_tensor = self.Tensor(input.size()).fill_(self.fake_label) self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False) target_tensor = self.fake_label_var return target_tensor def __call__(self, input, target_is_real): target_tensor = self.get_target_tensor(input, target_is_real) return self.loss(input, target_tensor.to(torch.device(&#39;cuda&#39;))) . . Weight Init . In the paper, they used Glorot Initialization (another name of Xavier initialization). I prefer to use He. Initialization. . # collapse-show def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) . . Image Encoder . This is the final part of our model. It is used if you want to transfer style from one image to the output of SPADE. It works by outputting the mean and variance values from which we compute the random gaussian noise that we input to the generator. . . # collapse-show def conv_inst_lrelu(in_chan, out_chan): return nn.Sequential( nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=2, bias=False, padding=1), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEEncoder(nn.Module): def __init__(self, args): super().__init__() self.layer1 = conv_inst_lrelu(3, 64) self.layer2 = conv_inst_lrelu(64, 128) self.layer3 = conv_inst_lrelu(128, 256) self.layer4 = conv_inst_lrelu(256, 512) self.layer5 = conv_inst_lrelu(512, 512) self.layer6 = conv_inst_lrelu(512, 512) self.linear_mean = nn.Linear(8192, args.gen_input_size) self.linear_var = nn.Linear(8192, args.gen_input_size) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.layer5(x) x = self.layer6(x) x = x.view(x.size(0), -1) return self.linear_mean(x), self.linear_var(x) . . Why Spectral Normalization? . Spectral Normalization Explained by Christian Cosgrove. This article discusses spectral norm in detail with all the maths behind it. Ian Goodfellow even commented on spectral normalization and considers it to be an important tool. . The reason we need spectral norm is that when we are generating images, it can become a problem to train our model to generate images of say 1000 categories on ImageNet. Spectral Norm helps by stabilizing the training of discriminator. There are theoretical justifications behind this, on why this should be done, but all that is beautifully explained in the above blog post that I linked to. . To use spectral norm in your model, just apply spectral_norm to all convolutional layers in your generator and discriminator. . Brief Discussion on Instance normalization . . Batch Normalization uses the complete batch to compute the mean and std and then normalizes the complete batch with a single value of mean and std. This is good when we are doing classification, but when we are generating images, we want to keep the normalization of these images independent. . One simple reason for that is if in my batch one image is being generated for blue sky and in another image, generating a road then clearly normalizing these with the same mean and std would add extra noise to the images, which would make training worse. So instance norm is used instead of batch normalization here. . Resources . My Implementation link | SPADE Paper link | Official Implementation link | pix2pixHD link | Spectral Normalization paper link | Spectral Norm blog link | Instance Normalization paper link | Instance norm other resources, blog, stack overflow | .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/17/post-004.html",
            "relUrl": "/paper-implementation/2019/04/17/post-004.html",
            "date": " • Apr 17, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "SPADE: State of the art in Image-to-Image Translation by Nvidia",
            "content": "Link to implementation code, paper . To give motivation for this paper, see the demo released by Nvidia. . . What is Semantic Image Synthesis? . It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class. . . New things in the paper . SPADE paper introduces a new normalization technique called spatially-adaptive normalization. Earlier models used the seg map only at the input layer but as seg map was only available in one layer the information contained in the seg map washed away in the deeper layers. SPADE solves this problem. In SPADE, we give seg map as input to all the intermediate layers. . How to train the model? . Before getting into the details of the model, I would discuss how models are trained for a task like Semantic Image Synthesis. . The core idea behind the model training is a GAN. Why GAN is needed? Because whenever we want to generate something that looks photorealistic or more technically closer to the output images, we have to use GANs. . So for GAN we need three things 1) Generator 2) Discriminator 3) Loss Function. For the Generator, we need to input some random values. Now you can either take random normal values. But if you want your output image to resemble some other image i.e. take the style of some image and add it your output image, you will also need an image encoder which would provide the mean and variance values for the random Gaussian distribution. . For the loss function, we would use the loss function used in pix2pixHD paper with some modifications. Also, I would discuss this technique where we extract features from the VGG model and then compute loss function (perceptual loss). . SPADE . This is the basic block that we would use. . . How to resize segmentation map? . Every pixel value in your seg map corresponds to a class and you cannot introduce new pixel values. When we use the defaults in various libraries for resizing, we do some form of interpolation like linear, which can change up the pixel values and result in values that were not there before. To solve this problem, whenever you have to resize your segmentation map use ‘nearest’ as the upsampling or downsampling method. . How we use it? Consider some layer in your model, you want to add the information from the segmentation map to the output of that layer. That will be done using SPADE. . SPADE first resizes your seg map to match the size of the features and then we apply a conv layer to the resized seg map to extract the features. To normalize our feature map, we first normalize our feature map using BatchNorm and then denormalize using the values we get from the seg map. . # collapse-show class SPADE(Module): def __init__(self, args, k): super().__init__() num_filters = args.spade_filter kernel_size = args.spade_kernel self.conv = spectral_norm(Conv2d(1, num_filters, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_gamma = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_beta = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) def forward(self, x, seg): N, C, H, W = x.size() sum_channel = torch.sum(x.reshape(N, C, H*W), dim=-1) mean = sum_channel / (N*H*W) std = torch.sqrt((sum_channel**2 - mean**2) / (N*H*W)) mean = torch.unsqueeze(torch.unsqueeze(mean, -1), -1) std = torch.unsqueeze(torch.unsqueeze(std, -1), -1) x = (x - mean) / std seg = F.interpolate(seg, size=(H,W), mode=&#39;nearest&#39;) seg = relu(self.conv(seg)) seg_gamma = self.conv_gamma(seg) seg_beta = self.conv_beta(seg) x = torch.matmul(seg_gamma, x) + seg_beta return x . . SPADERes Block . Just like Resnet where we combine conv layers into a ResNet Block, we combine SPADE into a SPADEResBlk. . . The idea is simple we are just extending the ResNet block. The skip-connection is important as it allows for training of deeper networks and we do not have to suffer from problems of vanishing gradients. . # collapse-show class SPADEResBlk(Module): def __init__(self, args, k, skip=False): super().__init__() kernel_size = args.spade_resblk_kernel self.skip = skip if self.skip: self.spade1 = SPADE(args, 2*k) self.conv1 = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade_skip = SPADE(args, 2*k) self.conv_skip = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) else: self.spade1 = SPADE(args, k) self.conv1 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade2 = SPADE(args, k) self.conv2 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) def forward(self, x, seg): x_skip = x x = relu(self.spade1(x, seg)) x = self.conv1(x) x = relu(self.spade2(x, seg)) x = self.conv2(x) if self.skip: x_skip = relu(self.spade_skip(x_skip, seg)) x_skip = self.conv_skip(x_skip) return x_skip + x . . Now we have our basic blocks, we start coding up our GAN. Again, the three things that we need for GAN are: . Generator | Discriminator | Loss Function | Generator . . # collapse-show class SPADEGenerator(nn.Module): def __init__(self, args): super().__init__() self.linear = Linear(args.gen_input_size, args.gen_hidden_size) self.spade_resblk1 = SPADEResBlk(args, 1024) self.spade_resblk2 = SPADEResBlk(args, 1024) self.spade_resblk3 = SPADEResBlk(args, 1024) self.spade_resblk4 = SPADEResBlk(args, 512) self.spade_resblk5 = SPADEResBlk(args, 256) self.spade_resblk6 = SPADEResBlk(args, 128) self.spade_resblk7 = SPADEResBlk(args, 64) self.conv = spectral_norm(Conv2d(64, 3, kernel_size=(3,3), padding=1)) def forward(self, x, seg): b, c, h, w = seg.size() x = self.linear(x) x = x.view(b, -1, 4, 4) x = interpolate(self.spade_resblk1(x, seg), size=(2*h, 2*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk2(x, seg), size=(4*h, 4*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk3(x, seg), size=(8*h, 8*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk4(x, seg), size=(16*h, 16*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk5(x, seg), size=(32*h, 32*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk6(x, seg), size=(64*h, 64*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk7(x, seg), size=(128*h, 128*w), mode=&#39;nearest&#39;) x = tanh(self.conv(x)) return x . . Discriminator . . # collapse-show def custom_model1(in_chan, out_chan): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=2, padding=1)), nn.LeakyReLU(inplace=True) ) def custom_model2(in_chan, out_chan, stride=2): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=stride, padding=1)), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEDiscriminator(nn.Module): def __init__(self, args): super().__init__() self.layer1 = custom_model1(4, 64) self.layer2 = custom_model2(64, 128) self.layer3 = custom_model2(128, 256) self.layer4 = custom_model2(256, 512, stride=1) self.inst_norm = nn.InstanceNorm2d(512) self.conv = spectral_norm(nn.Conv2d(512, 1, kernel_size=(4,4), padding=1)) def forward(self, img, seg): x = torch.cat((seg, img.detach()), dim=1) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = leaky_relu(self.inst_norm(x)) x = self.conv(x) return x . . Loss function . The most important piece for training a GAN. We are all familiar with the loss function of minimizing the Generator and maximizing the discriminator, where the objective function looks something like this. . $$ mathbb{E}_{( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})}[ log D( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})]+ mathbb{E}_{ boldsymbol{ mathrm{s}}}[ log (1-D( boldsymbol{ mathrm{s}},G( boldsymbol{ mathrm{s}})$$ . Now we extend this loss function to a feature matching loss. What do I mean? When we compute this loss function we are only computing the values on a fixed size of the image, but what if we compute the losses at different sizes of the image and then sum them all. . This loss would stabilize training as the generator has to produce natural statistics at multiple scales. To do so, we extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized images. This is done by taking features out of a pretrained VGG model. This is called perceptual loss. The code makes it easier to understand. . # collapse-show class VGGLoss(nn.Module): def __init__(self): super().__init__() self.vgg = VGG19().cuda() self.criterion = nn.L1Loss() self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0] def forward(self, x, y): x_vgg, y_vgg = self.vgg(x), self.vgg(y) loss = 0 for i in range(len(x_vgg)): loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach()) return loss . . So we take the two images, real and synthesized and pass it through VGG network. We compare the intermediate feature maps to compute the loss. We can also use ResNet, but VGG works pretty good and earlier layers of VGG are generally good at extracting the features of an image. . This is not the complete loss function. Below I show my implementation without the perceptual loss. I strongly recommend seeing the loss function implementation used by Nvidia themselves for this project as it combines the above loss also and it would also provide a general guideline on how to train GANs in 2019. . # collapse-show class GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0, tensor=torch.FloatTensor): super().__init__() self.real_label = target_real_label self.fake_label = target_fake_label self.real_label_var = None self.fake_label_var = None self.Tensor = tensor if use_lsgan: self.loss = nn.L1Loss() else: self.loss = nn.BCELoss() def get_target_tensor(self, input, target_is_real): target_tensor = None if target_is_real: create_label = ((self.real_label_var is None) or (self.real_label_var.numel() != input.numel())) if create_label: real_tensor = self.Tensor(input.size()).fill_(self.real_label) self.real_label_var = torch.tensor(real_tensor, requires_grad=False) target_tensor = self.real_label_var else: create_label = ((self.fake_label_var is None) or (self.fake_label_var.numel() != input.numel())) if create_label: fake_tensor = self.Tensor(input.size()).fill_(self.fake_label) self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False) target_tensor = self.fake_label_var return target_tensor def __call__(self, input, target_is_real): target_tensor = self.get_target_tensor(input, target_is_real) return self.loss(input, target_tensor.to(torch.device(&#39;cuda&#39;))) . . Weight Init . In the paper, they used Glorot Initialization (another name of Xavier initialization). I prefer to use He. Initialization. . # collapse-show def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) . . Image Encoder . This is the final part of our model. It is used if you want to transfer style from one image to the output of SPADE. It works by outputting the mean and variance values from which we compute the random gaussian noise that we input to the generator. . . # collapse-show def conv_inst_lrelu(in_chan, out_chan): return nn.Sequential( nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=2, bias=False, padding=1), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEEncoder(nn.Module): def __init__(self, args): super().__init__() self.layer1 = conv_inst_lrelu(3, 64) self.layer2 = conv_inst_lrelu(64, 128) self.layer3 = conv_inst_lrelu(128, 256) self.layer4 = conv_inst_lrelu(256, 512) self.layer5 = conv_inst_lrelu(512, 512) self.layer6 = conv_inst_lrelu(512, 512) self.linear_mean = nn.Linear(8192, args.gen_input_size) self.linear_var = nn.Linear(8192, args.gen_input_size) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.layer5(x) x = self.layer6(x) x = x.view(x.size(0), -1) return self.linear_mean(x), self.linear_var(x) . . Why Spectral Normalization? . Spectral Normalization Explained by Christian Cosgrove. This article discusses spectral norm in detail with all the maths behind it. Ian Goodfellow even commented on spectral normalization and considers it to be an important tool. . The reason we need spectral norm is that when we are generating images, it can become a problem to train our model to generate images of say 1000 categories on ImageNet. Spectral Norm helps by stabilizing the training of discriminator. There are theoretical justifications behind this, on why this should be done, but all that is beautifully explained in the above blog post that I linked to. . To use spectral norm in your model, just apply spectral_norm to all convolutional layers in your generator and discriminator. . Brief Discussion on Instance normalization . . Batch Normalization uses the complete batch to compute the mean and std and then normalizes the complete batch with a single value of mean and std. This is good when we are doing classification, but when we are generating images, we want to keep the normalization of these images independent. . One simple reason for that is if in my batch one image is being generated for blue sky and in another image, generating a road then clearly normalizing these with the same mean and std would add extra noise to the images, which would make training worse. So instance norm is used instead of batch normalization here. . Resources . My Implementation link | SPADE Paper link | Official Implementation link | pix2pixHD link | Spectral Normalization paper link | Spectral Norm blog link | Instance Normalization paper link | Instance norm other resources, blog, stack overflow | .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/17/post-0004.html",
            "relUrl": "/paper-implementation/2019/04/17/post-0004.html",
            "date": " • Apr 17, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Weight Standardization: A new normalization in town",
            "content": "Link to jupyter notebook, paper . Recently a new normalization technique is proposed not for the activations but for the weights themselves in the paper Weight Standardization. . In short, to get new state of the art results, they combined Batch Normalization and Weight Standardization. So in this post, I discuss what is weight standardization and how it helps in the training process, and I will show my own experiments on CIFAR-10 which you can also follow along. . . For my experiments, I will use cyclic learning. As the paper discusses training with constant learning rates, I would use cyclic LR as presented by Leslie N. Smith in his report. . To make things cleaner I would use this notation:- . BN -&gt; Batch Normalization | GN -&gt; Group Normalization | WS -&gt; Weight Standardization | . What is wrong with BN and GB? . Ideally, nothing is wrong with them. But to get the most benefit out of BN we have to use a large batch size. And when we have smaller batch sizes we prefer to use GN. (By smaller I mean 1–2 images/GPU). . Why is this so? . To understand it we have to see how BN works. To make things simple, consider we have only one-channel on which we want to apply BN and we have 2 images as our batch size. . Now we would compute the mean and variance using the 2 images and then normalize the one-channel of the 2 images. So we used 2 images to compute mean and variance. This is the problem. . By increasing batch size, we are able to sample the value of mean and variance from a larger population, which means that the computed mean and variance would be closer to their real values. . GN was introduced for cases of small batch sizes but it was not able to meet the results that BN was able to achieve using larger batch sizes. . How these normalization actually help? . It is one of the leading areas of research. But it was recently shown in the paper Fixup Initialization: Residual Learning without Normalization the reason for the performance gains using BN. . In short, it helps make the loss surface smooth. . . When we make the loss surface smooth we can take longer steps, which means we can increase our learning rate. So using Batch Norm actually stabilizes our training and also makes it faster. . Weight Standardization . Unlike BN and GN that we apply on activations i.e the output of the conv layer, we apply Weight Standardization on the weights of the conv layer itself. So we are applying WS to the kernels that our conv layer uses. . How does this help? . For the theoretical justification see the original paper where they prove WS reduces the Lipschitz constants of the loss and the gradients. . But there are easier ways to understand it. . First, consider the optimizer we use. The role of the optimizer is to optimize the weights of our model, but when we apply normalization layers like BN, we do not normalize our weights, but instead, we normalize the activations which are optimizer does not even care about. . By using WS we are essentially normalizing the gradients during the backpropagation. . The authors of the paper tested WS on various computer vision tasks and they were able to achieve better results with the combination of WS+GN and WS+BN. The tasks that they tested on included: . Image Classification | Object Detection | Video Recognition | Semantic Segmentation | Point Cloud Classification | Enough talk, let&#39;s go to experiments . The code is available in the notebook. . How to implement WS? . # collapse-show class Conv2d(nn.Module): def __init__(self, in_chan, out_chan, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True): super().__init__(in_chan, out_chan, kernel_size, stride, padding, dilation, groups, bias) def forward(self, x): weight = self.weight weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True) weight = weight - weight_mean std = weight.view(weight.size(0), -1).std(dim=1).view(-1,1,1,1)+1e-5 weight = weight / std.expand_as(weight) return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups) . . First, let&#39;s try out at batch size = 64 . This will provide a baseline of what we should expect. For this, I create 2 resnet18 models: . resnet18 $ rightarrow$ It uses the nn.Conv2d layers | resnet18_ws $ rightarrow$ It uses above Conv2d layer which uses weight standardization | I change the head of resnet model, as CIFAR images are already 32 in size and I don’t want to half their size initially. The code can be found in the notebook. And for the CIFAR dataset, I use the official train and valid split. . First I plot the value of loss v/s learning rate. . . For those not familiar with loss v/s learning_rate graph. We are looking for the maximum value of lr at which the loss value starts increasing. . In this case the max_lr is around 0.0005. So let’s try to train model for some steps and see. In case you wonder in the second case the graph is flatter around 1e-2, it is because the scale of the two graphs is different. . So now let’s train our model and see what happens. I am using the fit_one_cycle to train my model. . . There is not much difference between the two as valid loss almost remains the same. . Try at bs=2 . Now I take a batch size of 2 and train the models in a similar manner. . . One thing that I should add here, is the loss diverged quickly when I used only BN, after around 40 iterations, while in the case of WS+BN the loss did not diverge. . . There is not much difference in the loss values, but the time to run each cycle increased very much. . Trying bs=256 . Also, I run some more experiments where I used a batch size of 256. Although, I could use a larger learning rate but the time taken to complete the cycle increased. The results are shown below. . . . Again, in the graph we see we can use a larger learning rate. . Conclusion . From the above experiments, I think I would prefer not to use Weight Standardization when I am using cyclic learning. For large batch sizes, it even gave worse performance and for smaller batch sizes, it gave almost similar results, but using weight standardization we added a lot of time to our computation, which we could have used to train our model with Batch Norm alone. . For constant learning rate, I think weight standardization still makes sense as there we do not change our learning rate in the training process, so we must benefit from the smoother loss function. But in the case of cyclic learning, it does not offer us a benefit. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-003.html",
            "relUrl": "/paper-implementation/2019/04/11/post-003.html",
            "date": " • Apr 11, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Weight Standardization: A new normalization in town",
            "content": "Link to jupyter notebook, paper . Recently a new normalization technique is proposed not for the activations but for the weights themselves in the paper Weight Standardization. . In short, to get new state of the art results, they combined Batch Normalization and Weight Standardization. So in this post, I discuss what is weight standardization and how it helps in the training process, and I will show my own experiments on CIFAR-10 which you can also follow along. . . For my experiments, I will use cyclic learning. As the paper discusses training with constant learning rates, I would use cyclic LR as presented by Leslie N. Smith in his report. . To make things cleaner I would use this notation:- . BN -&gt; Batch Normalization | GN -&gt; Group Normalization | WS -&gt; Weight Standardization | . What is wrong with BN and GB? . Ideally, nothing is wrong with them. But to get the most benefit out of BN we have to use a large batch size. And when we have smaller batch sizes we prefer to use GN. (By smaller I mean 1–2 images/GPU). . Why is this so? . To understand it we have to see how BN works. To make things simple, consider we have only one-channel on which we want to apply BN and we have 2 images as our batch size. . Now we would compute the mean and variance using the 2 images and then normalize the one-channel of the 2 images. So we used 2 images to compute mean and variance. This is the problem. . By increasing batch size, we are able to sample the value of mean and variance from a larger population, which means that the computed mean and variance would be closer to their real values. . GN was introduced for cases of small batch sizes but it was not able to meet the results that BN was able to achieve using larger batch sizes. . How these normalization actually help? . It is one of the leading areas of research. But it was recently shown in the paper Fixup Initialization: Residual Learning without Normalization the reason for the performance gains using BN. . In short, it helps make the loss surface smooth. . . When we make the loss surface smooth we can take longer steps, which means we can increase our learning rate. So using Batch Norm actually stabilizes our training and also makes it faster. . Weight Standardization . Unlike BN and GN that we apply on activations i.e the output of the conv layer, we apply Weight Standardization on the weights of the conv layer itself. So we are applying WS to the kernels that our conv layer uses. . How does this help? . For the theoretical justification see the original paper where they prove WS reduces the Lipschitz constants of the loss and the gradients. . But there are easier ways to understand it. . First, consider the optimizer we use. The role of the optimizer is to optimize the weights of our model, but when we apply normalization layers like BN, we do not normalize our weights, but instead, we normalize the activations which are optimizer does not even care about. . By using WS we are essentially normalizing the gradients during the backpropagation. . The authors of the paper tested WS on various computer vision tasks and they were able to achieve better results with the combination of WS+GN and WS+BN. The tasks that they tested on included: . Image Classification | Object Detection | Video Recognition | Semantic Segmentation | Point Cloud Classification | Enough talk, let&#39;s go to experiments . The code is available in the notebook. . How to implement WS? . # collapse-show class Conv2d(nn.Module): def __init__(self, in_chan, out_chan, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True): super().__init__(in_chan, out_chan, kernel_size, stride, padding, dilation, groups, bias) def forward(self, x): weight = self.weight weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True) weight = weight - weight_mean std = weight.view(weight.size(0), -1).std(dim=1).view(-1,1,1,1)+1e-5 weight = weight / std.expand_as(weight) return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups) . . First, let&#39;s try out at batch size = 64 . This will provide a baseline of what we should expect. For this, I create 2 resnet18 models: . resnet18 $ rightarrow$ It uses the nn.Conv2d layers | resnet18_ws $ rightarrow$ It uses above Conv2d layer which uses weight standardization | I change the head of resnet model, as CIFAR images are already 32 in size and I don’t want to half their size initially. The code can be found in the notebook. And for the CIFAR dataset, I use the official train and valid split. . First I plot the value of loss v/s learning rate. . . For those not familiar with loss v/s learning_rate graph. We are looking for the maximum value of lr at which the loss value starts increasing. . In this case the max_lr is around 0.0005. So let’s try to train model for some steps and see. In case you wonder in the second case the graph is flatter around 1e-2, it is because the scale of the two graphs is different. . So now let’s train our model and see what happens. I am using the fit_one_cycle to train my model. . . There is not much difference between the two as valid loss almost remains the same. . Try at bs=2 . Now I take a batch size of 2 and train the models in a similar manner. . . One thing that I should add here, is the loss diverged quickly when I used only BN, after around 40 iterations, while in the case of WS+BN the loss did not diverge. . . There is not much difference in the loss values, but the time to run each cycle increased very much. . Trying bs=256 . Also, I run some more experiments where I used a batch size of 256. Although, I could use a larger learning rate but the time taken to complete the cycle increased. The results are shown below. . . . Again, in the graph we see we can use a larger learning rate. . Conclusion . From the above experiments, I think I would prefer not to use Weight Standardization when I am using cyclic learning. For large batch sizes, it even gave worse performance and for smaller batch sizes, it gave almost similar results, but using weight standardization we added a lot of time to our computation, which we could have used to train our model with Batch Norm alone. . For constant learning rate, I think weight standardization still makes sense as there we do not change our learning rate in the training process, so we must benefit from the smoother loss function. But in the case of cyclic learning, it does not offer us a benefit. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-0003.html",
            "relUrl": "/paper-implementation/2019/04/11/post-0003.html",
            "date": " • Apr 11, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch",
            "content": "Link to jupyter notebook . Data . To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work. . . Note: - I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do. . Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use ImageNet Object Localization Challenge to directly download all the files (warning 155GB). | Unzip the tar.gz file using tar xzvf file_name -C destination_path. | In the Data/CLS-LOC folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders. | Use this command from your terminal in the val folder wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash. It would move all the images to their respective class folders. | As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^&gt;” {}. | After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???. . . Data Loading . Steps involved:- . Create a dataset class or use a predefined class | Choose what transforms you want to perform on the data. | Create data loaders | . # collapse-show train_dir = &#39;../../../Data/ILSVRC2012/train&#39; val_dir = &#39;../../../Data/ILSVRC2012/val&#39; size = 224 batch_size = 32 num_workers = 8 data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]), &#39;val&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) } image_datasets = { &#39;train&#39;: ImageFolder(train_dir, transform=data_transforms[&#39;train&#39;]), &#39;val&#39;: ImageFolder(val_dir, transform=data_transforms[&#39;val&#39;]), } data_loader = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in [&#39;train&#39;, &#39;val&#39;] } . . The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step. . Check dataloaders . After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images. . # collapse-show # As our images are normalized we have to denormalize them and # convert them to numpy arrays. def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std*img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) #Pause is necessary to display images correctly images, labels = next(iter(data_loader[&#39;train&#39;])) grid_img = make_grid(images[:4], nrow=4) imshow(grid_img, title = [labels_list[x] for x in labels[:4]]) . . One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file LOC_synset_mapping.txt. . # collapse-show f = open(&quot;../../Data/LOC_synset_mapping.txt&quot;, &quot;r&quot;) labels_dict = {} # Get class label by folder name labels_list = [] # Get class label by indexing for line in f: split = line.split(&#39; &#39;, maxsplit=1) split[1] = split[1][:-1] label_id, label = split[0], split[1] labels_dict[label_id] = label labels_list.append(split[1]) . . Model Construction . Create your model. Pretrained models covered at the end of the post. . . Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’. . # collapse-show class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.conv_base = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False), nn.BatchNorm2d(96), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc_base = nn.Sequential( nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.conv_base(x) x = x.view(x.size(0), 256*6*6) x = self.fc_base(x) return x . . See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization). . Best practices for CNN . Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always. | Weight Initialization:- Use He initialization as default with ReLU. PyTorch provides kaimingnormal for this purpose. | Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. | Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model. | Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good. | Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference. | Weight Initialization . Do not use this method as a default. After, naming the layers you can do this very easily. . # collapse-show conv_list = [0, 4, 8, 10, 12] fc_list = [1, 4, 6] for i in conv_list: torch.nn.init.kaiming_normal_(model.conv_base[i].weight) for i in fc_list: torch.nn.init.kaiming_normal_(model.fc_base[i].weight) . . Create optimizers, schedulers and loss functions. . # collapse-show device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) # Cross entropy loss takes the logits directly, so we don&#39;t need to apply softmax in our CNN criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;max&#39;, verbose=True) . . PyTorch specific discussion . You have to specify the padding yourself. Check this thread for discussion on this topic. | Create the optimizer after moving the model to GPU. | The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself. | Do not forget to zero the grads. | . How to check my model is correct? . Check 1 . The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input. . # collapse-show inputs, labels = next(iter(data_loader[&#39;train&#39;])) inputs = inputs.to(device) labels = labels.to(device) criterion_check1 = nn.CrossEntropyLoss() optimizer_check1 = optim.SGD(model.parameters(), lr=0.001) model.train() for epoch in range(200): optimizer_check1.zero_grad() outputs = model(inputs) loss = criterion_check1(outputs, labels) _, preds = torch.max(outputs, 1) loss.backward() optimizer_check1.step() if epoch%10 == 0: print(&#39;Epoch {}: Loss = {} Accuracy = {}&#39;.format(epoch+1, loss.item(), torch.sum(preds == labels))) . . . . Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again. . Check 2 . Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems. . In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs. . Using Pretrained Models . As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset. . . Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended. . # collapse-show data = np.load(&#39;../../../Data/cifar_10.npz&#39;) alexnet = torch.load(&#39;../../../Data/Pytorch Trained Models/alexnet.pth&#39;) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) x_train = torch.from_numpy(data[&#39;train_data&#39;]) y_train = torch.from_numpy(data[&#39;train_labels&#39;]) x_test = torch.from_numpy(data[&#39;test_data&#39;]) y_test = torch.from_numpy(data[&#39;test_labels&#39;]) . . # collapse-show # Create data loader class CIFAR_Dataset(torch.utils.data.Dataset): &quot;&quot;&quot; Generally you would not load images in the __init__ as it forces the images to load into memory. Instead you should load the images in getitem function, but as CIFAR is small dataset I load all the images in memory. &quot;&quot;&quot; def __init__(self, x, y, transform=None): self.x = x self.y = y self.transform = transform def __len__(self): return self.x.size(0) def __getitem__(self, idx): image = self.x[idx] label = self.y[idx].item() if self.transform: image = self.transform(image) return (image, label) data_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) datasets = { &#39;train&#39;: CIFAR_Dataset(x_train, y_train, transform=data_transforms), &#39;test&#39;: CIFAR_Dataset(x_test, y_test, transform=data_transforms) } data_loader = { x: torch.utils.data.DataLoader(datasets[x], batch_size=64, shuffle=True, num_workers=8) for x in [&#39;train&#39;, &#39;test&#39;] } . . # collapse-show # Freeze conv layers for param in alexnet.parameters(): param.requires_grad = False # Initialize the last layer of alexnet model for out 10 class dataset alexnet.classifier[6] = nn.Linear(4096, 10) alexnet = alexnet.to(device) criterion = nn.CrossEntropyLoss() # Create list of params to learn params_to_learn = [] for name,param in alexnet.named_parameters(): if param.requires_grad == True: params_to_learn.append(param) optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True) . . Refer to this script on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from torchvision.datasets. . PyTorch has a very good tutorial on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook. . Conclusion . We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images. . References . https://github.com/soumith/imagenet-multiGPU.torch Helped in preprocessing of the Imagenet dataset. | https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html Many code references were taken from this tutorial. | https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf AlexNet paper |",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2019/02/18/post-002.html",
            "relUrl": "/tutorial/2019/02/18/post-002.html",
            "date": " • Feb 18, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch",
            "content": "Link to jupyter notebook . Data . To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work. . . Note: - I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do. . Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use ImageNet Object Localization Challenge to directly download all the files (warning 155GB). | Unzip the tar.gz file using tar xzvf file_name -C destination_path. | In the Data/CLS-LOC folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders. | Use this command from your terminal in the val folder wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash. It would move all the images to their respective class folders. | As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^&gt;” {}. | After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???. . . Data Loading . Steps involved:- . Create a dataset class or use a predefined class | Choose what transforms you want to perform on the data. | Create data loaders | . # collapse-show train_dir = &#39;../../../Data/ILSVRC2012/train&#39; val_dir = &#39;../../../Data/ILSVRC2012/val&#39; size = 224 batch_size = 32 num_workers = 8 data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]), &#39;val&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) } image_datasets = { &#39;train&#39;: ImageFolder(train_dir, transform=data_transforms[&#39;train&#39;]), &#39;val&#39;: ImageFolder(val_dir, transform=data_transforms[&#39;val&#39;]), } data_loader = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in [&#39;train&#39;, &#39;val&#39;] } . . The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step. . Check dataloaders . After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images. . # collapse-show # As our images are normalized we have to denormalize them and # convert them to numpy arrays. def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std*img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) #Pause is necessary to display images correctly images, labels = next(iter(data_loader[&#39;train&#39;])) grid_img = make_grid(images[:4], nrow=4) imshow(grid_img, title = [labels_list[x] for x in labels[:4]]) . . One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file LOC_synset_mapping.txt. . # collapse-show f = open(&quot;../../Data/LOC_synset_mapping.txt&quot;, &quot;r&quot;) labels_dict = {} # Get class label by folder name labels_list = [] # Get class label by indexing for line in f: split = line.split(&#39; &#39;, maxsplit=1) split[1] = split[1][:-1] label_id, label = split[0], split[1] labels_dict[label_id] = label labels_list.append(split[1]) . . Model Construction . Create your model. Pretrained models covered at the end of the post. . . Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’. . # collapse-show class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.conv_base = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False), nn.BatchNorm2d(96), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc_base = nn.Sequential( nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.conv_base(x) x = x.view(x.size(0), 256*6*6) x = self.fc_base(x) return x . . See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization). . Best practices for CNN . Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always. | Weight Initialization:- Use He initialization as default with ReLU. PyTorch provides kaimingnormal for this purpose. | Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. | Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model. | Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good. | Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference. | Weight Initialization . Do not use this method as a default. After, naming the layers you can do this very easily. . # collapse-show conv_list = [0, 4, 8, 10, 12] fc_list = [1, 4, 6] for i in conv_list: torch.nn.init.kaiming_normal_(model.conv_base[i].weight) for i in fc_list: torch.nn.init.kaiming_normal_(model.fc_base[i].weight) . . Create optimizers, schedulers and loss functions. . # collapse-show device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) # Cross entropy loss takes the logits directly, so we don&#39;t need to apply softmax in our CNN criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;max&#39;, verbose=True) . . PyTorch specific discussion . You have to specify the padding yourself. Check this thread for discussion on this topic. | Create the optimizer after moving the model to GPU. | The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself. | Do not forget to zero the grads. | . How to check my model is correct? . Check 1 . The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input. . # collapse-show inputs, labels = next(iter(data_loader[&#39;train&#39;])) inputs = inputs.to(device) labels = labels.to(device) criterion_check1 = nn.CrossEntropyLoss() optimizer_check1 = optim.SGD(model.parameters(), lr=0.001) model.train() for epoch in range(200): optimizer_check1.zero_grad() outputs = model(inputs) loss = criterion_check1(outputs, labels) _, preds = torch.max(outputs, 1) loss.backward() optimizer_check1.step() if epoch%10 == 0: print(&#39;Epoch {}: Loss = {} Accuracy = {}&#39;.format(epoch+1, loss.item(), torch.sum(preds == labels))) . . . . Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again. . Check 2 . Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems. . In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs. . Using Pretrained Models . As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset. . . Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended. . # collapse-show data = np.load(&#39;../../../Data/cifar_10.npz&#39;) alexnet = torch.load(&#39;../../../Data/Pytorch Trained Models/alexnet.pth&#39;) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) x_train = torch.from_numpy(data[&#39;train_data&#39;]) y_train = torch.from_numpy(data[&#39;train_labels&#39;]) x_test = torch.from_numpy(data[&#39;test_data&#39;]) y_test = torch.from_numpy(data[&#39;test_labels&#39;]) . . # collapse-show # Create data loader class CIFAR_Dataset(torch.utils.data.Dataset): &quot;&quot;&quot; Generally you would not load images in the __init__ as it forces the images to load into memory. Instead you should load the images in getitem function, but as CIFAR is small dataset I load all the images in memory. &quot;&quot;&quot; def __init__(self, x, y, transform=None): self.x = x self.y = y self.transform = transform def __len__(self): return self.x.size(0) def __getitem__(self, idx): image = self.x[idx] label = self.y[idx].item() if self.transform: image = self.transform(image) return (image, label) data_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) datasets = { &#39;train&#39;: CIFAR_Dataset(x_train, y_train, transform=data_transforms), &#39;test&#39;: CIFAR_Dataset(x_test, y_test, transform=data_transforms) } data_loader = { x: torch.utils.data.DataLoader(datasets[x], batch_size=64, shuffle=True, num_workers=8) for x in [&#39;train&#39;, &#39;test&#39;] } . . # collapse-show # Freeze conv layers for param in alexnet.parameters(): param.requires_grad = False # Initialize the last layer of alexnet model for out 10 class dataset alexnet.classifier[6] = nn.Linear(4096, 10) alexnet = alexnet.to(device) criterion = nn.CrossEntropyLoss() # Create list of params to learn params_to_learn = [] for name,param in alexnet.named_parameters(): if param.requires_grad == True: params_to_learn.append(param) optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True) . . Refer to this script on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from torchvision.datasets. . PyTorch has a very good tutorial on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook. . Conclusion . We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images. . References . https://github.com/soumith/imagenet-multiGPU.torch Helped in preprocessing of the Imagenet dataset. | https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html Many code references were taken from this tutorial. | https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf AlexNet paper |",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2019/02/18/post-0002.html",
            "relUrl": "/tutorial/2019/02/18/post-0002.html",
            "date": " • Feb 18, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Theoretical Machine Learning: Probabilistic and Statistical Math",
            "content": "I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here. . . Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as Polynomial Curve Fitting in ML. . In curve fitting, we usually use the maximum likelihood approach which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data). . To overcome overfitting we use regularization techniques by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin. . Quick Probability Overview . Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as X = Event that heads come and Y = Event that tails come. Now P(X) = Probability of that particular event happening. . Takeaway, represent your probabilities as events of something happening. . In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable. . Next, we discuss expectation. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value? . Suppose x takes the value 10, the average of that value is the number itself. . . Next, we discuss variance. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is. . Another important concept in probabilistic maths is the concept of prior, posterior and likelihood. . . Prior = P(X). It is probability available before we observing an event . Posterior = P(X|Y). It is the probability of X after event Y has happened. . Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X . Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X). . . Tip: Follow the resource list given at the end for further reading of the topic. . Gaussian Distribution . As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution. . . But before that let us discuss why we need to know about probability distributions in the first place? . When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output. . Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases). . . Note: From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule. . As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models? . After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial. . Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later. . . Note: In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights. . Now I present a general way of approaching the problem of finding the values for the variables of a prior. . The Gaussian Equation is represented as follow . $$ mathcal{N}(x| mu, sigma^2) = frac{1}{(2 pi sigma^2)^{1/2}}exp {- frac{1}{2 sigma^2}(x- mu)^2 }$$ . Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution. . So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed N values as input. Now all these N values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as . $$p(x| mu, sigma^2)= prod_{n=1}^{N} mathcal{N}(x_n| mu, sigma^2)$$ . After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function. . $$ ln p(x| mu, sigma^2)=- frac{1}{2 sigma^2} sum_{n=1}^{N}(x_n- mu)^2- frac{N}{2} ln sigma^2- frac{N}{2} ln (2 pi)$$ . Now take the derivative w.r.t the variables and get their values. . $$ mu_{ML}= frac{1}{N} prod_{n=1}^{N}x_n$$ $$ sigma^2_{ML}= frac{1}{N} prod_{n=1}{N}(x_n- mu_{ML})^2$$ . . Note: In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data. . Why was all this important? . All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior. . In cases where you have to take a prior like Poisson MSE would not be a good metric to consider. . Set Prior on the variables also . . Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution? . Ideally, you can choose any distribution. But practically you want to choose a conjugate prior for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors. . Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance. . Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition. . Here is the list of resources that I would highly recommend for learning ML:- . CS229: Machine Learning by Stanford | Pattern Recognition and Machine Learning by Christopher M. Bishop | An Introduction to Statistical Learning: with Applications in R | The Elements of Statistical Learning Data Mining, inference and Prediction |",
            "url": "https://kushajveersingh.github.io/blog/notes/machine-learning/2018/09/14/post-001.html",
            "relUrl": "/notes/machine-learning/2018/09/14/post-001.html",
            "date": " • Sep 14, 2018"
        }
        
    
  
    
        ,"post18": {
            "title": "Theoretical Machine Learning: Probabilistic and Statistical Math",
            "content": "I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here. . . Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as Polynomial Curve Fitting in ML. . In curve fitting, we usually use the maximum likelihood approach which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data). . To overcome overfitting we use regularization techniques by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin. . Quick Probability Overview . Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as X = Event that heads come and Y = Event that tails come. Now P(X) = Probability of that particular event happening. . Takeaway, represent your probabilities as events of something happening. . In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable. . Next, we discuss expectation. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value? . Suppose x takes the value 10, the average of that value is the number itself. . . Next, we discuss variance. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is. . Another important concept in probabilistic maths is the concept of prior, posterior and likelihood. . . Prior = P(X). It is probability available before we observing an event . Posterior = P(X|Y). It is the probability of X after event Y has happened. . Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X . Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X). . . Tip: Follow the resource list given at the end for further reading of the topic. . Gaussian Distribution . As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution. . . But before that let us discuss why we need to know about probability distributions in the first place? . When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output. . Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases). . . Note: From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule. . As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models? . After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial. . Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later. . . Note: In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights. . Now I present a general way of approaching the problem of finding the values for the variables of a prior. . The Gaussian Equation is represented as follow . $$ mathcal{N}(x| mu, sigma^2) = frac{1}{(2 pi sigma^2)^{1/2}}exp {- frac{1}{2 sigma^2}(x- mu)^2 }$$ . Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution. . So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed N values as input. Now all these N values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as . $$p(x| mu, sigma^2)= prod_{n=1}^{N} mathcal{N}(x_n| mu, sigma^2)$$ . After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function. . $$ ln p(x| mu, sigma^2)=- frac{1}{2 sigma^2} sum_{n=1}^{N}(x_n- mu)^2- frac{N}{2} ln sigma^2- frac{N}{2} ln (2 pi)$$ . Now take the derivative w.r.t the variables and get their values. . $$ mu_{ML}= frac{1}{N} prod_{n=1}^{N}x_n$$ $$ sigma^2_{ML}= frac{1}{N} prod_{n=1}{N}(x_n- mu_{ML})^2$$ . . Note: In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data. . Why was all this important? . All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior. . In cases where you have to take a prior like Poisson MSE would not be a good metric to consider. . Set Prior on the variables also . . Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution? . Ideally, you can choose any distribution. But practically you want to choose a conjugate prior for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors. . Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance. . Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition. . Here is the list of resources that I would highly recommend for learning ML:- . CS229: Machine Learning by Stanford | Pattern Recognition and Machine Learning by Christopher M. Bishop | An Introduction to Statistical Learning: with Applications in R | The Elements of Statistical Learning Data Mining, inference and Prediction |",
            "url": "https://kushajveersingh.github.io/blog/notes/machine-learning/2018/09/14/post-0001.html",
            "relUrl": "/notes/machine-learning/2018/09/14/post-0001.html",
            "date": " • Sep 14, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Check README for a summary of all my work with relevant links. Resume. I am active on fastai forums and pytorch forums. . Deep Learning Researcher with interest in Computer Vision, Natural Language Processing and Reinforcement Learning. Have worked with GANs, style transfer, image-to-image translation, colorizing segmentation maps, object detection, classification. Currently trying to expand my knowledge in the field of Computer Vision by exploring new topics in the field and experimenting with the stuff. A fan of fastai courses and an active member of fastai community. . Completed B.Tech. in Electronics and Communication Engineering from Punjab Engineering College, Chandigarh, India (2016-2020). Won Techgig Code Gladiators competition in Artificial Intelligence theme. I was also part of Skill India program where I participated in IT Software Solutions for Business in 2018 and made it to the North-Zone Nationals. . I am a national level handball player having played handball for five years from class 8 to 12. . You can reach me at kushajreal@gmail.com. .",
          "url": "https://kushajveersingh.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Notebooks",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Summary of few of my jupyter notebooks . Mish activation function is tested for transfer learning. Here mish is used only in the last fully-connected layers of a pretrainened Resnet50 model. I test the activation function of CIFAR10, CIFAR100 using three different learning rate values. I found that Mish gave better results than ReLU. notebook, paper . | Multi Sample Dropout is implemented and tested on CIFAR-100 using cyclic learning. My losses converged 4x faster when using num_samples=8 than using simple dropout. notebook, paper . | Data Augmentation in Computer Vision Notebook implementing single image data augmentation techniques using just Python notebook | . | Summarizing Leslie N. Smith’s research in cyclic learning and hyper-parameter setting techniques. notebook A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay paper | Super-Convergence: Very Fast Training of Neural Networks Using Learning Rates paper | Exploring loss function topology with cyclical learning rates paper | Cyclical Learning Rates for Training Neural Networks paper | . | Photorealisitc Style Transfer. Implementation of the High-Resolution Network for Photorealistic Style Transfer paper. notebook, paper . | Weight Standardization is implemented and tested using cyclic learning. I find that it does not work well with cyclic learning when using CIFAR-10. notebook, paper . | Learning Rate Finder. Implementation of learning rate finder as introduced in the paper Cyclical Learning Rates for Training Neural Networks. A general template for custom models is provided. notebook . | PyTorch computer vision tutorial. AlexNet with tips and checks on how to train CNNs. The following things are included: notebook Dataloader creation | Plotting dataloader results | Weight Initialization | Simple training loop | Overfitting a mini-batch | . | How to deal with outliers . | How to choose number of bins in a histogram | .",
          "url": "https://kushajveersingh.github.io/blog/notebooks/",
          "relUrl": "/notebooks/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Unsupervised Parking Lot Detection . A complete approach to detect parking lot spaces from images and then tell which spaces are occupied or not. Here I do not use any dataset for training my model to detect parking spaces. My implementation can be divided into these three modules: . Object Detection Module :- Use COCO pretrained model, no need to do finetuning. | Label Processing Module :- As out model is not finetuned, there are some tricks that I add to overcome these limitations | Classification Module :- Use the processed labels/bounding_boxes to tell if that parking space is occupied or not. | . | SPADE by Nvidia . Unofficial implementation of SPDAE for image-to-translation from segmentation maps to the colored pictures. Due to compute limit I test it out for a simplified model on Cityscapes dataset and get descent results after 80 epochs with batch_size=2. . | Waste Seggregation using trashnet . Contains the code to train models for trashnet and then export them using ONNX. It was part of a bigger project where we ran these models on Rasberry Pi, which controlled wooden planks to classify the waste into different categories (code for rasberry pi not included here). . | Unscramble game . Python script to solve the unscramble android game. You are given 5 random letters and you have to find 3-letter, 4-letter, 5-letter english words from these 5 random letters. It is a simple brute force method with a english dictionary lookup. . | Random Duty List . A PHP and MySQL based work where the aim is to assign duties from a list to various stations and make sure the duties are not repeated and the repetition occurs only after the list is exhasuted. . | .",
          "url": "https://kushajveersingh.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushajveersingh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}