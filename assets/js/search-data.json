{
  
    
        "post0": {
            "title": "Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch",
            "content": "Link to jupyter notebook . Data . To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work. . . Note: - I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do. . Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use ImageNet Object Localization Challenge to directly download all the files (warning 155GB). | Unzip the tar.gz file using tar xzvf file_name -C destination_path. | In the Data/CLS-LOC folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders. | Use this command from your terminal in the val folder wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash. It would move all the images to their respective class folders. | As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^&gt;” {}. | After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???. . . Data Loading . Steps involved:- . Create a dataset class or use a predefined class | Choose what transforms you want to perform on the data. | Create data loaders | . # collapse-show train_dir = &#39;../../../Data/ILSVRC2012/train&#39; val_dir = &#39;../../../Data/ILSVRC2012/val&#39; size = 224 batch_size = 32 num_workers = 8 data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]), &#39;val&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) } image_datasets = { &#39;train&#39;: ImageFolder(train_dir, transform=data_transforms[&#39;train&#39;]), &#39;val&#39;: ImageFolder(val_dir, transform=data_transforms[&#39;val&#39;]), } data_loader = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in [&#39;train&#39;, &#39;val&#39;] } . . The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step. . Check dataloaders . After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images. . # collapse-show # As our images are normalized we have to denormalize them and # convert them to numpy arrays. def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std*img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) #Pause is necessary to display images correctly images, labels = next(iter(data_loader[&#39;train&#39;])) grid_img = make_grid(images[:4], nrow=4) imshow(grid_img, title = [labels_list[x] for x in labels[:4]]) . . One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file LOC_synset_mapping.txt. . # collapse-show f = open(&quot;../../Data/LOC_synset_mapping.txt&quot;, &quot;r&quot;) labels_dict = {} # Get class label by folder name labels_list = [] # Get class label by indexing for line in f: split = line.split(&#39; &#39;, maxsplit=1) split[1] = split[1][:-1] label_id, label = split[0], split[1] labels_dict[label_id] = label labels_list.append(split[1]) . . Model Construction . Create your model. Pretrained models covered at the end of the post. . . Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’. . # collapse-show class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.conv_base = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False), nn.BatchNorm2d(96), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc_base = nn.Sequential( nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.conv_base(x) x = x.view(x.size(0), 256*6*6) x = self.fc_base(x) return x . . See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization). . Best practices for CNN . Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always. | Weight Initialization:- Use He initialization as default with ReLU. PyTorch provides kaimingnormal for this purpose. | Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. | Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model. | Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good. | Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference. | Weight Initialization . Do not use this method as a default. After, naming the layers you can do this very easily. . # collapse-show conv_list = [0, 4, 8, 10, 12] fc_list = [1, 4, 6] for i in conv_list: torch.nn.init.kaiming_normal_(model.conv_base[i].weight) for i in fc_list: torch.nn.init.kaiming_normal_(model.fc_base[i].weight) . . Create optimizers, schedulers and loss functions. . # collapse-show device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) # Cross entropy loss takes the logits directly, so we don&#39;t need to apply softmax in our CNN criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;max&#39;, verbose=True) . . PyTorch specific discussion . You have to specify the padding yourself. Check this thread for discussion on this topic. | Create the optimizer after moving the model to GPU. | The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself. | Do not forget to zero the grads. | . How to check my model is correct? . Check 1 . The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input. . inputs, labels = next(iter(data_loader[&#39;train&#39;])) inputs = inputs.to(device) labels = labels.to(device) criterion_check1 = nn.CrossEntropyLoss() optimizer_check1 = optim.SGD(model.parameters(), lr=0.001) model.train() for epoch in range(200): optimizer_check1.zero_grad() outputs = model(inputs) loss = criterion_check1(outputs, labels) _, preds = torch.max(outputs, 1) loss.backward() optimizer_check1.step() if epoch%10 == 0: print(&#39;Epoch {}: Loss = {} Accuracy = {}&#39;.format(epoch+1, loss.item(), torch.sum(preds == labels))) . . . Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again. . Check 2 . Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems. . In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs. . Using Pretrained Models . As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset. . . Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended. . # collapse-show data = np.load(&#39;../../../Data/cifar_10.npz&#39;) alexnet = torch.load(&#39;../../../Data/Pytorch Trained Models/alexnet.pth&#39;) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) x_train = torch.from_numpy(data[&#39;train_data&#39;]) y_train = torch.from_numpy(data[&#39;train_labels&#39;]) x_test = torch.from_numpy(data[&#39;test_data&#39;]) y_test = torch.from_numpy(data[&#39;test_labels&#39;]) . . # collapse-show # Create data loader class CIFAR_Dataset(torch.utils.data.Dataset): &quot;&quot;&quot; Generally you would not load images in the __init__ as it forces the images to load into memory. Instead you should load the images in getitem function, but as CIFAR is small dataset I load all the images in memory. &quot;&quot;&quot; def __init__(self, x, y, transform=None): self.x = x self.y = y self.transform = transform def __len__(self): return self.x.size(0) def __getitem__(self, idx): image = self.x[idx] label = self.y[idx].item() if self.transform: image = self.transform(image) return (image, label) data_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) datasets = { &#39;train&#39;: CIFAR_Dataset(x_train, y_train, transform=data_transforms), &#39;test&#39;: CIFAR_Dataset(x_test, y_test, transform=data_transforms) } data_loader = { x: torch.utils.data.DataLoader(datasets[x], batch_size=64, shuffle=True, num_workers=8) for x in [&#39;train&#39;, &#39;test&#39;] } . . # collapse-show # Freeze conv layers for param in alexnet.parameters(): param.requires_grad = False # Initialize the last layer of alexnet model for out 10 class dataset alexnet.classifier[6] = nn.Linear(4096, 10) alexnet = alexnet.to(device) criterion = nn.CrossEntropyLoss() # Create list of params to learn params_to_learn = [] for name,param in alexnet.named_parameters(): if param.requires_grad == True: params_to_learn.append(param) optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True) . . Refer to this script on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from torchvision.datasets. . PyTorch has a very good tutorial on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook. . Conclusion . We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images. . References . https://github.com/soumith/imagenet-multiGPU.torch Helped in preprocessing of the Imagenet dataset. | https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html Many code references were taken from this tutorial. | https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf AlexNet paper |",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2019/02/18/post-002.html",
            "relUrl": "/tutorial/2019/02/18/post-002.html",
            "date": " • Feb 18, 2019"
        }
        
    
  
    
        ,"post1": {
            "title": "Theoretical Machine Learning: Probabilistic and Statistical Math",
            "content": "I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here. . . Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as Polynomial Curve Fitting in ML. . In curve fitting, we usually use the maximum likelihood approach which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data). . To overcome overfitting we use regularization techniques by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin. . Quick Probability Overview . Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as X = Event that heads come and Y = Event that tails come. Now P(X) = Probability of that particular event happening. . Takeaway, represent your probabilities as events of something happening. . In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable. . Next, we discuss expectation. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value? . Suppose x takes the value 10, the average of that value is the number itself. . . Next, we discuss variance. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is. . Another important concept in probabilistic maths is the concept of prior, posterior and likelihood. . . Prior = P(X). It is probability available before we observing an event . Posterior = P(X|Y). It is the probability of X after event Y has happened. . Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X . Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X). . . Tip: Follow the resource list given at the end for further reading of the topic. . Gaussian Distribution . As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution. . . But before that let us discuss why we need to know about probability distributions in the first place? . When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output. . Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases). . . Note: From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule. . As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models? . After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial. . Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later. . . Note: In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights. . Now I present a general way of approaching the problem of finding the values for the variables of a prior. . The Gaussian Equation is represented as follow . $$ mathcal{N}(x| mu, sigma^2) = frac{1}{(2 pi sigma^2)^{1/2}}exp {- frac{1}{2 sigma^2}(x- mu)^2 }$$ . Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution. . So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed N values as input. Now all these N values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as . $$p(x| mu, sigma^2)= prod_{n=1}^{N} mathcal{N}(x_n| mu, sigma^2)$$ . After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function. . $$ ln p(x| mu, sigma^2)=- frac{1}{2 sigma^2} sum_{n=1}^{N}(x_n- mu)^2- frac{N}{2} ln sigma^2- frac{N}{2} ln (2 pi)$$ . Now take the derivative w.r.t the variables and get their values. . $$ mu_{ML}= frac{1}{N} prod_{n=1}^{N}x_n$$ $$ sigma^2_{ML}= frac{1}{N} prod_{n=1}{N}(x_n- mu_{ML})^2$$ . . Note: In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data. . Why was all this important? . All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior. . In cases where you have to take a prior like Poisson MSE would not be a good metric to consider. . Set Prior on the variables also . . Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution? . Ideally, you can choose any distribution. But practically you want to choose a conjugate prior for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors. . Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance. . Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition. . Here is the list of resources that I would highly recommend for learning ML:- . CS229: Machine Learning by Stanford | Pattern Recognition and Machine Learning by Christopher M. Bishop | An Introduction to Statistical Learning: with Applications in R | The Elements of Statistical Learning Data Mining, inference and Prediction |",
            "url": "https://kushajveersingh.github.io/blog/machine-learning/2018/09/14/post-001.html",
            "relUrl": "/machine-learning/2018/09/14/post-001.html",
            "date": " • Sep 14, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Check README for a summary of all my work with relevant links. Resume. I am active on fastai forums and pytorch forums. . Deep Learning Researcher with interest in Computer Vision, Natural Language Processing and Reinforcement Learning. Have worked with GANs, style transfer, image-to-image translation, colorizing segmentation maps, object detection, classification. Currently trying to expand my knowledge in the field of Computer Vision by exploring new topics in the field and experimenting with the stuff. A fan of fastai courses and an active member of fastai community. . Completed B.Tech. in Electronics and Communication Engineering from Punjab Engineering College, Chandigarh, India (2016-2020). Won Techgig Code Gladiators competition in Artificial Intelligence theme. I was also part of Skill India program where I participated in IT Software Solutions for Business in 2018 and made it to the North-Zone Nationals. . I am a national level handball player having played handball for five years from class 8 to 12. . You can reach me at kushajreal@gmail.com. .",
          "url": "https://kushajveersingh.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Notebooks",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Summary of few of my jupyter notebooks . Mish activation function is tested for transfer learning. Here mish is used only in the last fully-connected layers of a pretrainened Resnet50 model. I test the activation function of CIFAR10, CIFAR100 using three different learning rate values. I found that Mish gave better results than ReLU. notebook, paper . | Multi Sample Dropout is implemented and tested on CIFAR-100 using cyclic learning. My losses converged 4x faster when using num_samples=8 than using simple dropout. notebook, paper . | Data Augmentation in Computer Vision Notebook implementing single image data augmentation techniques using just Python notebook | . | Summarizing Leslie N. Smith’s research in cyclic learning and hyper-parameter setting techniques. notebook A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay paper | Super-Convergence: Very Fast Training of Neural Networks Using Learning Rates paper | Exploring loss function topology with cyclical learning rates paper | Cyclical Learning Rates for Training Neural Networks paper | . | Photorealisitc Style Transfer. Implementation of the High-Resolution Network for Photorealistic Style Transfer paper. notebook, paper . | Weight Standardization is implemented and tested using cyclic learning. I find that it does not work well with cyclic learning when using CIFAR-10. notebook, paper . | Learning Rate Finder. Implementation of learning rate finder as introduced in the paper Cyclical Learning Rates for Training Neural Networks. A general template for custom models is provided. notebook . | PyTorch computer vision tutorial. AlexNet with tips and checks on how to train CNNs. The following things are included: notebook Dataloader creation | Plotting dataloader results | Weight Initialization | Simple training loop | Overfitting a mini-batch | . | How to deal with outliers . | How to choose number of bins in a histogram | .",
          "url": "https://kushajveersingh.github.io/blog/notebooks/",
          "relUrl": "/notebooks/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Unsupervised Parking Lot Detection . A complete approach to detect parking lot spaces from images and then tell which spaces are occupied or not. Here I do not use any dataset for training my model to detect parking spaces. My implementation can be divided into these three modules: . Object Detection Module :- Use COCO pretrained model, no need to do finetuning. | Label Processing Module :- As out model is not finetuned, there are some tricks that I add to overcome these limitations | Classification Module :- Use the processed labels/bounding_boxes to tell if that parking space is occupied or not. | . | SPADE by Nvidia . Unofficial implementation of SPDAE for image-to-translation from segmentation maps to the colored pictures. Due to compute limit I test it out for a simplified model on Cityscapes dataset and get descent results after 80 epochs with batch_size=2. . | Waste Seggregation using trashnet . Contains the code to train models for trashnet and then export them using ONNX. It was part of a bigger project where we ran these models on Rasberry Pi, which controlled wooden planks to classify the waste into different categories (code for rasberry pi not included here). . | Unscramble game . Python script to solve the unscramble android game. You are given 5 random letters and you have to find 3-letter, 4-letter, 5-letter english words from these 5 random letters. It is a simple brute force method with a english dictionary lookup. . | Random Duty List . A PHP and MySQL based work where the aim is to assign duties from a list to various stations and make sure the duties are not repeated and the repetition occurs only after the list is exhasuted. . | .",
          "url": "https://kushajveersingh.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushajveersingh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}