{
  
    
  
    
  
    
  
    
  
    
  
    
  
    
        "post6": {
            "title": "ImageNet Dataset Advancements",
            "content": "ImageNet database . Source: http://www.image-net.org/ | Paper: &quot;ImageNet: A large-scale hierarchical image database&quot; https://ieeexplore.ieee.org/document/5206848 | . Quoting from ILSVRC paper . ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). . ImageNet-1k dataset . Source: http://image-net.org/challenges/LSVRC/2012/index | Paper: &quot;ImageNet Large Scale Visual Recognition Challenge&quot; https://arxiv.org/abs/1409.0575 | rwightman/pytorch-image-models: results-imagenet.csv | . The standard 50,000 image ImageNet-1k dataset. . The ILSVRC (ImageNet Large Scale Visual Recognition Challenge) ran from 2010-2017. This challenge provided the teams with a subset of ImageNet database, called ILSVRC-2012 or ImageNet-1k or ImageNet (I think ILSVRC-2012 is the correct name, but people also refer to this dataset by the later two names). . Dataset creation process . Selecting categories:- The 1000 categories were manually (based on heuristics related to WordNet hierarchy). Also, to include fine-grained classification in the dataset the authors included 120 categories of dog breeds (this is why ImageNet models generally dream about dogs). . | Selecting candidate images:- Taken directly from ImageNet database. They basically did search queries for each category (synset) on several image search engines. The queries were also translated to Chinese, Spanish, Dutch and Italian to increase the diversity of the images. . | . Important: Step 2 introduces the problem of inaccurate annotations because we don&#8217;t know whether the search engines are correct or not. . Annotating images:- Amazon Mechanical Turk (AMT) was used to label the images. Each user was a given a set of candidate images and the definition of the target category (synset). The users were then asked to verify if the image contained the category. There was also a quality control system setup which you can read in the paper. | Problems with ImageNet . Are we done with ImageNet? goes into the details. But the main problem is the classification task. There are a lot of images in the dataset which have multiple classes or classes with multiple meanings. This is shown in the figure below taken from the above paper. . . From the figure, we can see that multi-label classification would be a better option to train on the dataset. Deep learning models are generally considered to be robust to some of the noise, so maybe that is why we can still train using the classification and get the awesome results. . One thing that the above image shows is that blindly following the top-1 accuracy on validation dataset is not a good idea. As at that point a model is simply learning to overfit or learn which class to predict for the validation images. . We cannot do anything about the training dataset. As collecting new labels for the images would be a big project on its own, but we can try to test if the model really generalized or not by coming up with newer validation datasets. For some datasets we would prioritize robustness, as generalization also means that a model should be robust to unseen changes in the training dataset. . Another problem is we do not have access to ImageNet test data. This means people have to resort to validation results to infer which model works better (in terms of accuracy). The main problem here is the extensive hyperparameter tuning on the validation set. . ImageNetv2 Matched Frequency . Source: https://github.com/modestyachts/ImageNetV2 | Paper: &quot;Do ImageNet Classifiers Generalize to ImageNet?&quot; - https://arxiv.org/abs/1902.10811 | rwightman/pytorch-image-models: results-imagenetv2-matched-frequency.csv | . An ImageNet test set of 10,000 images sampled from new images. Care was taken to replicate the original ImageNet curation/sampling process. . In the paper, the authors observed a drop of 11%-14% in accuracy for the models they tested. The main reason for this is extensive hyperparameter tuning on the validation set. . This paper solves this problem by collecting 10,000 new images (10 for each class) from Flickr. These images are much harder than the original ImageNet validation images. There are three versions of the dataset available which you can check on source link (the difference is in the method to select the 10 images for each class). MatchedFrequency dataset is used in the rwightman repo. . ImageNet-Sketch . Source: https://github.com/HaohanWang/ImageNet-Sketch | Paper: &quot;Learning Robust Global Representations by Penalizing Local Predictive Power&quot; https://arxiv.org/abs/1905.13549 | rwightman/pytorch-image-models: results-sketch.csv | . 50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. . . In this dataset we penalize the predictive power of the networks by discarding predictive signals such as color and texture that can be obtained from local receptive fields and rely instead on the global structure of the image. . This dataset basically consists of black and white sketches, doodles of the 1000 classes. This dataset focuses on model robustness by defining robustness to generalize to structure of the categories (i.e. low-frequency signal). . I don&#39;t think high accuracy on this dataset should be the primary goal. The reasoning being the hardware has gone pretty strong and using RGB images is not that expensive, so I don&#39;t see any point as to why we should penalize our models by taking out color and texture to check the robustness of the model (as we will never have that input during inference). Instead we should find ways to check robustness in the original RGB domain (check the next datasets for this). . ImageNet-A / ImageNet-O . Source: https://github.com/hendrycks/natural-adv-examples | Paper: &quot;Natural Adversarial Examples&quot; - https://arxiv.org/abs/1907.07174 | rwightman/pytorch-image-models: results-imagenet-a.csv | . 7500 images covering 200 of the 1000 classes. The images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. Typical ResNet-50 will score 0% top-1 accuracy on this dataset. . There are two datasets introduced in the paper with different purposes . ImageNet-Adversarial (ImageNet-A): Contains 7500 images which are naturally adversarial (200 classes out of 1000 in ImageNet). Classifiers should be able to classify the images correctly. | ImageNet-Out-of-Distribution-Detection (ImageNet-O): Contains 2000 images with classes that are not in ImageNet-1k dataset (out-of-distribution). Classifiers should output low-confidence predictions on the images. | How ImageNet-A is constructed? . First, a lot of images for the 200 classes of ImageNet were collected from the Internet. Then all the images correctly classified by ResNet-50 are removed from the dataset (reason for 0% top-1 acc using ResNet-50). Finally, a subset of high quality images are selected for the final dataset. . How ImageNet-O is constructed? . ImageNet database was used to get the images (excluding the 1000 classes in ImageNet-1k). Then ResNet-50 is used to select the images for which the model predicts high-confidence predictions. . ImageNet-A and ImageNet-O are good datasets to check the robustness of the models. ImageNet-A can be tested automatically. In case of ImageNet-O we have to come up with our own evaluation strategy. Manually looking at the predictions is possible. Plotting a histogram of model confidence predictions is also a possibility to get a sense of the confidence values. . I want to test a new thing for image classifier models, where we use sigmoid instead of softmax. The problem with softmax is that it forces one prediction to a large value, where as with sigmoid all the predictions are independent of each other. This can also help counter the problem of multiple classes in images of ImageNet dataset. I still have to explore this method, on how to train the model. . ImageNet-C / ImageNet-P . Source: https://github.com/hendrycks/robustness | Paper: &quot;Benchmarking Neural Network Robustness to Common Corruptions and Perturbations&quot; https://arxiv.org/abs/1903.12261 | . Evaluates performance on common corruptions and perturbations that may happen in real-life. 3.75 million images in ImageNet-C. . How ImageNet-C is constructed? . ImageNet-C consists of 15 common corruptions (Gaussian Noise, Shot Noise, Impulse Noise, Defocus Blur, Frosted Glass Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog, Brightness, Contrast, Elastic, Pixelate, JPEG) with 5 levels of severity, resulting in 75 distinct corruptions for each image. These corruptions are applied to all 50,000 validation images of the original ImageNet-1k dataset, resulting in 3.75 million images. . This assumes that you did not use any of these corruptions as data augmentation during the training phase. . How ImageNet-P is constructed? . Perturbations hear mean applying the same corruption successively on the previous applied image. This dataset measures classifier&#39;s prediction stability, reliability and consistency in case of minor change in input image. . For each image we generate 30 frames of perturbation (from 10 corruptions) 5 levels of severity resulting in total of 7.5 million images. Starting with a clean ImageNet image apply brighness corruption to it and then apply brightness corruption on the current image and keep doing it for 30 times. . In the paper, the authors also introduce a metric for ImageNet-P. Check the paper for its details. . The only problem with this dataset is size. The authors of the paper have also created a new dataset ImageNet-R which we can use instead. . ImageNet-&quot;Real Labels&quot; . Source: https://github.com/google-research/reassessed-imagenet | Paper: &quot;Are we done with ImageNet?&quot; - https://arxiv.org/abs/2006.07159 | rwightman/pytorch-image-models: results-imagenet-real.csv | . New labels for the original ImageNet-1k intended to improve on mistakes in the original annotation process. . This dataset can be easily summarized with Figure 3. . . This dataset provides new labels for the validation set of original ImageNet-1k dataset (50,000 images). . In the paper, the authors propose a new metric Real accuracy as we cannot use top-1 accuracy for this multi-label dataset. This metric measures the precision of the model&#39;s top-1 prediction, which is deemed correct if it is included in the set of labels, and incorrect otherwise. . ImageNet-Rendition . Source: https://github.com/hendrycks/imagenet-r | Paper: &quot;The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization&quot; - https://arxiv.org/abs/2006.16241 | rwightman/pytorch-image-models: results-imagenet-r.csv | . Renditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. . The dataset consists of artistic renditions (art, cartoons, graffiti, embroidery, graphics, origami, paintings, patterns, sculptures and more) of object classes. These kind of images are not included in the original ImageNet dataset (it consists of photos of real objects only). . DeepAugment . This paper also introduces a new data augmentation technique called DeepAugment. Take any image-to-image network (like image autoencoder or a superresolution network) and pass the images through it. Now, distort the internal weights of the network (zeroing, negating, transposing, ...) and the images you get as output can be used for training. . I highly recommend reading this paper. . What to use? . For the training phase, I would still use the original ImageNet-1k validation dataset. The top-1 accuracy on this dataset does not matter much due to incorrect labels. Next I would use ImageNetv2 Matched Frequency dataset as a proxy of test set. In the end, when everything is done I would get the results of ImageNet Real labels dataset to get a real sense of the accuracy of the model. . To test the robustness of the model, I would work with ImageNet-Adversarial and ImageNet-Rendition datasets. With ImageNet-A we can get the accuracy of the model for really hard images. With ImageNet-R we can get results for out-of-distribution images. If the results on these datasets are satisfactory ImageNet-O can also be used for further testing. . References . rwightman/pytorch-image-models link | WordNet: a lexical database for English link | ImageNet: A large-scale hierarchical image database link | ImageNet Large Scale Visual Recognition Challenge link | Learning Robust Global Representations by Penalizing Local Predictive Power link | Natural Adversarial Examples link | Benchmarking Neural Network Robustness to Common Corruptions and Perturbations link | Are we done with ImageNet? link | The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization link | .",
            "url": "https://kushajveersingh.github.io/blog/imagenet-dataset-advancements",
            "relUrl": "/imagenet-dataset-advancements",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Deep Learning Model Initialization in Detail",
            "content": "The following papers are discussed in this post: . Understanding the difficulty of training deep feedforward neural networks paper | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification paper | import gzip, pickle, math from pathlib import Path import numpy as np import matplotlib.pyplot as plt import seaborn as sns from fastai2.data.external import download_data # if you do not have fastai2, you can # download the dataset manually and place # in the extra folder import torch import torch.nn as nn import torch.nn.functional as F . Get Data . MNIST dataset is used for quick experimentation. The things discussed in the post are independent of the dataset used. . def get_data(): # if you do not have fastai, you can download the dataset directly from the given URL MNIST_URL = &#39;http://deeplearning.net/data/mnist/mnist.pkl.gz&#39; path = download_data(MNIST_URL, &#39;extra/mnist.pkl.gz&#39;) with gzip.open(path, &#39;rb&#39;) as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=&#39;latin-1&#39;) # By default these are numpy arrays, so convert them to pytorch tensors return map(torch.tensor, (x_train,y_train,x_valid,y_valid)) def normalize(x, m, s): return (x-m)/s . x_train, y_train, x_valid, y_valid = get_data() . train_mean, train_std = x_train.mean(), x_train.std() x_train = normalize(x_train, train_mean, train_std) x_valid = normalize(x_valid, train_mean, train_std) # use training stats for validation set also . x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . (torch.Size([50000, 784]), torch.Size([50000]), torch.Size([10000, 784]), torch.Size([10000])) . x_train.mean(), x_train.std(), x_valid.mean(), x_valid.std() . (tensor(-7.6999e-06), tensor(1.), tensor(-0.0059), tensor(0.9924)) . Now the input comes from a distribution of $mean = 0$ and $std = 1$. . Objective . We want activations in the neural network to have mean=0, std=1. By activation I mean the output of activation_function(Linear(x, w, b)). Convolution layer can also be represented as a linear layer (take a vector of 0&#39;s, replace the 0&#39;s where the convolution filter would be applied). . . Important: Batch Normalization is not used in this post. BatchNorm is still needed because if we just use Kaiming init in ResNet, the weights would explode. This happens due to the shortcut connection which doubles the variance, thus exponentially increasing the weights. . Kaiming Init (Code) . Kaiming Init (He Init) is the most commonly used initialization. It&#39;s implementation is as follows: . w1 = torch.randn(784, 50) * math.sqrt(2/784) b1 = torch.zeros(50) w2 = torch.randn(50,1) * math.sqrt(2/50) b2 = torch.zeros(1) . And that&#39;s it. You sample from a Normal Gaussian Distribution (mean=0, std=1) and then change it&#39;s standard deviation to math.sqrt(2/num_inputs). This preserves your activations std in the forward propagation. . In PyTorch you can implement this using nn.init.kaiming_normal_(...). The implementation details will be discussed in the end. . Now we have got the bigger picture that initialization is just multiplying by some value. Let&#39;s get into details of initialization. . Why Initialization is Important? . Example 1 . Pass the input through linear and relu layers and see the standard deviation of the outputs. . m = 784 nh = 10 #Num hidden units . w1 = torch.randn(m,nh) b1 = torch.zeros(nh) # bias initialized to 0 w2 = torch.randn(nh,1) b2 = torch.zeros(1) . def stats(x): # Utility function to print mean and std return x.mean(), x.std() . stats(x_valid) . (tensor(-0.0059), tensor(0.9924)) . def lin(x, w, b): return x@w + b . t = lin(x_valid, w1, b1) stats(t) . (tensor(-2.8136), tensor(26.8307)) . The mean and std are way off from 0 and 1 respectively. Now we can take ReLU of this. . def relu(x): return x.clamp_min(0.) . t = relu(t) stats(t) . (tensor(9.3641), tensor(14.4010)) . We can see from this example, why initialization is important. After, just one layer(relu+linear) our mean and standard deviation have diverged from their initial value of 0 and 1. . Now imagine if we had 100 of these layers. Let&#39;s try that now. . Example 2 . To understand why initialization is important in a neural net, we&#39;ll focus on the basic operation you have there: matrix multipications. So let&#39;s just take a vector x, and a matrix a initialized randomly, then multiply them 100 times (as if we had 100 layers). . x = torch.randn(512) a = torch.randn(512, 512) . for i in range(100): x = a@x . stats(x) . (tensor(nan), tensor(nan)) . The problem we get is of activation explosion. Very soon our activations go to nan i.e. their values go to a large value that cannot be represented in memory. We can see when that happens. . x = torch.randn(512) a = torch.randn(512, 512) for i in range(100): x = a@x if x.mean() != x.mean(): break # nan != nan i . 27 . So it only took 27 multipications. Now only possible solution to mitigate this problem is to reduce the scale of the matrix a. . x = torch.randn(512) a = torch.randn(512,512)*0.01 for i in range(100): x = a@x . stats(x) . (tensor(0.), tensor(0.)) . Now we got the problem of vanishing gradients where our activations vanish to 0. . We can try Xavier initialization to solve this problem. (Xavier init is same as Kaiming init except in the numerator we have 1 instead of 2). . x = torch.randn(512) a = torch.randn(512,512)*math.sqrt(1/512) for i in range(100): x = a@x . stats(x) . (tensor(-0.1040), tensor(2.8382)) . It works. . We can try this experiment by adding relu also, in which case we would use Kaiming init. . The difference between Xavier and Kaiming init is . Xavier Init: Only Linear layer is considered | Kaiming Init: Linear + ReLU layer is considered | . x = torch.randn(512) a = torch.randn(512,512)*math.sqrt(2/512) for i in range(100): x = a@x x = x.clamp_min(0.) stats(x) . (tensor(1.2160), tensor(1.7869)) . And it works. The activations did not explode nor vanish. . Example 3 . Train a simple feedforward network on MNIST dataset and visualize the histogram of the activation values. . def lin_relu(c_in, c_out): return nn.Sequential( nn.Linear(c_in, c_out), nn.ReLU(inplace=True), ) class Net(nn.Module): def __init__(self): super().__init__() # Create a linear-relu model self.model = nn.Sequential( *lin_relu(784, 1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), nn.Linear(1000,10), # No need of softmax as it will be in the loss function ) self.initialize_model() def forward(self, x): return self.model(x) def initialize_model(self): # Before Xavier init, people used to sample weights from # a uniform distribution for layer in self.model: if isinstance(layer, nn.Linear): value = 1./math.sqrt(layer.weight.shape[1]) layer.weight.data.uniform_(-value, value) layer.bias.data.fill_(0) net = Net() net.model . Sequential( (0): Linear(in_features=784, out_features=1000, bias=True) (1): ReLU(inplace=True) (2): Linear(in_features=1000, out_features=1000, bias=True) (3): ReLU(inplace=True) (4): Linear(in_features=1000, out_features=1000, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=1000, out_features=1000, bias=True) (7): ReLU(inplace=True) (8): Linear(in_features=1000, out_features=1000, bias=True) (9): ReLU(inplace=True) (10): Linear(in_features=1000, out_features=10, bias=True) ) . bs = 128 device = torch.device(&#39;cuda&#39;) net = net.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) dataset = torch.utils.data.TensorDataset(x_train, y_train) dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) . for epoch in range(70): running_loss = .0 for i, data in enumerate(dataloader,0): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch+1}: {running_loss/len(dataloader)}&#39;) . 1: 2.2928382635116575 2: 2.2065723312206758 3: 1.194048885657237 4: 0.5237417409817378 5: 0.3836347458072198 6: 0.3146293921730457 7: 0.2697077517135021 8: 0.2339217932942586 9: 0.20382011488844187 10: 0.17836485357047654 11: 0.1592684537745439 12: 0.14289972114448363 13: 0.1286666537133547 14: 0.11684789568758928 15: 0.10570534034990348 16: 0.09746135014754076 17: 0.08864978084770533 18: 0.08108058688827814 19: 0.07408739045166816 20: 0.06816183782349794 21: 0.06298899033512824 22: 0.05734651746849219 23: 0.05177393974497532 24: 0.047053486519517046 25: 0.04282796499438775 26: 0.039077873800236446 27: 0.03567600295138665 28: 0.03188744994023671 29: 0.029368048552901316 30: 0.02597655968215221 31: 0.023195109860255168 32: 0.020861454260272857 33: 0.018848492100070686 34: 0.017209559499930877 35: 0.015161331571065462 36: 0.013735617821415266 37: 0.012041004417607417 38: 0.010973294661977353 39: 0.010113576904703409 40: 0.009030632378581243 41: 0.008074290845065545 42: 0.007224243325300706 43: 0.006513816581513637 44: 0.0059723575384571 45: 0.00554538136586929 46: 0.004938531619233963 47: 0.004390296693413685 48: 0.004122716288727063 49: 0.0038293207541872294 50: 0.0034256023139907763 51: 0.003229047467884345 52: 0.002975824174399559 53: 0.002749748910084749 54: 0.002592976238483038 55: 0.0023717502848460124 56: 0.0021688310572734247 57: 0.002044256757467221 58: 0.0019431500003123895 59: 0.0017717176236403294 60: 0.001694328805957085 61: 0.0015888030234819804 62: 0.001510932248754379 63: 0.0014456520477930705 64: 0.0013670944441587496 65: 0.0013064238218924939 66: 0.0012553433672739909 67: 0.0011863003556544965 68: 0.0011301248310468135 69: 0.0010923029807133552 70: 0.001042095409371914 . Now visualize the activation values by using a test image. . torch.save(net.state_dict(), &#39;uniform.pt&#39;) . actvs = {} net = net.to(&#39;cpu&#39;) images = x_valid[:10000] out = images.clone() for i in range(len(net.model)): if i in (1,3,5,7,9): out = net.model[i](out) actvs[i] = out.squeeze(0).detach().mean(axis=0).numpy() else: out = net.model[i](out) . Visualize the activations using a kdeplot. It is similar to histogram, but gives a better view of the data. Alternatively, histogram can also be plotted to see the effect. . fig, ax = plt.subplots(figsize=(15,5)) ax.set_xlim(-1, 1) for k,v in actvs.items(): sns.kdeplot(v, ax=ax, label=f&#39;Layer {k}&#39;) . Test above example with Kaiming Init . def lin_relu(c_in, c_out): return nn.Sequential( nn.Linear(c_in, c_out), nn.ReLU(inplace=True), ) class Net(nn.Module): def __init__(self): super().__init__() # Create a linear-relu model self.model = nn.Sequential( *lin_relu(784, 1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), *lin_relu(1000,1000), nn.Linear(1000,10), # No need of softmax as it will be in the loss function ) self.initialize_model() def forward(self, x): return self.model(x) def initialize_model(self): for layer in self.model: if isinstance(layer, nn.Linear): nn.init.kaiming_normal_(layer.weight.data, mode=&#39;fan_out&#39;) layer.bias.data.fill_(0) net = Net() . bs = 128 device = torch.device(&#39;cuda&#39;) net = net.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) dataset = torch.utils.data.TensorDataset(x_train, y_train) dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) . for epoch in range(70): running_loss = .0 for i, data in enumerate(dataloader,0): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch+1}: {running_loss/len(dataloader)}&#39;) . 1: 0.7095098866484104 2: 0.0923736249263852 3: 0.03983800867333626 4: 0.01766058729531673 5: 0.009082728055998301 6: 0.005672304465984687 7: 0.004172981537591953 8: 0.003315979051284301 9: 0.0027411463121191047 10: 0.0023460902225894806 11: 0.0020474239801749204 12: 0.0018151276721022067 13: 0.0016298005023063758 14: 0.0014792730888495077 15: 0.0013441605111345267 16: 0.0012390905131514256 17: 0.001146641082297533 18: 0.0010655345013126348 19: 0.0009982335691650708 20: 0.000935702603787948 21: 0.0008791005668731837 22: 0.0008290120758689366 23: 0.0007855443713756708 24: 0.0007456283538769453 25: 0.0007100430006782214 26: 0.0006764667108654976 27: 0.0006458146258806571 28: 0.0006196549830910488 29: 0.0005923675229916206 30: 0.0005684418173936697 31: 0.0005472387879704817 32: 0.0005264173046900676 33: 0.0005066754296422004 34: 0.0004908624558876722 35: 0.00047273215575095934 36: 0.0004578854984197861 37: 0.00044289983044832183 38: 0.0004285678219718811 39: 0.00041579040579306774 40: 0.0004036462746369533 41: 0.00039161906983607855 42: 0.000380707326798867 43: 0.0003695801998942326 44: 0.00035994008947641423 45: 0.00035058893263339996 46: 0.000341620133855404 47: 0.0003329002513335301 48: 0.0003244552044914319 49: 0.0003169629149712049 50: 0.0003093158348630636 51: 0.0003023809108596582 52: 0.0002947433923299496 53: 0.00028860634909226346 54: 0.00028179665215504477 55: 0.00027597242823013894 56: 0.00027051137712521433 57: 0.0002648675336669653 58: 0.0002595099023519418 59: 0.0002543628406830323 60: 0.0002490593359256402 61: 0.0002443719846315873 62: 0.0002396430533665877 63: 0.00023537879953017602 64: 0.00023074357364422237 65: 0.00022677934895723293 66: 0.00022275162239869436 67: 0.00021877073897765234 68: 0.00021507212748894325 69: 0.0002110987806167358 70: 0.00020773537839070344 . torch.save(net.state_dict(), &#39;kaiming.pt&#39;) . actvs = {} net = net.to(&#39;cpu&#39;) images = x_valid[:10000] out = images.clone() for i in range(len(net.model)): if i in (1,3,5,7,9): out = net.model[i](out) actvs[i] = out.squeeze(0).detach().mean(axis=0).numpy() else: out = net.model[i](out) . fig, ax = plt.subplots(figsize=(15,5)) ax.set_xlim(-1, 1) for k,v in actvs.items(): sns.distplot(v, ax=ax, label=f&#39;Layer {k}&#39;, hist=False) . We can see clear improvement from the previous figure. The activation values for later layers is greater than those observed in the previous figure. This shows we made some improvement. . Kaiming Init Detailed Derivation . I will derive the initialization formula for the forward pass only. In practice we always set the initialization values so as to maintain the mean, std for the forward pass. . Consider a layer $l$ $$ vec{y}_l = W_l vec{x}_l+ vec{b}_l$$ . $ vec{x}_l rightarrow$ is a $(n_l,1)$ vector that represents the activations of the previous layer $ vec{y}_{l-1}$ that were passed through an activation function $f$ i.e. $$ vec{x}_l = f( vec{y}_{l-1})$$ | $W_l rightarrow$ is a $(d_l,n_l)$ matrix from layer $l-1$ to layer $l$, with $d_l$ the number of filters of the convolutional layer | $ vec{b}_l rightarrow$ is a $(d_l,1)$ vector of biases of layer $l$ (initialized to 0) | $ vec{y}_l rightarrow$ is a $(d_l,1)$ vector of activations of layer l before they go through the activation function. | . . Note: Vectors are shown with an arrow on top of them. For matrices I do not use any special notation. . A few assumptions are made: . Each element in $W_l$ is mutually independent from other elements and all the elements come from the same distribution. | Same is true for $ vec{x}_l$. | $ vec{x}_l$ and $W_l$ are independent of each other. | . Compute Variance of $ vec{y}_l$ . . Note: This is a heavy math section. I have explained the complete derivation step-by-step, but I also assumed that you know the Variance and Expectation formulas and what independence means. The result is what matter the most, so you can skip this section if you want. . Our end goal is to have unit standard deviation in all layers of the model. To do so, we first compute the variance of the above equation. . $$Var[ vec{y}_l] = Var[W_l vec{x}_l] + Var[ vec{b}_l] + 2*Cov(W_l vec{x}_l, vec{b}_l)$$ . $Var[ vec{b}_l] rightarrow$ is 0, because $ vec{b}_l$ is initialized to a zero vector. | $Cov(W_l vec{x}_l, vec{b}_l) rightarrow$ is 0, because $ vec{b}_l$ is always initialized to 0, no matter what value the first argument takes. | . Therefore, $$Var[ vec{y}_l] = Var[W_l vec{x}_l]$$ . Let $W_l = (w_{i,j})_{1 leq i leq d_l, 1 leq j leq n_l}$, where all $w_{i,j}$ follow the same distribution. Similarly, let $ vec{x}_l = (x_j)_{1 leq j leq n_l}$, with all $x_j$ following the same distribution. . Plugging all this into the variance equation, we have $$ Var[ vec{y}_l] = Var left( begin{bmatrix} sum limits_{j=1}^{n_l}w_{1,j}x_j sum limits_{j=1}^{n_l}w_{2,j}x_j vdots sum limits_{j=1}^{n_l}w_{d_l,j}x_j end{bmatrix} right) $$ . To derive the variance of a random vector, check these videos for more details: . Expectations and variance of a random vector - part 2 link | Expectations and variance of a random vector - part 3 link | Variance of $ vec{y}_l$ is defined as $Var[ vec{y}_l] = mathbb{E}[( vec{y}_l - mu)( vec{y}_l - mu)^T]$, where $ mu$ is the expectation/mean of $ vec{y}_l$. This would give us $(n_l,n_l)$ matrix as shown below: . $$ Var[ vec{y}_l] = mathbb{E} left( begin{bmatrix} Var left( sum limits_{j=1}^{n_l}w_{1,j}x_j right) &amp; Cov left( sum limits_{j=1}^{n_l}w_{1,j}x_j, sum limits_{j=1}^{n_l}w_{2,j}x_j right) &amp; cdots &amp; Cov left( sum limits_{j=1}^{n_l}w_{1,j}x_j, sum limits_{j=1}^{n_l}w_{d_l,j}x_j right) Cov left( sum limits_{j=1}^{n_l}w_{2,j}x_j, sum limits_{j=1}^{n_l}w_{1,j}x_j right) &amp; Var left( sum limits_{j=1}^{n_l}w_{2,j}x_j right) &amp; cdots &amp; vdots vdots &amp; &amp; ddots&amp; Cov left( sum limits_{j=1}^{n_l}w_{d_l-1,j}x_j, sum limits_{j=1}^{n_l}w_{d_l,j}x_j right) Cov left( sum limits_{j=1}^{n_l}w_{d_l,j}x_j, sum limits_{j=1}^{n_l}w_{1,j}x_j right) &amp; cdots &amp; Cov left( sum limits_{j=1}^{n_l}w_{d_l,j}x_j, sum limits_{j=1}^{n_l}w_{d_l-1,j}x_j right) &amp; Var left( sum limits_{j=1}^{n_l}w_{d_l,j}x_j right) end{bmatrix} right) $$ Now $$Cov left( sum limits_{j=1}^{n_l}w_{k,j}x_j, sum limits_{j=1}^{n_l}w_{l,j}x_j right) = 0 text{ , for } k neq l$$ This is because of the assumption the three assumptions that we had defined earlier i.e. . Each element in $W_l$ is mutually independent from other elements and all the elements come from the same distribution. | Same is true for $ vec{x}_l$. | $ vec{x}_l$ and $W_l$ are independent of each other. | . Putting this into the above equation we have $$ Var[ vec{y}_l] = mathbb{E} left( begin{bmatrix} Var left( sum limits_{j=1}^{n_l}w_{1,j}x_j right) &amp; 0 &amp; cdots &amp; 0 0 &amp; Var left( sum limits_{j=1}^{n_l}w_{2,j}x_j right) &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; Var left( sum limits_{j=1}^{n_l}w_{d_l,j}x_j right) end{bmatrix} right) $$ . Because all the $w_{i,j}$ and $x_i$ are independent from each other, the variance of the sum is the sum of the variances. Thus: $$ Var[ vec{y}_l] = mathbb{E} left( begin{bmatrix} sum limits_{j=1}^{n_l}Var(w_{1,j}x_j) &amp; 0 &amp; cdots &amp; 0 0 &amp; sum limits_{j=1}^{n_l}Var(w_{2,j}x_j) &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; sum limits_{j=1}^{n_l}Var(w_{d_l,j}x_j) end{bmatrix} right) $$ . Now because all the $w_{i,j}$ and $x_i$ follow the same distribution (respectively), all the variances in the sum are equal. Thus: $$ Var[ vec{y}_l] = mathbb{E} left( begin{bmatrix} n_l*Var(w_1x_1) &amp; 0 &amp; cdots &amp; 0 0 &amp; n_l*Var(w_2x_2) &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; n_l*Var(w_{d_l}x_{d_l}) end{bmatrix} right) $$ . Next, expectation of a matrix is the expectation of each value (this is also true for a vector). $$ Var[ vec{y}_l] = left( begin{bmatrix} mathbb{E}[n_l*Var(w_1x_1)] &amp; mathbb{E}[0] &amp; cdots &amp; mathbb{E}[0] mathbb{E}[0] &amp; mathbb{E}[n_l*Var(w_2x_2)] &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; mathbb{E}[0] mathbb{E}[0] &amp; cdots &amp; mathbb{E}[0] &amp; mathbb{E}[n_l*Var(w_{d_l}x_{d_l})] end{bmatrix} right) $$ . Now, . $ mathbb{E}[n_l*Var(w_ix_i)] = n_l*Var(w_ix_i)$ because $n_l$ is constant and $Var(w_ix_i)$ is also a constant (variance tells us how far we are from the mean (technically it is standard deviation, but you get the point)). | $ mathbb{E}[0] = 0$ because 0 is a constant. | . Putting this in the equation, we have, $$ Var[ vec{y}_l] = left( begin{bmatrix} n_l*Var(w_1x_1) &amp; 0 &amp; cdots &amp; 0 0 &amp; n_l*Var(w_2x_2) &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; n_l*Var(w_{d_l}x_{d_l}) end{bmatrix} right) $$ . We can apply the same logic to expand $Var[ vec{y}_l]$ as we used to expand the right hand side (as demonstrated in the above equations). . The final equation we get is as follows: . $$ begin{bmatrix} y_1 &amp; 0 &amp; cdots &amp; 0 0 &amp; y_2 &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; y_{d_l} end{bmatrix} = $$$$ begin{bmatrix} n_l*Var(w_1x_1) &amp; 0 &amp; cdots &amp; 0 0 &amp; n_l*Var(w_2x_2) &amp; cdots &amp; vdots vdots &amp; &amp; ddots &amp; 0 0 &amp; cdots &amp; 0 &amp; n_l*Var(w_{d_l}x_{d_l}) end{bmatrix} $$ Comparing, both sides we get the value of variance for an activation $y_l$ $$Var[y_l] = n_l*Var[w_lx_l]$$ . Note: $y_l,x_l$ are scalars. $w_l$ is $(1,n_l)$ row-vector. To keep the notation same as the original paper, I did not use the vector notation. From this point on it does not matter much that $w_l$ is a row-vector or not. For the derivation below, I drop subscript l (as it would just clutter the equations). I use the standard formula for Variance to expand the above equation as shown. . . Now we use the fact that $w_l$ and $x_l$ are independent of each other. This means . $Cov(w^2,x^2)=0$ | $Cov(w,x)=0$ | . Also, the weights ($w_l$) have zero mean, as we are sampling the weights from a random normal distribution. I also used the fact that $Var[w_l]= mathbb{E}[w_l^2]- mathbb{E}[w_l]^2$ where the second term is zero. . . $Var[x_l] = mathbb{E}[x_l^2] - mathbb{E}[x_l]^2$. As $x_l$ is output of an activation function (like ReLU), it has non-zero mean, thus $ mathbb{E}[x_l] neq 0$. . Derivation for ReLU activation function . $$ ReLU(x) = begin{cases} x text{ if }x geq 0 0 text{ if }x&lt;0 end{cases} = max(x,0) $$$w_l$ is initialized as a normal distribution. So half the probability lies on left of origin and half on right side of origin. . a = torch.randn(1000,1000) plt.hist(a.flatten().numpy(), bins=100); . Our objective is to solve this equation: $$Var[y_l] = n_lVar[w_l] mathbb{E}[x_l^2]$$ . For this we have to solve $ mathbb{E}[x_l^2]= mathbb{E}[ max(0,y_{l-1})^2]$. . . Note: $x_l$ is the output of activation function applied to a linear layer. . We first show that $y_{l-1}$ is symmetric around 0. Formula for $y_{l-1}$ is $$y_{l-1}=w_{l-1}x_{l-1}+b_{l-1}$$ Taking the expectation of above gives: . . This shows that $y_{l-1}$ is centered around 0. Also, it is symmetric around 0, as $w_{l-1}$ is symmetric around 0, this combined with the fact that $y_{l-1}$ is centered around 0 gives the conclusion that $y_{l-1}$ is symmetric around 0 (i.e. half probability lies on left of origin and half on right of origin). . The above fact can also be derived mathematically as follows (I drop the $l-1$ subscript for cleaner equations): . . Now we are ready to compute $ mathbb{E}[x_l^2]$: . . Using this result in the equation of variance we have: . . With $L$ layers put together, we have . . This product is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect the product to take a proper scalar (e.g., 1). A sufficient condition is: . $$ frac{1}{2}n_lVar[w_l] = 1, text{ } forall l$$ . This leads to zero-mean gaussian whose standard deviation is $ sqrt{2/n_l}$ and the biases are initialized to 0. . And this is it. To initialize the weight matrix we can use the following code: . # so n_l in this case is 784 (i.e. preserve standard deviation # along the forward pass). weights = torch.randn(784,10)*math.sqrt(2/784) . General Recipe for any activation function . Find . $$ mathbb{E}[x_l^2]= mathbb{E}[activation _function(y_l)], where y_l=w_{l-1}x_{l-1}+b_{l-1}$$ . | in terms of $Var[y_{l-1}]$. Use the fact that $y_{l-1}$ is symmetric around 0. . Use the above value to find variance of layer l $$Var[y_l] = n_lVar[w_l] mathbb{E}[x_l^2]$$ | Compute the variance at the last layer $$Var[y_L] = Var[y_1]*Var[y_2] cdots*Var[y_{L-1}]$$. | Find a way to make the variance at the last layer constant. | Derivation for LeakyReLU activation function . $$ LeakyReLU(x) = begin{cases} x text{ if }x geq 0 ax text{ if }x&lt;0 end{cases} = max(ax,x), a&gt;0 $$ Step 1: Find $ mathbb{E}[x_l^2]$ in terms of $Var[y_{l-1}]$. . . Step 2: Compute variance of each layer. $$ Var[y_l] = n_lVar[w_l] left( frac{1+a^2}{2}Var[y_{l-1}] right) $$ . Step 3: Compute variance at last layer. $$ Var[y_L] = Var[y_1] left( prod limits_{l=2}^{L} frac{1+a^2}{2}n_lVar[w_l] right) $$ . Step 4: Make the variance constant (=1 in this case would be enough). . $$ frac{1+a^2}{2}n_lVar[w_l] = 1 Var[w_l] = frac{2}{1+a^2}$$ . This leads to a zero-mean gaussian whose standard deviation is $ sqrt{2/(1+a^2)}$ and the biases are initialized to 0. . Conclusion . Weight initialization plays an important role in the training of a model. It can speed up the training process. Kaiming init is a good default initialization technique for most of the tasks. .",
            "url": "https://kushajveersingh.github.io/blog/deep-learning-model-initialization-in-detail",
            "relUrl": "/deep-learning-model-initialization-in-detail",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "How to setup personal blog using Ghost and Github hosting",
            "content": ". Important: I have moved to fastpages. fastpages is the best option is you want to write jupyter notebook and share them as posts. All the details of setup are provided on the fastpages homepage. . The website looks as shown below: . . My system info . Ubuntu 20.04 LTS | Ghost 3.15.3 | Yarn 1.22.4 | Nodejs 12.16.3 | . Short summary of what we are going to do. . Install Ghost locally from source | Use default casper theme to make the website | Generate a static site using gssg | Host the static site on Github | Install Ghost locally and it&#39;s dependencies . Install NodeJS. (v12 is the recommended for Ghost). curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash sudo apt install -y nodejs . | Install Yarn. curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo &quot;deb https://dl.yarnpkg.com/debian/ stable main&quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update &amp;&amp; sudo apt install yarn . | Install Ghost from source. (The official setup guide can be found here Note: Replace KushajveerSingh with your Github username. git clone --recurse-submodules git@github.com:TryGhost/Ghost cd Ghost cd core/client cd ../../ . | This is a hack. If you follow the official setup guide then you would have forked the Ghost repo and added that as your upstream in the previous step. . I skipped that step, as I was having some problems with that. Instead I deleted the .git folder and initialized a new github repo for version control. . So go the main Ghost folder and delete .git folder. Then go to core/client and delete the .git folder and submodule file and you are done. . | Install dependencies (We are in home directory of Ghost) sudo npm install sudo npm install -g knex-migrator knex-migrator i . | Make your Ghost folder a github repo. Goto Github and create a new repo where you want to store the Ghost folder. | In the Ghost folder run these commands to push it to github. | | Create website using Ghost . Use npm start to start the Ghost server. This will open the server at http://localhost:2368. . Goto http://localhost:2368/ghost from where you can start creating your website. . Now create your website locally, and when you are done move to the next step. . Download ghost-static-site-generator . This is the tool that we will use to get a static site out of our Ghost site. You can check the official github repo of the package for more details on the usage. . To download the package run npm install -g ghost-static-site-generator. If you get errors run this command again. I ran this command twice and it worked. Maybe try sudo if it still not works. . Now you can create your static site using gssg --url=https://kushajveersingh.github.io/ and it will create a static folder in your current directory, from where you can copy the contents to your .github.io repo. . Automating the above process . To automate the complete process and ensure that my Ghost repo and .github.io repo are in sync, I created this script. . # Change git_folder, ghost_folder, url # git_folder -&gt; location of your .github.io repo # ghost_folder -&gt; location of the Ghost folder in which you are creating your site # url -&gt; The github address where your website will be published git_folder=&quot;/home/kushaj/Desktop/Github/KushajveerSingh.github.io&quot; ghost_folder=&quot;/home/kushaj/Desktop/Github/Ghost&quot; url=&quot;https://kushajveersingh.github.io/&quot; # Remove all contents of git_folder rm -r $git_folder/* # Generate static site using ghost-static-site-generator # The contents of the site are directly placed in the git_folder gssg --url $url --dest $git_folder # Commit the changes of git_folder cd $git_folder git add -A git commit -m &quot;$1&quot; git push origin master # Commit the changes of ghost_folder cd $ghost_folder git add -A git commit -m &quot;$1&quot; git push origin master . You need to change only git_folder, ghost_folder and url as per your requirements. . Usage . ./generate_script.sh &quot;initial commit&quot; . Your repositories will be pushed to Github with the provided commit message. .",
            "url": "https://kushajveersingh.github.io/blog/how-to-setup-personal-blog-using-ghost-and-github-hosting",
            "relUrl": "/how-to-setup-personal-blog-using-ghost-and-github-hosting",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Study of Mish activation function in transfer learning with code and discussion",
            "content": "Link to jupyter notebook, paper, fastai discussion thread . Mish activation function is proposed in Mish: A Self Regularized Non-Monotonic Neural Activation Function paper. The experiments conducted in the paper shows it achieves better accuracy than ReLU. Also, many experiments have been conducted by the fastai community and they were also able to achieve better results than ReLU. . Mish is defined as x * tanh(softplus(x)) or by this equation $x* tanh ( ln (1+e^x))$. . PyTorch implementation . class Mish(nn.Module): r&quot;&quot;&quot; Mish activation function is proposed in &quot;Mish: A Self Regularized Non-Monotonic Neural Activation Function&quot; paper, https://arxiv.org/abs/1908.08681. &quot;&quot;&quot; def __init__(self): super().__init__() def forward(self, x): return x * torch.tanh(F.softplus(x)) . . Plot mish function . To build upon this activation function let’s first see the plot of the function. . x = np.linspace(-7, 7, 700) y = x * np.tanh(np.log(1 + np.exp(x))) fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.spines[&#39;left&#39;].set_position(&#39;center&#39;) ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) plt.plot(x, y, &#39;b&#39;) plt.savefig(fname=&#39;/home/kushaj/Desktop/Temp/SOTA/images/mish_plot.png&#39;, dpi=1200) . . . Properties of mish . Unbounded Above:- Being unbounded above is a desired property of an activation function as it avoids saturation which causes training to slow down to near-zero gradients. | Bounded Below:- Being bounded below is desired because it results in strong regularization effects. | Non-monotonic:- This is the important factor in mish. We preserve small negative gradients and this allows the network to learn better and it also improves the gradient flow in the negative region as, unlike ReLU where all negative gradients become zero. | Continuity:- Mish’s first derivative is continuous over the entire domain which helps in effective optimization and generalization. Unlike ReLU which is discontinuous at zero. | To compute the first derivative expand the tanh(softplus(x)) term and you will get the following term and then do product rule of the derivative. . $$y=x* frac{e^{2x}+2e^x}{e^{2x}+2e^x+2}$$ . When using Mish against ReLU use a lower learning rate in the case of Mish. Range of around 1e-5 to 1e-1 showed good results. . Testing Mish against ReLU . Rather than training from scratch which is already done in the paper, I would test for transfer learning. When we use pretrained models for our own dataset we keep the CNN filter weights the same (we update them during finetuning) but we initialize the last fully-connected layers randomly (head of the model). So I would test for using ReLU and Mish in these fully-connected layers. . . Note: I would be using OneCycle training. In case you are unfamiliar with the training technique that I would use here, I have written a complete notebook summarizing them in fastai. You can check the notebook here. . I use CIFAR10 and CIFAR100 dataset to test a pretrained Resnet50 model. I would run the model for 10 epochs and then compare the results at the fifth and tenth epoch. Also, the results would be averaged across 3 runs using different learning rates (1e-2, 5e-3, 1e-3). The weighs of the CNN filters would not be updated, only the fully connected layers would be updated/trained. . For the fully connected layers, I would use the following architecture. In case of Mish, replace the ReLU with Mish. . # AdaptiveConcatPool2d is just combining AdaptiveAvgPool and AdaptiveMaxPool. head = nn.Sequential( AdaptiveConcatPool2d(), Flatten(), nn.BatchNorm1d(4096), nn.Dropout(p=0.25), nn.Linear(in_features=4096, out_features=512), nn.ReLU(inplace=True), nn.BatchNorm1d(512), nn.Dropout(p=0.5), nn.Linear(in_features=512, out_features=10) ) . . The final results are shown below. It was observed that Mish required training with a smaller learning rate otherwise it overfits quickly, thus suggesting that it requires stronger regularization than ReLU. It was consistent across multiple runs. Generally, you can get away with using a higher learning rate in the case of ReLU but when using Mish a higher learning rate always lead to overfitting. . Although the results are quite similar but by using Mish we can see some marginal improvements. This is a very limited test as only one Mish activation is used in the entire network and also the network has been run for only 10 epochs. . Visualization of output landscape . We would use a 5 layer randomly initialized fully connected neural network to visualize the output landscape of ReLU and Mish. The code and the results are given below. . from sklearn.preprocessing import MinMaxScaler from PIL import Image # The following code has been taken from # https://github.com/digantamisra98/Mish/blob/master/output_landscape.py def get_model(act_fn=&#39;relu&#39;): if act_fn is &#39;relu&#39;: fn = nn.ReLU(inplace=True) if act_fn is &#39;mish&#39;: fn = Mish() model = nn.Sequential( nn.Linear(2, 64), fn, nn.Linear(64, 32), fn, nn.Linear(32, 16), fn, nn.Linear(16, 1), fn ) return model # Main code relu_model = get_model(&#39;relu&#39;) mish_model = get_model(&#39;mish&#39;) x = np.linspace(0., 10., 100) y = np.linspace(0., 10., 100) grid = [torch.tensor([xi, yi]) for xi in x for yi in y] np_img_relu = np.array([relu_model(point).detach().numpy() for point in grid]).reshape(100, 100) np_img_mish = np.array([mish_model(point).detach().numpy() for point in grid]).reshape(100, 100) scaler = MinMaxScaler(feature_range=(0, 255)) np_img_relu = scaler.fit_transform(np_img_relu) np_img_mish = scaler.fit_transform(np_img_mish) plt.imsave(&#39;relu_land.png&#39;, np_img_relu) plt.imsave(&#39;mish_land.png&#39;, np_img_mish) . . From the above output landscapes, we can observe that the mish produces a smoother output landscape thus resulting is smoother loss functions which are easier to optimize and thus the network generalizes better. .",
            "url": "https://kushajveersingh.github.io/blog/study-of-mish-activation-function-in-transfer-learning",
            "relUrl": "/study-of-mish-activation-function-in-transfer-learning",
            "date": " • Nov 11, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Reproducing Cyclic Learning papers + SuperConvergence using fastai",
            "content": "The following papers by Leslie N. Smith are covered in this notebook :- . A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay. paper | Super-Convergence: Very Fast Training of Neural Networks Using Learning Rates. paper | Exploring loss function topology with cyclical learning rates. paper | Cyclical Learning Rates for Training Neural Networks. paper | Although, the main aim is to reproduce the papers but a lot of research has been done since than and thus where needed I would change some things to match the state of the art practices. Most of these things are taught in the fastai courses, namely Practical Deep Learning for Coders, v3. . If you are not familiar with fastai, it is a deep learning library build on top of PyTorch and it contains the implementations of most of the state of the art practices, which keep changing over time. As a result of this you can get state of the art results in most of the tasks by using the defaults of the library. . How this notebook is structured. I would explain all the concepts discussed in the paper and would provide a walkthrough with a CIFAR-100 example along the way. So there would be explanation of the topic and then the code for that. If you are to use these techniques for your own work, you can follow along the notebook from top-to-bottom. For the implementations of some concepts, I would use the fastai built in functions as fastai provides a callback system that really helps a lot when working on projects in real life. So if you do not know fastai, you can watch the course mentioned above or read the docs, as the docs contains ample examples. For Tensorflow users, as of now I am not aware if some library provides this much functionality as fastai but you can still follow on as the concepts discussed are general, only the implementation is different. . If you are learning fastai, this notebook can be a very good tutorial on how to use the vision API in fastai. . # and much more, so no need to import them again in our work. from fastai import * from fastai.vision import * . Although deep learning has produced dazzling successes for applications of image, speech and video processing in the past few years, most trainings are done with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyperparameters reamains a black art that requires years of experience to acquire. So I present several efficient ways to set the hyper-parameters that significantly reduce the training time and improves performance. Specifically, we examine the training and validation/test loss for subtle clues of underfitting and overfitting and suggest guidelines for moving toward the optimal balance point. . . Note: Should I use hyper-parameter tuning library? I asked this question on the fastai forums and Jeremy replied, &quot;I would suggest avoiding black-box hyperparam tuning wherever possible. I&#8217;ve only used it once in my life, and even then it wasen&#8217;t really a win. Instead, use the techniqeus we&#8217;ve learned to try carefully selected choices.&quot; . And I agree with that. Most of the times there is not much need to use these libraries and as we would soon find that most of the hyper-parameters are linked with each other, so we should tune them accordingly. An argument can be made for Bayesian Optimizations, but I have not used them and I find the techniques discussed here much simpler and safer. . Summary of hyper-parameters . Learning rate :- Use learning rate finder test and get the maximum value of learning rate that you would use in the 1cycle policy. | Batch size :- The largest value that fits on your GPU. You can use batch sizes like 20 that are not powers of two. The performance drop that is discussed when using batch size that are not powers of 2 is true, but we can ignore it if we want. | Momentum :- Use cyclic momentum in most of the tasks. When you are using GANs use a constant value of momentum. | Weight Decay :- Larger value when using smaller dataset and model. Smaller value when using bigger datasets and models. Use a constant value. | . Hyper-params not discussed . Architecture | Regularization | Dataset | Task | . Things to remember . Setting hyperparameters is very important. Every dataset would have their own set of hyperparameter values, and setting the right hyper-parameter values should be your only priority I would say initially. | Regularization vs weight decay. In regularization we subtract something from the loss function, while in weight decay we subtract something from the parameter update step. | When we use modern architectures like Resnet it is better to use weight decay than L2 regularization. | You can set all your hyper-parameters in a few epochs. | Underfitting vs Overfitting . The basis of this notebook is based on the concept of underfitting vsersus overfitting. Specifically, it consists of examining the training&#39;s test/validation loss for clues of underfitting and overfitting in order to strive for the optimal set of hyper-parameters. By observing and understanding the clues available early during training, we can tune our architecture and hyper-parameters with short runs of a few epochs. In particular, by monitoring validation/test loss early in the training, enough information is available to tune the architecture and hyper-parameters and this eliminates the necessity of running complete grid or random searches. . One key finding in the paper is that the total regularization needs to be in balance for a given dataset and architecture. And it was found that learning rate, momentum and regularization are tightly coupled and optimal values must be determined together. This means that if you set a learning rate as a large value than other regularizations like momentum must come down, so that the total remains preserved. . . . In Figure 2(a) early overfitting is observed but test loss still decreases a little after that. This can be misleading in some cases where one can get blindsided by reduction in the amount of test loss. . . Important: Draw your losses and see whether you overfit or not. You should not use a model that overfitted early as after that point, the model is essentially just learning some examples to reduce test loss, which results in poor generalization. . Deep Dive into Underfitting and Overfitting . Underfitting . Underfitting is when the machine learning model is unable to reduce the error for either the test or training set, which is due to the under capacity of the machine learning model i.e. it is now pwerful enough to fit the underlying complexities of the data distributions. Whenever your valid loss is less than the training loss than it means your model is underfitting. . . Overfitting . Overfitting is when your model is too powerful, such that the model starts fitting the training set too well. Using small learning rates can exhibit some overfitting behaviour (as shown in Fig2(a)) . . Choosing Learning Rate . If the learning rate is too low overfitting can occur. Large learning rates help to regularize the training but if the learning rate is too large, the training will diverge. Hence a grid search of short runs to find learning rates that converge or diverge is possible but there is an easier way. . By training with high learning rates we can reach a model that gets 93% accuracy in 70 epochs which is less than 7k iterations (as opposed to the 6rk iterations which made roughly 360 epochs in the original paper of Resnet). . Cyclical Learning Rates(CLR) and learning rate test . Figure 5, shows how fast state-of-the-art results can be achieved using CLR. . . Sylvain Gugger has written two very good blog posts explaining this topic, so I would recommend you to read those first. . How Do You Find A Good Learning Rate link | The 1cycle policy link | . Here I just summarize the topic with important details, for a detailed overview refer to the above two links. . # https://course.fast.ai/datasets. # &#39;Path&#39; is a python package which makes working with directory # names a lot easier, to import it use `from pathlib import Path` path = Path(&#39;/home/kushaj/Desktop/Data/cifar100/&#39;) path.ls() . [PosixPath(&#39;/home/kushaj/Desktop/Data/cifar100/test&#39;), PosixPath(&#39;/home/kushaj/Desktop/Data/cifar100/models&#39;), PosixPath(&#39;/home/kushaj/Desktop/Data/cifar100/train&#39;)] . # where every image is placed in a folder with the same name as the # class of the image. . src = (ImageList.from_folder(path) .split_by_folder(valid=&#39;test&#39;) .label_from_folder()) . data = (src.transform(get_transforms(), size=(32,32)) .databunch(bs=256, val_bs=512, num_workers=8) .normalize(cifar_stats)) cifar_stats . ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]) . data.show_batch(rows=3, figsize=(4,4)) . After creating the databunch we have done the following things . Added data augmentation with the following transforms and the size of images is taken as (32,32) | Normalied the images with the CIFAR_STATS | I am using a batch size of 256 for both trian and valid sets | . Resnet-56 . Original ResNet model is not used due to the two downsampling layers in the starting, namely conv and max_pool. If these layers are used then the image size is reduced to 8x8 after two layers. . Resnet-56 is also used in the orginal papers and in general is a good model for small images. . class AdaptiveConcatPool2d(nn.Module): &quot;Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.&quot; def __init__(self, sz=None): &quot;Output will be 2*sz or 2 if sz is None&quot; super().__init__() self.output_size = sz or 1 self.ap = nn.AdaptiveAvgPool2d(self.output_size) self.mp = nn.AdaptiveMaxPool2d(self.output_size) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) class Flatten(nn.Module): &quot;Flatten `x` to a single dimension, often used at the end of a model. `full` for rank-1 tensor&quot; def __init__(self, full:bool=False): super().__init__() self.full = full def forward(self, x): return x.view(-1) if self.full else x.view(x.size(0), -1) class BasicBlock(nn.Module): def __init__(self, c_in, c_out, stride=1): super().__init__() self.conv1 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(c_out) self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(c_out) if stride != 1 or c_in != c_out: self.shortcut = nn.Sequential( nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(c_out) ) def forward(self, x): shortcut = self.shortcut(x) if hasattr(self, &#39;shortcut&#39;) else x out = F.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) out += shortcut return F.relu(out) class Resnet(nn.Module): def __init__(self, num_blocks=[9,9,9], num_classes=100): super().__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(16) self.layer1 = self.make_group(16, 16, num_blocks[0], stride=1) self.layer2 = self.make_group(16, 32, num_blocks[1], stride=2) self.layer3 = self.make_group(32, 64, num_blocks[2], stride=2) self.head = nn.Sequential( AdaptiveConcatPool2d(), Flatten(), nn.BatchNorm1d(128), nn.Dropout(0.25), nn.Linear(128, 128, bias=True), nn.ReLU(inplace=True), nn.BatchNorm1d(128), nn.Dropout(0.5), nn.Linear(128, num_classes, bias=True) ) def make_group(self, c_in, c_out, num_blocks, stride): layers = [BasicBlock(c_in, c_out, stride)] for i in range(num_blocks-1): layers.append(BasicBlock(c_out, c_out, stride=1)) return nn.Sequential(*layers) def forward(self, x): out = F.relu(self.bn1(self.conv1(x))) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.head(out) return out . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=[ShowGraph]) . Till now we have done the following: . Fot out data, which is stored in data as a databunch | A Resnet model is created | A learner object is created which is names learn | AdamW is used as the optimization function | CrossEntropyLoss is used as loss function | . Cyclic Learning Rate . The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial. So we vary learning rate from a small value to large value than back to small value. This is termed as one complete cycle. . . This learning rate policy is taken from the original paper, but that was few years back. Now we use a cosine policy that lookes something like this. . learn.fit_one_cycle(1, max_lr=1e-2) learn.recorder.plot_lr() . If you have any question on why is the learning rate policy defined as this, or in general to the learning rate policy in fastai, I have answered all these questions in the forums which you can check here. There I clearly explain why we use a learning rate policy that looks like the one shown above and what are the reasons behind choosing the deafults. (The main reason is we want to train our model at higher learning rates and then fine-tune them at lower learning rates). . For the implementation of this, you can check KushajveerSingh/fastai_without_fastai where I implement the one-cycle policy in pure pytorch. . Difference from original paper . There are some changes from the original paper. First, in the papers for most of the cases accuracy is used as the metric, while I use loss values. The reason loss values are used to compare hyper-parameter value is because loss is the actual thing that is being optimized, we want to reduce it the most. . One-cycle policy summary . There are two phases, first learning rate increases from small lr to maximum value and in the second phase it decreased from the maximum value to the minimum value (the minimum value is smaller than the starting value in the first phase). In implementation, you only need to define the maximum value of the learning rate and the minimum values would be calculated appropriately. . Learning rate finder test . In order to find the maximum value of leanring rate (max_lr) we using a learning rate test. In this test we start with a small value of learning rate (1e-7) and then increase this value to a maximum of 10 as shown in the figure below. . learn.lr_find() learn.recorder.plot_lr() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . In fastai this test can be done using learn.lr_find. After running this test we plot a diagram of the loss values for the different values of learning rate and try to find the maximum value of learning rate that we can use. . learn.lr_find(start_lr=1e-7, end_lr=10, num_it=100) . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . Now you need to look at the above figure and select the lr value that you want to use. I would use lr=1e-2. The art of selecting good values comes down to practice, just try the values you are confused about and select the one that gives the best results. There are certain guidelines that you should follow while finding the lr value . Select the one where the loss is small for larger amount of time | When loss begins to increase, select a value that is 10 times less | . For experimentation, let us see what the resutls are when I use max_lr=1e-2 and max_lr=1e-1 . # So you don&#39;t have to code up anything learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=[ShowGraph]) learn.fit_one_cycle(3, max_lr=1e-2) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.117561 | 4.697643 | 0.047400 | 0.175600 | 01:48 | . 1 | 3.663519 | 3.413377 | 0.173200 | 0.446900 | 01:47 | . 2 | 3.335882 | 3.095275 | 0.239100 | 0.529900 | 01:48 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=[ShowGraph]) learn.fit_one_cycle(3, max_lr=1e-1) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.334694 | 4.820587 | 0.036000 | 0.156500 | 01:51 | . 1 | 3.994016 | 3.712073 | 0.109600 | 0.351700 | 01:52 | . 2 | 3.565970 | 3.270659 | 0.197700 | 0.490200 | 01:46 | . As you can see clearly max_lr=1e-2 is better than max_lr=1e-1. Athough this test took around 11 minutes but we easily got the value of lr that we should use. With enough practice you can predict the best value of max_lr from directly the lr_find graph but whenever you are confused running some epochs can help. . Introducing Superconvergence . Now we know what is 1cycle policy and how we can use it for our own work in fastai, next I want to discuss Superconvergence which is basically what we are getting using the 1cycle policy. . . There are two noteworthy things to see in this figure. First is the dip in the accuracy around LR=0.1. Second is the consistently high test accuracy over a large span of learning rates (LR=0.25 to 1.0), which is unusual. Seeing these unsual facts, experiments were carried out using cyclic learning rate and the following counterintuitive things results appeared. . . You can see an anomly that occurs as the LR increases from 0.1 to 0.35. The training loss increases sharply by four orders of magnitude at a learning rate of approximately 0.255 but training convergence resumes at larger learning rates. In addition, there are divergenct behaviours between test accuracy and loss curves that are not easily explained.In the first cycle, when the learning rate is increasing from 0.13 to 0.18 the test loss increases but the test accuracy also increases. This simultaneous increase in the test loss and the test accuracy also occurs in the second cycle as the learning rate decrease from 0.35 to 0.1 and in various portions of subsequent cycles. . Another interesting fact can be see from the second figure. The cyclic learning rate method is able to get 0.93 accuracy after just once cycle and it remains the same for the subsequent cycles, while the standard approach of using a constant learning rate manages to achieve the accuracy close to 0.93 at about 5 times more iterations. . Superconvergence . Super-convergence refers to this phenomenon where a network is trained to a better final test accuracy compared to tradional training, but with fewer iterations and a much larger learning rate. . Testing linear interpolation results . In the Exploring Loss Function Topology with Cyclical Learning Rates (only 3 pages) an important topic of networkinterpolation is discussed which shows that the solution found by each cycle is different from each other (intuitively you can think the solutions as belonging to different valleys). So we can interpolate solutions from different cycles and we can except better accuracy. So I test this fact now? . Interpolation between two values is defined as $w_{new}= alpha * w_1+(1- alpha)*w_2$. . How it was found in the first place? . Before getting into code, I would discuss how this fact was found. So interpolations between different cycle weights was tested and the plots for interpolation were drawn for the train and test loss. One of the figure is shown below . . From this plot it was observed that the solution found by each cycle indeed belonged to different minima. Also an additional noteworthy feature, some amount of regularization is possible through interpolating between two solutions. The minima for training loss are at a=0.0 and 1.0 but the test loss minima are slightly offset towards the center. . Coding Linear Interpolation . To check the results for the above topic the following things would be done: . Do some training epochs on the training dataset and get the weights for diferent cycles | Make a plot of interpolation between different weight values | To modify our training loop callbacks would be used. I consifer callbacks to tbe the most important feature of fastai as it allows infinite customization without chaning the training loop. . class GetWeights(Callback): def __init__(self, learn, save_list=[5, 7]): self.learn = learn self.save_list = save_list def on_train_begin(self, **kwargs): self.weights = {} def on_epoch_end(self, **kwargs): if kwargs[&#39;epoch&#39;] in self.save_list: weights = learn.model.state_dict() for k, v in weights.items(): weights[k] = v.cpu() self.weights[kwargs[&#39;epoch&#39;]] = weights def get_weights(self): return self.weights learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=ShowGraph) getWeights = GetWeights(learn) . learn.fit_one_cycle(8, max_lr=1e-2, callbacks=getWeights) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.191520 | 4.533091 | 0.053900 | 0.195400 | 01:54 | . 1 | 3.791083 | 4.001433 | 0.102000 | 0.318300 | 01:53 | . 2 | 3.429356 | 3.574863 | 0.161400 | 0.429000 | 01:50 | . 3 | 3.125475 | 3.035374 | 0.231600 | 0.533600 | 01:53 | . 4 | 2.832539 | 2.619795 | 0.312300 | 0.643800 | 01:53 | . 5 | 2.584690 | 2.346036 | 0.374700 | 0.704900 | 01:53 | . 6 | 2.407087 | 2.109318 | 0.422400 | 0.756300 | 01:53 | . 7 | 2.304121 | 2.082097 | 0.429800 | 0.761200 | 01:48 | . weights = getWeights.get_weights() w1 = weights[5] w2 = weights[7] . learn.model.load_state_dict(w1) learn.validate() . [2.3460364, tensor(0.3747), tensor(0.7049)] . def interpolate(alpha): w_new = {} keys = list(w1.keys()) for key in keys: w_new[key] = alpha*w1[key] + (1 - alpha)*w2[key] return w_new . alpha_range = np.linspace(start=-0.5, stop=1.5, num=100) train_loss = [] val_loss = [] for i, alpha in enumerate(alpha_range): print(f&#39;{i}/{len(alpha_range)} started&#39;) w_new = interpolate(alpha) learn.model.load_state_dict(w_new) loss1, _, _ = learn.validate() loss2, _, _ = learn.validate(data.train_dl) val_loss.append(loss1) train_loss.append(loss2) . plt.figure(figsize=(10,6)) plt.plot(alpha_range, train_loss, &#39;b&#39;) plt.plot(alpha_range, val_loss, &#39;r&#39;) plt.show() . alpha_range = np.linspace(start=-10, stop=4, num=50) train_loss2 = [] val_loss2 = [] for i, alpha in enumerate(alpha_range): print(f&#39;{i}/{len(alpha_range)} started&#39;) w_new = interpolate(alpha) learn.model.load_state_dict(w_new) loss1, _, _ = learn.validate() loss2, _, _ = learn.validate(data.train_dl) val_loss2.append(loss1) train_loss2.append(loss2) . plt.figure(figsize=(10,6)) plt.plot(alpha_range, train_loss2, &#39;b&#39;) plt.plot(alpha_range, val_loss2, &#39;r&#39;) plt.show() . Losses went nan for the initial values. Seeing this we cannot combine both of them, but the reason being in the paper SGD was used while here we are using AdamW with cyclic momentum by default so it changes up the situation by a lot. But it is a fun experiment. . Explanation behind Super-Convergence . As we discussed earlier, one of the indicators of super-convergence are the consistent high value of accuracies with increasing learning rate. Cyclic Learning which allowed super-convergence is indeed a combination of Curriculm learning and simulated annealing. Also, as the amount of data decreases the gap in performance between the result of standard training and super-convergence increases. Specifically, with a peicewise constant learning rate schedule the training encounters difficulties and diverges along the way. . . The above figure gives an intuitive understanding of how super-convergence happens. The blue line in the figure represents the trajectory of the training while converging and the x&#39;s indicate the location of the solution at each iteration and indicates the progress made during the training. . The while loss surface can be divided into 3 phases:- . In early training, the learning rate must be small in order for the training to make progress in appropriate direction. As you can see in the figure a significant progress is made in those early iterations (the part where we descend the valley) | Now as the slope decreases so does the amount of progress made per iteration and little improvement occurs over the bulk of the iteration. This is the reason why we increase the learning rate to high values, so that we can quickly move over this region. | As we approach the bottom of the loss surface (you can think of it as bottom of valley with bumps), so here we need to slow down and get to the bottom of these bumps i.e. why we decrease the value of learning rate to minimum value so that we can fine-tune our final result. | A quick summary . Initially when the learning starts there is huge slope and we move down it quickly, now there is a straight out path and we make very less progress through each iterations. At the end we enter a valley and we have to move towards the minima. Cyclic Learning Rate solves it. We start with small learning rate to get over that initial big slope. Then we increase the learning rate to quickly move through the straight path and then we again decrease learning rate to move through the valley. . Choosing momentum value . Choosing the value of momentum depends on the task at hand. So therea are two options either use cyclic momentum or constant value of momentum. I would directly give you the results here . IF you are training GAN&#39;s or any task where you are quickly shifting between different models (like in GANs we shift between generator and discriminator) you should use constant value of momentum. Because you will not have enough time to get the benefits of cyclic momentum in these tasks. | Cyclic momentum should be your default choice for other tasks. And for the momentum values they should be high-&gt;low i.e. opposite of learning rate. | . In implementation we would specify the minimum and maximum value of momentum and move from max-&gt;min and then min-&gt;max. Below I show the cyclic learning rate and cyclic momentum values side by side. . learn = Learner(data, Resnet()) learn.fit_one_cycle(1) learn.recorder.plot_lr(show_moms=True) . epoch train_loss valid_loss time . 0 | 4.013027 | 3.831154 | 01:47 | . The reason behind using this approach is we have to keep the total regularization in check. So if I use higher learning rates which provide regularization on their own, I do not need high values of momentum (as high momentum value would also provide large regularization values) and this would ensure that this results in convergence for a longer range of learning rate and faster convergence. The optimal learning rate is dependent on the momentum and momentum is dependent on the learning rate. . Some good values of momentum to test . This is a difficult question to answer. Genrally the default of (0.95 - 0.85) works good, but there are some values that you can test . 0.99 - 0.90 | 0.95 - 0.85 | 0.99 | 0.9 | 0.85 | . Note:- In practice, we would choose this value in combination with the value of weight decay. But for quick demonstration I show how to choose value of momentum only. . I test for the first two cases. . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=ShowGraph) learn.fit_one_cycle(3, max_lr=1e-2, moms=(0.99, 0.90)) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.074203 | 4.473627 | 0.058200 | 0.209500 | 01:45 | . 1 | 3.603846 | 3.422370 | 0.168400 | 0.436200 | 01:46 | . 2 | 3.306487 | 3.084147 | 0.243600 | 0.533900 | 01:47 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=ShowGraph) learn.fit_one_cycle(3, max_lr=1e-2, moms=(0.95, 0.85)) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.091744 | 4.376647 | 0.064800 | 0.228900 | 01:47 | . 1 | 3.645397 | 3.546236 | 0.142300 | 0.401900 | 01:47 | . 2 | 3.348282 | 3.130295 | 0.232100 | 0.524700 | 01:46 | . The testing took around 11 minutes and I think the value of 0.99-0.90 for momentum is better than 0.95-0.85. This is it. Using this method you can get creative and test out many of your hyperparameter choices without using any external libraries which in most cases do not even work. . One of the things that you may be wondering what to do if I have large datasets like Imagenet dataset where every epoch takes hours to run. In that situation I would suggest to take a small smaple of the dataset and adjust your hyper-parameters using those. Generally you don&#39;t need to have very large validation set, a subset of the validation set can also work provided that subset is a good representation of the actual validation set. You can become creative when using large datasets where for the training dataset you treat a specific number of batches as a single epoch. For example, if my dataset can be divided into 100 batches and I decide every 20 batches would be treated as a single epoch. So after 100 batches I would have done 5 epochs instead of 1. . Note:- I did not even need 3 epochs to decide which value is better. If we just compare the resutls from the first two epochs we can clearly make our choice without having to do third epoch. . Choosing Weight Decay . Weight decay is not like momentum and learning rate and the best value should remain constant throughout the training. Since the networks performance is dependent on a proper weight decay value, a grid search is worthwhile and differences are visible early in the training. That is the validation loss early is sufficient for determining a good value. . So to set the value of weight decay you should run combined runs using different values of weight decay and momentum and possibly learning rate. Generally what I found good is using the lr_range test to get the value of learning rate and then adjust momentum and weight decay accordingly using various combinations. . The reason we can avoid finding the values of learning rate, momentum and weight decay simultaneously is all of these hyper-parameters are coupled and if we set a low value for some param we can set higher values of other params, so as to keep the total regularization in check. . How to set the value . This requires a grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy. Use your knowledge of the dataset and architecture to decide which values to use. For example, a more complex dataset requires less regularization so test smaller weight decay values such as 1e-4, 1e-5, 1e-6. A shallow architecture requires more regularization so test larger weight decay values such as 1e-2, 1e-3, 1e-4. The reason being complex datasets provide regularization on their own and other regularizations should be reduced. . So if you guess that 1e-4 should be a good value than test 3e-5, 1e-4, 3e-4. How I chose the value 3? So if you think your weight decay best value lies between 10-4 and 10-3, than you should choose the value 10-3.5 i.e take average of the exponent. You can keep going in this way. . To make testing simpler I would use cyclic momentum as (0.99-0.90) and find the optimal value of weight decay. The reason I do not need to change the momentum value here is that momentum is already changing from 0.99 to 0.9 as we are using cyclic momentum, so in implementation we can first find a good cyclic momentum value and then test out weight decay values. . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=0) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.114937 | 4.064409 | 0.061100 | 0.235800 | 01:56 | . 1 | 3.662782 | 3.437547 | 0.168800 | 0.436900 | 01:46 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=1e-4) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.124258 | 3.914982 | 0.085900 | 0.281900 | 01:48 | . 1 | 3.668026 | 3.459805 | 0.171700 | 0.436300 | 01:47 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=1e-5) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.139386 | 4.032632 | 0.074400 | 0.247900 | 01:49 | . 1 | 3.692349 | 3.504380 | 0.165300 | 0.419900 | 01:49 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=1e-3) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.130610 | 4.117643 | 0.064100 | 0.235600 | 01:49 | . 1 | 3.725400 | 3.531017 | 0.156800 | 0.414100 | 01:48 | . From the above results is clear that WD=1e-4 is the best. So now I test around 1e-4 with the same rule as explained above. So I take the average of -4 and -3 and I get -3.5, so the multiplier I choose is 10-3.5 which is approx 3.12*10-3. The values I test are 3x10-4 and 3x10-5 . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=3e-4) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.128520 | 4.023188 | 0.069500 | 0.250500 | 01:45 | . 1 | 3.689037 | 3.464480 | 0.162000 | 0.426000 | 01:46 | . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy]) learn.fit_one_cycle(2, max_lr=1e-2, moms=(0.99-0.90), wd=3e-5) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.116028 | 3.965793 | 0.077100 | 0.269000 | 01:46 | . 1 | 3.712324 | 3.490986 | 0.166200 | 0.425000 | 01:46 | . The final weight decay value that I choose after seeing the above resuls is 1e-4. In this way you can test different hyperparameter values and see which performs the best. . Train a final classifier model with the above param values . learn = Learner(data, Resnet(), metrics=[accuracy, top_k_accuracy], callback_fns=ShowGraph) . learn.fit_one_cycle(15, max_lr=1e-2, moms=(0.99-0.9), wd=1e-4) . epoch train_loss valid_loss accuracy top_k_accuracy time . 0 | 4.271368 | 4.100077 | 0.058000 | 0.230000 | 01:47 | . 1 | 3.935396 | 4.278841 | 0.058900 | 0.215700 | 01:46 | . 2 | 3.700406 | 3.707660 | 0.128600 | 0.364100 | 01:46 | . 3 | 3.440132 | 3.624694 | 0.159000 | 0.405800 | 01:46 | . 4 | 3.107491 | 3.175907 | 0.230000 | 0.519900 | 01:46 | . 5 | 2.872592 | 2.738831 | 0.286900 | 0.611200 | 01:46 | . 6 | 2.674556 | 2.516749 | 0.336100 | 0.666500 | 01:46 | . 7 | 2.477309 | 2.270287 | 0.394800 | 0.717000 | 01:46 | . 8 | 2.316169 | 2.268882 | 0.390300 | 0.728300 | 01:46 | . 9 | 2.188307 | 1.973801 | 0.458700 | 0.774900 | 01:46 | . 10 | 2.041943 | 1.839805 | 0.488200 | 0.798400 | 01:47 | . 11 | 1.946529 | 1.751944 | 0.512200 | 0.814900 | 01:46 | . 12 | 1.835809 | 1.701965 | 0.524300 | 0.822100 | 01:46 | . 13 | 1.792115 | 1.664993 | 0.531100 | 0.830100 | 01:47 | . 14 | 1.786060 | 1.660618 | 0.531600 | 0.830200 | 01:46 | . Congratulations you made it to the end. You can now set any hyper-parameter value by just visualizing the validation loss for a few epochs and seeing whether our models overfit or not. The content of this notebook has been taken from the four papers by Lesli N. Smith as mentioned in the starting of the notebook and from the fastai courses taught by Jeremy Howard. .",
            "url": "https://kushajveersingh.github.io/blog/reproducing-cyclic-learning-papers-and-superconvergence",
            "relUrl": "/reproducing-cyclic-learning-papers-and-superconvergence",
            "date": " • Jun 26, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "How to become an expert in NLP in 2019",
            "content": "In this post, I would focus on all of the theoretical knowledge you need for the latest trends in NLP. I made this reading list as I learned new concepts. For the resources, I include papers, blogs, videos. . It is not necessary to read most of the stuff. Your main goal should be to understand that in this paper this thing was introduced and do I understand how it works, how it compares it with state of the art. . . Trend: Use bigger transformer based models and solve multi-task learning. . . Warning: Warning: It is an increasing trend in NLP that if you have a new idea in NLP during reading any of the papers, you will have to use massive compute power to get any reasonable results. So you are limited by the open-source models. . fastai:- I had already watched the videos, so I thought I should add it to the top of the list. . Lesson 4 Practical Deep Learning for Coders. It will get you up with how to implement a language model in fastai. | Lesson 12 Deep Learning from the Foundations. Goes further into ULMFit training. | . | LSTM:- Although transformers are mainly used nowadays, in some cases you can still use LSTM and it was the first successful model to get good results. You should use AWD_LSTM now if you want. . Long Short-Term Memory paper. A quick skim of the paper is sufficient. | Understanding LSTM Networks blog. It explains all the details of the LSTM network graphically. | . | AWD_LSTM:- It was proposed to overcome the shortcoming of LSTM by introducing dropout between hidden layers, embedding dropout, weight tying. You should use AWS_LSTM instead of LSTM. . Regularizing and Optimizing LSTM Language Models paper. AWD_LSTM paper | Official code by Salesforce | fastai implementation | . | Pointer Models:- Although not necessary, it is a good read. You can think of it as pre-attention theory. . Pointer Sentinel Mixture Models paper | Official video of above paper. | Improving Neural Language Models with a continuous cache paper | . | . Tip: What is the difference between weight decay and regularization? In weight decay, you directly add something to the update rule while in regularization it is added to the loss function. Why bring this up? Most probably the DL libraries are using weight_decay instead of regularization under the hood. . . Note: In some of the papers, you would see that the authors preferred SGD over Adam, citing that Adam does not give good performance. The reason for that is (maybe) PyTorch/Tensorflow are doing the above mistake. This thing is explained in detail in this post. . Attention:- Remember Attention is not all you need. CS224n video explaining attention. Attention starts from 1:00:55 hours. | Attention is all you need paper. This paper also introduces the Transformer which is nothing but a stack of encoder and decoder blocks. The magic is how these blocks are made and connected. | Read an annotated version of the above paper in PyTorch. | Official video explaining Attention | Google blog for Transformer | If you are interested in video you can check these link1, link2. | Transformer-XL: Attentive Language Models Beyond a Fixed Length Context paper. Better version of Transformer but BERT does not use this. | Google blog for Transformer-XL | Transformer-XL — Combining Transformers and RNNs Into a State-of-the-art Language Model blog | For video check this link. | The Illustrated Transformer blog | Attention and Memory in Deep Learning and NLP blog. | Attention and Augmented Recurrent Neural Networks blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 1 blog. | Building the Mighty Transformer for Sequence Tagging in PyTorch: Part 2 blog. | . | There is a lot of research going on to make better transformers, maybe I will read more papers on this in the future. Some other transformers include the Universal Transformer and Evolved Transformer which used AutoML to come up with Transformer architecture. . Random resources . Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) [blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/_ | Character-Level Language Modeling with Deeper Self-Attention paper. | Using the output embedding to Improve Langauge Models paper. | Quasi-Recurrent Neural Networks paper. A very fast version of LSTM. It uses convolution layers to make LSTM computations parallel. Code can be found in the fastai_library or official_code. | Deep Learning for NLP Best Practices blog by Sebastian Ruder. A collection of best practices to be used when training LSTM models. | Notes on the state of the art techniques for language modeling blog. A quick summary where Jeremy Howard summarizes some of his tricks which he uses in fastai library. | Language Modes and Contextualized Word Embeddings blog. Gives a quick overview of ELMo, BERT, and other models. | The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) blog. | . | Multi-task Learning:- I am really excited about this. In this case, you train a single model for multiple tasks (more than 10 if you want). So your data looks like “translate to english some_text_in_german”. Your model actually learns to use the initial information to choose the task that it should perform. . An overview of Multi-Task Learning in deep neural networks paper. | The Natural Language Decathlon: Multitask Learning as Question Answering paper. | Multi-Task Deep Neural Networks for Natural Language Understanding paper. | OpenAI GPT is an example of this. | . | PyTorch:- Pytorch provide good tutorials giving you good references on how to code up most of the stuff in NLP. . | ELMo:- The first prominent research done where we moved from pretrained word-embeddings to using pretrained-models for getting the word-embeddings. So you use the input sentence to get the embeddings for the tokens present in the sentence. . Deep Contextualized word representations paper, video | . | ULMFit:- Is this better than BERT maybe not, but still in Kaggle competitions and external competitions ULMFiT gets the first place. . Universal Language Model Fine-tuning for Text Classification paper. | Jeremy Howard blog post announcing ULMFiT. | . | OpenAI GPT:- I have not compared BERT with GPT2, but you work on some kind on ensemble if you want. Do not use GPT1 as BERT was made to overcome the limitations of GPT1. . GPT1 paper, blog, code | GPT2 paper, blog, code | Check video by openai on GPT2 | . | BERT:- The most successful language model right now (as of May 2019). . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding paper. | Google blog on BERT | Dissecting BERT Part 1: The Encoder blog | Understanding BERT Part 2: BERT Specifics blog | Dissecting BERT Appendix: The Decoder blog | . | To use all these models in PyTorch/Tensorflow you can use hugginface/transformers which gives complete implementations along with pretrained models for BERT, GPT1, GPT2, TransformerXL. . Congrats you made it to the end. You now have most of the theoretical knowledge needed to practice NLP using the latest models and techniques. . What to do now? You only learned the theory, now practice as much as you can. .",
            "url": "https://kushajveersingh.github.io/blog/how-to-become-an-expert-in-nlp-in-2019",
            "relUrl": "/how-to-become-an-expert-in-nlp-in-2019",
            "date": " • May 15, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "All you need for Photorealistic Style Transfer in PyTorch",
            "content": "Link to jupyter notebook, paper . What is style transfer? . We have two images as input one is content image and the other is style image. . . Our aim is to transfer the style from style image to the content image. This looks something like this. . . Why another paper? . Earlier work on style transfer although successful was not able to maintain the structure of the content image. For instance, see Fig2 and then see the original content image in Fig1. As you can see the curves and structure of the content image are not distorted and the output image has the same structure as content image. . . Gram Matrix . The main idea behind the paper is using Gram Matrix for style transfer. It was shown in these 2 papers that Gram Matrix in feature map of convolutional neural network (CNN) can represent the style of an image and propose the neural style transfer algorithm for image stylization. . Texture Synthesis Using Convolution Neural Networks by Gatys et al. 2015 | Image Style Transfer Using Convolutional Neural Networks by Gatys et al. 2016 | Details about gram matrix can be found on wikipedia. Mathematically, given a vector V gram matrix is computed as $$G=V^TV$$ . High-Resolution Models . It is a recent research paper accepted at CVPR 2019 paper. So generally what happens in CNNs is we first decrease the image size while increasing the number of filters and then increase the size of the image back to the original size. . Now this forces our model to generate output images from a very small resolution and this results in loss of finer details and structure. To counter this fact High-Res model was introduced. . High-resolution network is designed to maintain high-resolution representations through the whole process and continuously receive information from low-resolution networks. So we train our models on the original resolution. . Example of this model would be covered below. You can refer to the original papers for more details on this. I will cover this topic in detail in my next week blog post. . Style transfer details . The general architecture of modern deep learning style transfer algorithms looks something like this. . . There are three things that style transfer model needs . Generating model:- It would generate the output images. In Fig4 this is ‘Hi-Res Generation Network’ | Loss function:- Correct choice of loss functions is very important in case you want to achieve good results. | Loss Network:- You need a CNN model that is pretrained and can extract good features from the images. In our case, it is VGG19 pretrained on ImageNet. | So we load VGG model. The complete code is available at my GitHub repo. . if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: raise Exception(&#39;GPU is not available&#39;) # Load VGG19 features. We do not need the last linear layers, # only CNN layers are needed vgg = vgg19(pretrained=True).features vgg = vgg.to(device) # We don&#39;t want to train VGG for param in vgg.parameters(): param.requires_grad_(False) torch.backends.cudnn.benchmark = True . . Next we load our images from disk. My images are stored as src/imgs/content.png and src/imgs/style.png. . content_img = load_image(os.path.join(args.img_root, args.content_img), size=500) content_img = content_img.to(device) style_img = load_image(os.path.join(args.img_root, args.style_img)) style_img = style_img.to(device) # Show content and style image fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10)) ax1.imshow(im_convert(content_img)) ax2.imshow(im_convert(style_img)) plt.show() # Utility functions def im_convert(img): &quot;&quot;&quot; Convert img from pytorch tensor to numpy array, so we can plot it. It follows the standard method of denormalizing the img and clipping the outputs Input: img :- (batch, channel, height, width) Output: img :- (height, width, channel) &quot;&quot;&quot; img = img.to(&#39;cpu&#39;).clone().detach() img = img.numpy().squeeze(0) img = img.transpose(1, 2, 0) img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) img = img.clip(0, 1) return img def load_image(path, size=None): &quot;&quot;&quot; Resize img to size, size should be int and also normalize the image using imagenet_stats &quot;&quot;&quot; img = Image.open(path) if size is not None: img = img.resize((size, size)) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) img = transform(img).unsqueeze(0) return img . . Detail:- When we load our images, what sizes should we use? Your content image size should be divisible by 4, as our model would downsample images 2 times. For style images, do not resize them. Use their original resolution. Size of content image is (500x500x3) and size of style image is (800x800x3). . Hi-Res Generation Network . . The model is quite simple we start with 500x500x3 images and maintain this resolution for the complete model. We downsample to 250x250 and 125x125 and then fuse these back together with 500x500 images. . Details:- . No pooling is used (as pooling causes loss of information). Instead strided convolution (i.e. stride=2) are used. | No dropout is used. But if you need regularization you can use weight decay. | 3x3 conv kernels are used everywhere with padding=1. | Zero padding is only used. Reflex padding was tested but the results were not good. | For upsampling,’bilinear’ mode is used. | For downsampling, conv layers are used. | InstanceNorm is used. | # Downsampling function def conv_down(in_c, out_c, stride=2): return nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1) # Upsampling function def upsample(input, scale_factor): return F.interpolate(input=input, scale_factor=scale_factor, mode=&#39;bilinear&#39;, align_corners=False) . . Implementation code . Residual connections are used between every block. We use BottleNeck layer from the ResNet architecture. (In Fig5 all the horizontal arrows are bottleneck layers). . Refresher on bottleneck layer. . . # Helper class for BottleneckBlock class ConvLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1): super().__init__() # We have to keep the size of images same, so choose padding accordingly num_pad = int(np.floor(kernel_size / 2)) self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=num_pad) def forward(self, x): return self.conv(x) class BottleneckBlock(nn.Module): &quot;&quot;&quot; Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used instead of BatchNorm because when we want to generate images, we normalize all the images independently. (In batch norm you compute mean and std over complete batch, while in instance norm you compute mean and std for each image channel independently). The reason for doing this is, the generated images are independent of each other, so we should not normalize them using a common statistic. If you confused about the bottleneck architecture refer to the official pytorch resnet implementation and paper. &quot;&quot;&quot; def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): super().__init__() self.in_c = in_channels self.out_c = out_channels self.identity_block = nn.Sequential( ConvLayer(in_channels, out_channels//4, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels//4, kernel_size, stride=stride), nn.InstanceNorm2d(out_channels//4), nn.ReLU(), ConvLayer(out_channels//4, out_channels, kernel_size=1, stride=1), nn.InstanceNorm2d(out_channels), nn.ReLU(), ) self.shortcut = nn.Sequential( ConvLayer(in_channels, out_channels, 1, stride), nn.InstanceNorm2d(out_channels), ) def forward(self, x): out = self.identity_block(x) if self.in_c == self.out_c: residual = x else: residual = self.shortcut(x) out += residual out = F.relu(out) return out . . Now we are ready to implement our style_transfer model, which we call HRNet (based on the paper). Use the Fig5 as reference. . class HRNet(nn.Module): &quot;&quot;&quot; For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf. Naming convention used. I refer to vertical layers as a single layer, so from left to right we have 8 layers excluding the input image. E.g. layer 1 contains the 500x500x16 block layer 2 contains 500x500x32 and 250x250x32 blocks and so on self.layer{x}_{y}: x :- the layer number, as explained above y :- the index number for that function starting from 1. So if layer 3 has two downsample functions I write them as `downsample3_1`, `downsample3_2` &quot;&quot;&quot; def __init__(self): super().__init__() self.layer1_1 = BottleneckBlock(3, 16) self.layer2_1 = BottleneckBlock(16, 32) self.downsample2_1 = conv_down(16, 32) self.layer3_1 = BottleneckBlock(32, 32) self.layer3_2 = BottleneckBlock(32, 32) self.downsample3_1 = conv_down(32, 32) self.downsample3_2 = conv_down(32, 32, stride=4) self.downsample3_3 = conv_down(32, 32) self.layer4_1 = BottleneckBlock(64, 64) self.layer5_1 = BottleneckBlock(192, 64) self.layer6_1 = BottleneckBlock(64, 32) self.layer7_1 = BottleneckBlock(32, 16) self.layer8_1 = conv_down(16, 3, stride=1) # Needed conv layer so reused conv_down function def forward(self, x): map1_1 = self.layer1_1(x) map2_1 = self.layer2_1(map1_1) map2_2 = self.downsample2_1(map1_1) map3_1 = torch.cat((self.layer3_1(map2_1), upsample(map2_2, 2)), 1) map3_2 = torch.cat((self.downsample3_1(map2_1), self.layer3_2(map2_2)), 1) map3_3 = torch.cat((self.downsample3_2(map2_1), self.downsample3_3(map2_2)), 1) map4_1 = torch.cat((self.layer4_1(map3_1), upsample(map3_2, 2), upsample(map3_3, 4)), 1) out = self.layer5_1(map4_1) out = self.layer6_1(out) out = self.layer7_1(out) out = self.layer8_1(out) return out . . Loss functions . In style transfer we use feature extraction, to calculate the value of losses. Feature extraction put in simple terms, means you take a pretrained imagenet model and pass your images through it and store the intermediate layer outputs. Generally, VGG model is used for such tasks. . . So you take the outputs from the conv layers. Like for the above fig, you can take the output from the second 3x3 conv 64 layer and then 3x3 conv 128. . To extract features from VGG we use the following code. . def get_features(img, model, layers=None): &quot;&quot;&quot; Use VGG19 to extract features from the intermediate layers. &quot;&quot;&quot; if layers is None: layers = { &#39;0&#39; : &#39;conv1_1&#39;, # style layer &#39;5&#39; : &#39;conv2_1&#39;, # style layer &#39;10&#39;: &#39;conv3_1&#39;, # style layer &#39;19&#39;: &#39;conv4_1&#39;, # style layer &#39;28&#39;: &#39;conv5_1&#39;, # style layer &#39;21&#39;: &#39;conv4_2&#39; # content layer } features = {} x = img for name, layer in model._modules.items(): x = layer(x) if name in layers: features[layers[name]] = x return features . . We use 5 layers in total for feature extraction. Only conv4_2 is used as layer for content loss. . Refer to Fig4, we pass our output image from HRNet and the original content and style image through VGG. . There are two losses . Content Loss | Style Loss | Content Loss . Content image and the output image should have a similar feature representation as computed by loss network VGG. Because we are only changing the style without any changes to the structure of the image. For the content loss, we use Euclidean distance as shown by the formula . $$l_{content}^{ phi,j}(y, hat{y})= frac{1}{C_jJ_jW_j} left | phi_j( hat{y}= phi_j(y) right |^2$$ . $ phi_j$ means we are referring to the activations of the j-th layer of loss network. In code it looks like this. . style_net = HRNet().to(device) . target = style_net(content_img).to(device) target.requiresgrad(True) . target_features = get_features(target, vgg) content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;]) ** 2) . Style Loss . We use gram matrix for this. So style of an image is given by its gram matrix. Our aim is to make style of two images close, so we compute the difference of gram matrix of style image and output image and then take their Frobenius norm. . $$l_{style}^{ phi,j}(y, hat{y})= left |G_j^{ phi}(y)-G_j^{ phi}( hat{y}) right |^2$$ . def get_gram_matrix(img): &quot;&quot;&quot; Compute the gram matrix by converting to 2D tensor and doing dot product img: (batch, channel, height, width) &quot;&quot;&quot; b, c, h, w = img.size() img = img.view(b*c, h*w) gram = torch.mm(img, img.t()) return gram # There are 5 layers, and we compute style loss for each layer and sum them up style_loss = 0 for layer in layers: target_gram_matrix = get_gram_matrix(target_feature) # we already computed gram matrix for our style image style_gram_matrix = style_gram_matrixs[layer] layer_style_loss = style_weights[layer] * torch.mean((target_gram_matrix - style_gram_matrix) ** 2) b, c, h, w = target_feature.shape style_loss += layer_style_loss / (c*h*w) . . Difficult part . To compute our final losses, we multiply them with some weights. . content_loss = content_weight * content_loss style_loss = style_weight * style_loss . The difficulty comes in setting these values. If you want some desired output, then you would have to test different values before you get your desired result. . To build your own intuitions you can choose two images and try different range of values. I am working on providing like a summary of this. It will be available in my repo README. . Paper recommends content_weight = [50, 100] and style_weight = [1, 10]. . Conclusion . Well, congratulation made it to the end. You can now implement style transfer. Now read the paper for more details on style transfer. . Check out my repo README, it will contain the complete instructions on how to use the code in the repo, along with complete steps on how to train your model. .",
            "url": "https://kushajveersingh.github.io/blog/all-you-need-for-photorealistic-style-transfer-in-pytorch",
            "relUrl": "/all-you-need-for-photorealistic-style-transfer-in-pytorch",
            "date": " • May 6, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "SPADE: State of the art in Image-to-Image Translation by Nvidia",
            "content": "Link to implementation code, paper . To give motivation for this paper, see the demo released by Nvidia. . . What is Semantic Image Synthesis? . It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class. . . New things in the paper . SPADE paper introduces a new normalization technique called spatially-adaptive normalization. Earlier models used the seg map only at the input layer but as seg map was only available in one layer the information contained in the seg map washed away in the deeper layers. SPADE solves this problem. In SPADE, we give seg map as input to all the intermediate layers. . How to train the model? . Before getting into the details of the model, I would discuss how models are trained for a task like Semantic Image Synthesis. . The core idea behind the model training is a GAN. Why GAN is needed? Because whenever we want to generate something that looks photorealistic or more technically closer to the output images, we have to use GANs. . So for GAN we need three things 1) Generator 2) Discriminator 3) Loss Function. For the Generator, we need to input some random values. Now you can either take random normal values. But if you want your output image to resemble some other image i.e. take the style of some image and add it your output image, you will also need an image encoder which would provide the mean and variance values for the random Gaussian distribution. . For the loss function, we would use the loss function used in pix2pixHD paper with some modifications. Also, I would discuss this technique where we extract features from the VGG model and then compute loss function (perceptual loss). . SPADE . This is the basic block that we would use. . . How to resize segmentation map? . Every pixel value in your seg map corresponds to a class and you cannot introduce new pixel values. When we use the defaults in various libraries for resizing, we do some form of interpolation like linear, which can change up the pixel values and result in values that were not there before. To solve this problem, whenever you have to resize your segmentation map use ‘nearest’ as the upsampling or downsampling method. . How we use it? Consider some layer in your model, you want to add the information from the segmentation map to the output of that layer. That will be done using SPADE. . SPADE first resizes your seg map to match the size of the features and then we apply a conv layer to the resized seg map to extract the features. To normalize our feature map, we first normalize our feature map using BatchNorm and then denormalize using the values we get from the seg map. . class SPADE(Module): def __init__(self, args, k): super().__init__() num_filters = args.spade_filter kernel_size = args.spade_kernel self.conv = spectral_norm(Conv2d(1, num_filters, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_gamma = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) self.conv_beta = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1)) def forward(self, x, seg): N, C, H, W = x.size() sum_channel = torch.sum(x.reshape(N, C, H*W), dim=-1) mean = sum_channel / (N*H*W) std = torch.sqrt((sum_channel**2 - mean**2) / (N*H*W)) mean = torch.unsqueeze(torch.unsqueeze(mean, -1), -1) std = torch.unsqueeze(torch.unsqueeze(std, -1), -1) x = (x - mean) / std seg = F.interpolate(seg, size=(H,W), mode=&#39;nearest&#39;) seg = relu(self.conv(seg)) seg_gamma = self.conv_gamma(seg) seg_beta = self.conv_beta(seg) x = torch.matmul(seg_gamma, x) + seg_beta return x . . SPADERes Block . Just like Resnet where we combine conv layers into a ResNet Block, we combine SPADE into a SPADEResBlk. . . The idea is simple we are just extending the ResNet block. The skip-connection is important as it allows for training of deeper networks and we do not have to suffer from problems of vanishing gradients. . class SPADEResBlk(Module): def __init__(self, args, k, skip=False): super().__init__() kernel_size = args.spade_resblk_kernel self.skip = skip if self.skip: self.spade1 = SPADE(args, 2*k) self.conv1 = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade_skip = SPADE(args, 2*k) self.conv_skip = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) else: self.spade1 = SPADE(args, k) self.conv1 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) self.spade2 = SPADE(args, k) self.conv2 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False) def forward(self, x, seg): x_skip = x x = relu(self.spade1(x, seg)) x = self.conv1(x) x = relu(self.spade2(x, seg)) x = self.conv2(x) if self.skip: x_skip = relu(self.spade_skip(x_skip, seg)) x_skip = self.conv_skip(x_skip) return x_skip + x . . Now we have our basic blocks, we start coding up our GAN. Again, the three things that we need for GAN are: . Generator | Discriminator | Loss Function | Generator . . class SPADEGenerator(nn.Module): def __init__(self, args): super().__init__() self.linear = Linear(args.gen_input_size, args.gen_hidden_size) self.spade_resblk1 = SPADEResBlk(args, 1024) self.spade_resblk2 = SPADEResBlk(args, 1024) self.spade_resblk3 = SPADEResBlk(args, 1024) self.spade_resblk4 = SPADEResBlk(args, 512) self.spade_resblk5 = SPADEResBlk(args, 256) self.spade_resblk6 = SPADEResBlk(args, 128) self.spade_resblk7 = SPADEResBlk(args, 64) self.conv = spectral_norm(Conv2d(64, 3, kernel_size=(3,3), padding=1)) def forward(self, x, seg): b, c, h, w = seg.size() x = self.linear(x) x = x.view(b, -1, 4, 4) x = interpolate(self.spade_resblk1(x, seg), size=(2*h, 2*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk2(x, seg), size=(4*h, 4*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk3(x, seg), size=(8*h, 8*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk4(x, seg), size=(16*h, 16*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk5(x, seg), size=(32*h, 32*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk6(x, seg), size=(64*h, 64*w), mode=&#39;nearest&#39;) x = interpolate(self.spade_resblk7(x, seg), size=(128*h, 128*w), mode=&#39;nearest&#39;) x = tanh(self.conv(x)) return x . . Discriminator . . def custom_model1(in_chan, out_chan): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=2, padding=1)), nn.LeakyReLU(inplace=True) ) def custom_model2(in_chan, out_chan, stride=2): return nn.Sequential( spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=stride, padding=1)), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEDiscriminator(nn.Module): def __init__(self, args): super().__init__() self.layer1 = custom_model1(4, 64) self.layer2 = custom_model2(64, 128) self.layer3 = custom_model2(128, 256) self.layer4 = custom_model2(256, 512, stride=1) self.inst_norm = nn.InstanceNorm2d(512) self.conv = spectral_norm(nn.Conv2d(512, 1, kernel_size=(4,4), padding=1)) def forward(self, img, seg): x = torch.cat((seg, img.detach()), dim=1) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = leaky_relu(self.inst_norm(x)) x = self.conv(x) return x . . Loss function . The most important piece for training a GAN. We are all familiar with the loss function of minimizing the Generator and maximizing the discriminator, where the objective function looks something like this. . $$ mathbb{E}_{( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})}[ log D( boldsymbol{ mathrm{s}}, boldsymbol{ mathrm{x}})]+ mathbb{E}_{ boldsymbol{ mathrm{s}}}[ log (1-D( boldsymbol{ mathrm{s}},G( boldsymbol{ mathrm{s}})$$ . Now we extend this loss function to a feature matching loss. What do I mean? When we compute this loss function we are only computing the values on a fixed size of the image, but what if we compute the losses at different sizes of the image and then sum them all. . This loss would stabilize training as the generator has to produce natural statistics at multiple scales. To do so, we extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized images. This is done by taking features out of a pretrained VGG model. This is called perceptual loss. The code makes it easier to understand. . class VGGLoss(nn.Module): def __init__(self): super().__init__() self.vgg = VGG19().cuda() self.criterion = nn.L1Loss() self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0] def forward(self, x, y): x_vgg, y_vgg = self.vgg(x), self.vgg(y) loss = 0 for i in range(len(x_vgg)): loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach()) return loss . . So we take the two images, real and synthesized and pass it through VGG network. We compare the intermediate feature maps to compute the loss. We can also use ResNet, but VGG works pretty good and earlier layers of VGG are generally good at extracting the features of an image. . This is not the complete loss function. Below I show my implementation without the perceptual loss. I strongly recommend seeing the loss function implementation used by Nvidia themselves for this project as it combines the above loss also and it would also provide a general guideline on how to train GANs in 2019. . class GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0, tensor=torch.FloatTensor): super().__init__() self.real_label = target_real_label self.fake_label = target_fake_label self.real_label_var = None self.fake_label_var = None self.Tensor = tensor if use_lsgan: self.loss = nn.L1Loss() else: self.loss = nn.BCELoss() def get_target_tensor(self, input, target_is_real): target_tensor = None if target_is_real: create_label = ((self.real_label_var is None) or (self.real_label_var.numel() != input.numel())) if create_label: real_tensor = self.Tensor(input.size()).fill_(self.real_label) self.real_label_var = torch.tensor(real_tensor, requires_grad=False) target_tensor = self.real_label_var else: create_label = ((self.fake_label_var is None) or (self.fake_label_var.numel() != input.numel())) if create_label: fake_tensor = self.Tensor(input.size()).fill_(self.fake_label) self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False) target_tensor = self.fake_label_var return target_tensor def __call__(self, input, target_is_real): target_tensor = self.get_target_tensor(input, target_is_real) return self.loss(input, target_tensor.to(torch.device(&#39;cuda&#39;))) . . Weight Init . In the paper, they used Glorot Initialization (another name of Xavier initialization). I prefer to use He. Initialization. . def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) . . Image Encoder . This is the final part of our model. It is used if you want to transfer style from one image to the output of SPADE. It works by outputting the mean and variance values from which we compute the random gaussian noise that we input to the generator. . . def conv_inst_lrelu(in_chan, out_chan): return nn.Sequential( nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=2, bias=False, padding=1), nn.InstanceNorm2d(out_chan), nn.LeakyReLU(inplace=True) ) class SPADEEncoder(nn.Module): def __init__(self, args): super().__init__() self.layer1 = conv_inst_lrelu(3, 64) self.layer2 = conv_inst_lrelu(64, 128) self.layer3 = conv_inst_lrelu(128, 256) self.layer4 = conv_inst_lrelu(256, 512) self.layer5 = conv_inst_lrelu(512, 512) self.layer6 = conv_inst_lrelu(512, 512) self.linear_mean = nn.Linear(8192, args.gen_input_size) self.linear_var = nn.Linear(8192, args.gen_input_size) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.layer5(x) x = self.layer6(x) x = x.view(x.size(0), -1) return self.linear_mean(x), self.linear_var(x) . . Why Spectral Normalization? . Spectral Normalization Explained by Christian Cosgrove. This article discusses spectral norm in detail with all the maths behind it. Ian Goodfellow even commented on spectral normalization and considers it to be an important tool. . The reason we need spectral norm is that when we are generating images, it can become a problem to train our model to generate images of say 1000 categories on ImageNet. Spectral Norm helps by stabilizing the training of discriminator. There are theoretical justifications behind this, on why this should be done, but all that is beautifully explained in the above blog post that I linked to. . To use spectral norm in your model, just apply spectral_norm to all convolutional layers in your generator and discriminator. . Brief Discussion on Instance normalization . . Batch Normalization uses the complete batch to compute the mean and std and then normalizes the complete batch with a single value of mean and std. This is good when we are doing classification, but when we are generating images, we want to keep the normalization of these images independent. . One simple reason for that is if in my batch one image is being generated for blue sky and in another image, generating a road then clearly normalizing these with the same mean and std would add extra noise to the images, which would make training worse. So instance norm is used instead of batch normalization here. . Resources . My Implementation link | SPADE Paper link | Official Implementation link | pix2pixHD link | Spectral Normalization paper link | Spectral Norm blog link | Instance Normalization paper link | Instance norm other resources, blog, stack overflow | .",
            "url": "https://kushajveersingh.github.io/blog/spade-state-of-the-art-in-image-to-image-translation-by-nvidia",
            "relUrl": "/spade-state-of-the-art-in-image-to-image-translation-by-nvidia",
            "date": " • Apr 17, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Weight Standardization: A new normalization in town",
            "content": "Link to jupyter notebook, paper . Recently a new normalization technique is proposed not for the activations but for the weights themselves in the paper Weight Standardization. . In short, to get new state of the art results, they combined Batch Normalization and Weight Standardization. So in this post, I discuss what is weight standardization and how it helps in the training process, and I will show my own experiments on CIFAR-10 which you can also follow along. . . For my experiments, I will use cyclic learning. As the paper discusses training with constant learning rates, I would use cyclic LR as presented by Leslie N. Smith in his report. . To make things cleaner I would use this notation:- . BN -&gt; Batch Normalization | GN -&gt; Group Normalization | WS -&gt; Weight Standardization | . What is wrong with BN and GB? . Ideally, nothing is wrong with them. But to get the most benefit out of BN we have to use a large batch size. And when we have smaller batch sizes we prefer to use GN. (By smaller I mean 1–2 images/GPU). . Why is this so? . To understand it we have to see how BN works. To make things simple, consider we have only one-channel on which we want to apply BN and we have 2 images as our batch size. . Now we would compute the mean and variance using the 2 images and then normalize the one-channel of the 2 images. So we used 2 images to compute mean and variance. This is the problem. . By increasing batch size, we are able to sample the value of mean and variance from a larger population, which means that the computed mean and variance would be closer to their real values. . GN was introduced for cases of small batch sizes but it was not able to meet the results that BN was able to achieve using larger batch sizes. . How these normalization actually help? . It is one of the leading areas of research. But it was recently shown in the paper Fixup Initialization: Residual Learning without Normalization the reason for the performance gains using BN. . In short, it helps make the loss surface smooth. . . When we make the loss surface smooth we can take longer steps, which means we can increase our learning rate. So using Batch Norm actually stabilizes our training and also makes it faster. . Weight Standardization . Unlike BN and GN that we apply on activations i.e the output of the conv layer, we apply Weight Standardization on the weights of the conv layer itself. So we are applying WS to the kernels that our conv layer uses. . How does this help? . For the theoretical justification see the original paper where they prove WS reduces the Lipschitz constants of the loss and the gradients. . But there are easier ways to understand it. . First, consider the optimizer we use. The role of the optimizer is to optimize the weights of our model, but when we apply normalization layers like BN, we do not normalize our weights, but instead, we normalize the activations which are optimizer does not even care about. . By using WS we are essentially normalizing the gradients during the backpropagation. . The authors of the paper tested WS on various computer vision tasks and they were able to achieve better results with the combination of WS+GN and WS+BN. The tasks that they tested on included: . Image Classification | Object Detection | Video Recognition | Semantic Segmentation | Point Cloud Classification | Enough talk, let&#39;s go to experiments . The code is available in the notebook. . How to implement WS? . class Conv2d(nn.Module): def __init__(self, in_chan, out_chan, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True): super().__init__(in_chan, out_chan, kernel_size, stride, padding, dilation, groups, bias) def forward(self, x): weight = self.weight weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True) weight = weight - weight_mean std = weight.view(weight.size(0), -1).std(dim=1).view(-1,1,1,1)+1e-5 weight = weight / std.expand_as(weight) return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups) . . First, let&#39;s try out at batch size = 64 . This will provide a baseline of what we should expect. For this, I create 2 resnet18 models: . resnet18 $ rightarrow$ It uses the nn.Conv2d layers | resnet18_ws $ rightarrow$ It uses above Conv2d layer which uses weight standardization | I change the head of resnet model, as CIFAR images are already 32 in size and I don’t want to half their size initially. The code can be found in the notebook. And for the CIFAR dataset, I use the official train and valid split. . First I plot the value of loss v/s learning rate. . . For those not familiar with loss v/s learning_rate graph. We are looking for the maximum value of lr at which the loss value starts increasing. . In this case the max_lr is around 0.0005. So let’s try to train model for some steps and see. In case you wonder in the second case the graph is flatter around 1e-2, it is because the scale of the two graphs is different. . So now let’s train our model and see what happens. I am using the fit_one_cycle to train my model. . . There is not much difference between the two as valid loss almost remains the same. . Try at bs=2 . Now I take a batch size of 2 and train the models in a similar manner. . . One thing that I should add here, is the loss diverged quickly when I used only BN, after around 40 iterations, while in the case of WS+BN the loss did not diverge. . . There is not much difference in the loss values, but the time to run each cycle increased very much. . Trying bs=256 . Also, I run some more experiments where I used a batch size of 256. Although, I could use a larger learning rate but the time taken to complete the cycle increased. The results are shown below. . . . Again, in the graph we see we can use a larger learning rate. . Conclusion . From the above experiments, I think I would prefer not to use Weight Standardization when I am using cyclic learning. For large batch sizes, it even gave worse performance and for smaller batch sizes, it gave almost similar results, but using weight standardization we added a lot of time to our computation, which we could have used to train our model with Batch Norm alone. . For constant learning rate, I think weight standardization still makes sense as there we do not change our learning rate in the training process, so we must benefit from the smoother loss function. But in the case of cyclic learning, it does not offer us a benefit. .",
            "url": "https://kushajveersingh.github.io/blog/weight-standardization-a-new-normalization-in-town",
            "relUrl": "/weight-standardization-a-new-normalization-in-town",
            "date": " • Apr 11, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch",
            "content": "Link to jupyter notebook . Data . To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work. . . Note: - I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do. . Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use ImageNet Object Localization Challenge to directly download all the files (warning 155GB). | Unzip the tar.gz file using tar xzvf file_name -C destination_path. | In the Data/CLS-LOC folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders. | Use this command from your terminal in the val folder wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash. It would move all the images to their respective class folders. | As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^&gt;” {}. | After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???. . . Data Loading . Steps involved:- . Create a dataset class or use a predefined class | Choose what transforms you want to perform on the data. | Create data loaders | . train_dir = &#39;../../../Data/ILSVRC2012/train&#39; val_dir = &#39;../../../Data/ILSVRC2012/val&#39; size = 224 batch_size = 32 num_workers = 8 data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]), &#39;val&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) } image_datasets = { &#39;train&#39;: ImageFolder(train_dir, transform=data_transforms[&#39;train&#39;]), &#39;val&#39;: ImageFolder(val_dir, transform=data_transforms[&#39;val&#39;]), } data_loader = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in [&#39;train&#39;, &#39;val&#39;] } . . The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step. . Check dataloaders . After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images. . # As our images are normalized we have to denormalize them and # convert them to numpy arrays. def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std*img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) #Pause is necessary to display images correctly images, labels = next(iter(data_loader[&#39;train&#39;])) grid_img = make_grid(images[:4], nrow=4) imshow(grid_img, title = [labels_list[x] for x in labels[:4]]) . . One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file LOC_synset_mapping.txt. . f = open(&quot;../../Data/LOC_synset_mapping.txt&quot;, &quot;r&quot;) labels_dict = {} # Get class label by folder name labels_list = [] # Get class label by indexing for line in f: split = line.split(&#39; &#39;, maxsplit=1) split[1] = split[1][:-1] label_id, label = split[0], split[1] labels_dict[label_id] = label labels_list.append(split[1]) . . Model Construction . Create your model. Pretrained models covered at the end of the post. . . Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’. . class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.conv_base = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False), nn.BatchNorm2d(96), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc_base = nn.Sequential( nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.conv_base(x) x = x.view(x.size(0), 256*6*6) x = self.fc_base(x) return x . . See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization). . Best practices for CNN . Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always. | Weight Initialization:- Use He initialization as default with ReLU. PyTorch provides kaimingnormal for this purpose. | Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. | Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model. | Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good. | Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference. | Weight Initialization . Do not use this method as a default. After, naming the layers you can do this very easily. . conv_list = [0, 4, 8, 10, 12] fc_list = [1, 4, 6] for i in conv_list: torch.nn.init.kaiming_normal_(model.conv_base[i].weight) for i in fc_list: torch.nn.init.kaiming_normal_(model.fc_base[i].weight) . . Create optimizers, schedulers and loss functions. . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) # Cross entropy loss takes the logits directly, so we don&#39;t need to apply softmax in our CNN criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;max&#39;, verbose=True) . . PyTorch specific discussion . You have to specify the padding yourself. Check this thread for discussion on this topic. | Create the optimizer after moving the model to GPU. | The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself. | Do not forget to zero the grads. | . How to check my model is correct? . Check 1 . The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input. . inputs, labels = next(iter(data_loader[&#39;train&#39;])) inputs = inputs.to(device) labels = labels.to(device) criterion_check1 = nn.CrossEntropyLoss() optimizer_check1 = optim.SGD(model.parameters(), lr=0.001) model.train() for epoch in range(200): optimizer_check1.zero_grad() outputs = model(inputs) loss = criterion_check1(outputs, labels) _, preds = torch.max(outputs, 1) loss.backward() optimizer_check1.step() if epoch%10 == 0: print(&#39;Epoch {}: Loss = {} Accuracy = {}&#39;.format(epoch+1, loss.item(), torch.sum(preds == labels))) . . . . Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again. . Check 2 . Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems. . In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs. . Using Pretrained Models . As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset. . . Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended. . data = np.load(&#39;../../../Data/cifar_10.npz&#39;) alexnet = torch.load(&#39;../../../Data/Pytorch Trained Models/alexnet.pth&#39;) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) x_train = torch.from_numpy(data[&#39;train_data&#39;]) y_train = torch.from_numpy(data[&#39;train_labels&#39;]) x_test = torch.from_numpy(data[&#39;test_data&#39;]) y_test = torch.from_numpy(data[&#39;test_labels&#39;]) . . # Create data loader class CIFAR_Dataset(torch.utils.data.Dataset): &quot;&quot;&quot; Generally you would not load images in the __init__ as it forces the images to load into memory. Instead you should load the images in getitem function, but as CIFAR is small dataset I load all the images in memory. &quot;&quot;&quot; def __init__(self, x, y, transform=None): self.x = x self.y = y self.transform = transform def __len__(self): return self.x.size(0) def __getitem__(self, idx): image = self.x[idx] label = self.y[idx].item() if self.transform: image = self.transform(image) return (image, label) data_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) datasets = { &#39;train&#39;: CIFAR_Dataset(x_train, y_train, transform=data_transforms), &#39;test&#39;: CIFAR_Dataset(x_test, y_test, transform=data_transforms) } data_loader = { x: torch.utils.data.DataLoader(datasets[x], batch_size=64, shuffle=True, num_workers=8) for x in [&#39;train&#39;, &#39;test&#39;] } . . # Freeze conv layers for param in alexnet.parameters(): param.requires_grad = False # Initialize the last layer of alexnet model for out 10 class dataset alexnet.classifier[6] = nn.Linear(4096, 10) alexnet = alexnet.to(device) criterion = nn.CrossEntropyLoss() # Create list of params to learn params_to_learn = [] for name,param in alexnet.named_parameters(): if param.requires_grad == True: params_to_learn.append(param) optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True) . . Refer to this script on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from torchvision.datasets. . PyTorch has a very good tutorial on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook. . Conclusion . We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images. . References . https://github.com/soumith/imagenet-multiGPU.torch Helped in preprocessing of the Imagenet dataset. | https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html Many code references were taken from this tutorial. | https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf AlexNet paper |",
            "url": "https://kushajveersingh.github.io/blog/training-alexnet-with-tips-and-checks-on-how-train-cnns-practical-cnns-in-pytorch",
            "relUrl": "/training-alexnet-with-tips-and-checks-on-how-train-cnns-practical-cnns-in-pytorch",
            "date": " • Feb 18, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Theoretical Machine Learning: Probabilistic and Statistical Math",
            "content": "I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here. . . Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as Polynomial Curve Fitting in ML. . In curve fitting, we usually use the maximum likelihood approach which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data). . To overcome overfitting we use regularization techniques by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin. . Quick Probability Overview . Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as X = Event that heads come and Y = Event that tails come. Now P(X) = Probability of that particular event happening. . Takeaway, represent your probabilities as events of something happening. . In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable. . Next, we discuss expectation. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value? . Suppose x takes the value 10, the average of that value is the number itself. . . Next, we discuss variance. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is. . Another important concept in probabilistic maths is the concept of prior, posterior and likelihood. . . Prior = P(X). It is probability available before we observing an event . Posterior = P(X|Y). It is the probability of X after event Y has happened. . Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X . Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X). . . Tip: Follow the resource list given at the end for further reading of the topic. . Gaussian Distribution . As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution. . . But before that let us discuss why we need to know about probability distributions in the first place? . When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output. . Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases). . . Note: From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule. . As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models? . After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial. . Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later. . . Note: In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights. . Now I present a general way of approaching the problem of finding the values for the variables of a prior. . The Gaussian Equation is represented as follow . $$ mathcal{N}(x| mu, sigma^2) = frac{1}{(2 pi sigma^2)^{1/2}}exp {- frac{1}{2 sigma^2}(x- mu)^2 }$$ . Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution. . So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed N values as input. Now all these N values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as . $$p(x| mu, sigma^2)= prod_{n=1}^{N} mathcal{N}(x_n| mu, sigma^2)$$ . After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function. . $$ ln p(x| mu, sigma^2)=- frac{1}{2 sigma^2} sum_{n=1}^{N}(x_n- mu)^2- frac{N}{2} ln sigma^2- frac{N}{2} ln (2 pi)$$ . Now take the derivative w.r.t the variables and get their values. . $$ mu_{ML}= frac{1}{N} prod_{n=1}^{N}x_n$$ $$ sigma^2_{ML}= frac{1}{N} prod_{n=1}{N}(x_n- mu_{ML})^2$$ . . Note: In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data. . Why was all this important? . All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior. . In cases where you have to take a prior like Poisson MSE would not be a good metric to consider. . Set Prior on the variables also . . Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution? . Ideally, you can choose any distribution. But practically you want to choose a conjugate prior for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors. . Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance. . Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition. . Here is the list of resources that I would highly recommend for learning ML:- . CS229: Machine Learning by Stanford | Pattern Recognition and Machine Learning by Christopher M. Bishop | An Introduction to Statistical Learning: with Applications in R | The Elements of Statistical Learning Data Mining, inference and Prediction |",
            "url": "https://kushajveersingh.github.io/blog/theoretical-machine-learning-probalistic-and-statistical-math",
            "relUrl": "/theoretical-machine-learning-probalistic-and-statistical-math",
            "date": " • Sep 14, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Projects",
          "content": "Full-stack developer experienced with creating fully-responsive mobile-first web applications and doing API integrations, reading documentation to add new features to an existing codebase, code review processes, CI/CD testing. Self-motivated and passionate to learn new technologies and build projects on top of it. Experienced with deploying 5 projects on Vercel, contributing to open-source projects on GitHub for over 5 years, and maintaining a technical blog with 16 published posts. . . Education | Work Experience | Additional Experience | Projects | Blogs | Certifications | . Education . Master of Science in Computer Science University of Georgia, Athens, GA, USA (Jan 2021 - Dec 2022) | GPA: 3.82 / 4.00 | . | Bachelor of Technology in Electronics and Communication Engineering Punjab Engineering College (Deemed to be University), Chandigarh, India (Aug 2016 - Jun 2020) | CGPA: 7.0 / 10.0 | . | . Work Experience . Full Stack Developer (Jul 2022 - Dec 2022) (UGA) Worked in the IT department of the College of Agriculture and Environmental Sciences, University of Georgia (Athens, GA, USA) | Implemented Node.js/Express parser to do real time Microsoft Access to PostgreSQL query conversion | Brought up development and production servers, including IP whitelisting and backup configuration with periodic syncing, and migrated 18 years of data from Access to PostgreSQL database | Automated data entry pipeline using Python/TypeScript, resulting in a reduction of time from 2 minutes per handwritten form to 25 seconds on average, by implementing autocomplete functionality for various fields | Rewrote the entire codebase for a proprietary language, which included converting information from a 200-page manual to a TypeScript/Next.js application and updating the queries from Access to PostgreSQL | . | Graduate Teaching Assistant (Jan 2022 - Aug 2022) (UGA) Worked in the Computer Science department, University of Georgia (Athens, GA, USA) | Led weekly meetings and facilitated group discussions for the course Discrete Mathematics for Computer Science | Held one-on-one office hours for 2 transfer students, covering the entire course in detail while covering practical applications of the content and mentored students on the career paths in computer science | . | . Additional Experience . Google CS Research Mentorship Program (Sep 2022 - Dec 2022) (Athens, GA, USA (Remote)) Selected for a 12-week competitive Google CS Research Mentorship Program (CSRMP 2022B) among 50 candidates selected from all across USA and Canada. | The mentorship program inspires students from Historically Marginalized Groups (HMGs) to pursue and persist a career in CS research. | . | Poster submission, School of Computing Research Day (Sep 2022 - Oct 2022) (UGA) Submitted 2 posters for the School of Computing, Research Day at University of Georgia (Athens, GA, USA) | Created a module that improves the robustness of Graph Neural Networks (GNNs) against adversarial attacks | Proposed module beats state-of-the-art in 3 out of 4 datasets, while only adding 5 lines of code | Presented a poster on generating full human body 3D scans from random noise | . | Workshop paper reviewer, CVPR 2021 (Apr 2021) (UGA) Reviewed 2 papers for the Biometrics Workshop, jointly with the Workshop on Analysis and Modeling of Faces and Gestures, CVPR 2021 | Split the review into summary, strengths, weaknesses | . | Paper reviewer and presentation judge, GJSHS 2021 (Feb 2021 - Mar 2021) (UGA) Reviewed 6 papers for the 46th Georgia Junior Science &amp; Humanities Symposium (GJSHS) from high-school students. | The review included detailed feedback on the strengths and weaknesses of the paper and suggestions for improvement based on the recent research in the field | Presentation judge for the final round of the competition. | . | Winner TechGig CodeGladiators hackathon (May 2019 - July 2019) (Mumbai, Maharashtra, India) Won the first sole place in TechGig CodeGladiators, in the Artificial Intelligence theme where 15 teams were selected from all across India from a pool of 0.5 million candidates. | Implemented a fully-deployable parking space detection system using PyTorch from video. | Detections from multiple frames can be combined and adjusted over time depending on where the cars are being parked. | Frame can be split into a 3x3 grid allowing to make finer and more robust predictions and the result is combined into a single frame. | . | Third place at IndiaSkills Nationals (Sep 2018 - Nov 2018) (Chandigarh, India) Won third place at the North-Zone regional finals in IT Software Solutions for Business at IndiaSkills, representing my state Chandigarh (India) | Build a desktop application using C# and an android application using Java. | Both applications required a database setup using MySQL and phpMyAdmin. | . | 8-bit computer at PEC IEEE showcase (Feb 2017 - Apr 2017) (PEC, Chandigarh, India) Created an 8-bit computer on breadboard using NAND chips. | Implemented Add, Subtract logic and 2 byte memory module using Flip Flops | Showcased the work to high school students at Punjab Engineering College (Chandigarh, India) IEEE Project showcase | . | . Projects . Youtube Video Platform github, demo Built fully-responsive React/Next.js/TypeScript/Material-UI web app using YouTube API and deployed on Vercel | Implemented video section, category section, responsive channel and video cards, channel pages, video pages with ability to play videos straight from the app and see related videos | . | Video Sharing App github, demo Built fully-responsive full-stack video sharing social media app using React/Next.js/TypeScript/Tailwind CSS/Zustand/Sanity and deployed on Vercel | Google auth to register and login user, ability to upload, publish, share, comment on and like the videos | Advanced search functionalities, filter by categories, profile pages, and see suggested accounts | . | Chat Messaging App github, demo Built fully-responsive full-stack chat messaging application using React/TypeScript/Bcrypt/Stream Chat API | Support for authentication, Twilio SMS notification, direct and group chats, emojis and reactions, GIF support | Deployed React frontend on Vercel, and deployed Express.js backend on Heroku | . | Ecommerce Website github, demo Built full-responsive and scalable modern ecommerce website using Next.js/TypeScript and deployed on Vercel | Used Next.js as backend endpoint, Stripe for managing payments, shipping rates and entire checkout process | Manage entire content of website using Sanity, change details of products or add new products on the go | . | Music Player and Discovery App github, demo Built fully-responsive modern music player app using React/Next.js/Tailwind CSS/TypeScript/Redux/Shazam API | Choose genre and view top songs, see top charts and artists for the country or worldwide | Music player with controls to go to next/previous song, repeat, shuffle, fast forward, adjust volume | Fully functional search and pages to explore most popular songs in the country, using IP Geolocation API | View song’s lyrics and official video, related songs, and other songs by the artist | . | Sort Visualizer github, demo Built React/Redux/Material-UI web application for visualizing sorting algorithms and deployed on Vercel | Implemented over 30 sorting algorithms, with ability to view upto 9 algorithms in parallel | . | Group Video Chat github, demo Built group video chat app using JavaScript/Agora, and deployed the backend on Heroku and frontend on Vercel | Conference chat with multiple people at same time, screen share, private/group messaging, admin controls, polls | Worked with an enterprise scale codebase and added features on top of it by reading documentation | . | Restaurant Website github, demo Converted Figma design into fully-responsive Next.js/TypeScript/CSS(BEM) web app and deployed on Vercel | Modern UI/UX website built using CSS Flexbox/Grid, animations, and gradients | . | Pathfinding Visualizer github, demo Built fully-responsive React/Next.js/TypeScript/Redux web app for visualizing path-finding algorithms, maze-generation algorithms and deployed on Vercel | Implemented 9 different path-finding algorithms and added random weight generation methods to visualize weighted shortest cost algorithms | . | Sort Visualizer github, demo Deployed fully-responsive React/Next.js/TypeScript/Redux web app for visualizing sorting algorithms on Vercel | Implemented over 30 sorting algorithms, view multiple algorithms in parallel, generate 4 types of random array | . | JSON/YAML Graph Visualizer github, demo Built React/Next.js/Redux/Material-UI web app to convert JSON/YAML data into graph for improved readability | Wrote YAML to JSON parser in TypeScript | . | Bookstore application github Built full-stack bookstore application using HTML/CSS/JavaScript for frontend and Django/SQLite for backend | Implemented user authentication with email, change/forgot password, edit profile, advanced search options, cart management, apply promotions, entire process for placing an order | . | 3D Human Reconstruction github Engineered a machine learning project to generate a full human body 3D object from random noise | Combined GAN, Pose Estimation, RGB-to-3D object models from 3 different repositories | Upgraded PyTorch dependency from 0.4.0, 1.4.0, 1.9.0 to 1.11.1 and upgraded all repositories to CUDA 11.3 from 10.x versions | . | Credit Assessment github Created a fair and explainable model to approve credit card requests | Used LIME predictions and built PySimpleGUI to explain the predictions | . | Resizer Network for Computer Vision github, blog Built PyTorch model to resize images for downstream tasks, based on Google AI Learning to Resize Images for Computer VIsion Tasks | Tested model on 2 subsets of ImageNet dataset and demonstrated the improved performance using the proposed model | . | Parking Lot Detection github Built a fully deployable and unsupervised parking space detection system using PyTorch | Ability to adjust the predictions based on where cars are parked over time | Combine results from multiple frames to fill spots and make the predictions more robust | . | Semantic Image Synthesis github, blog Open-sourced the first public implementation of GauGAN paper by Nvidia | Implemented PyTorch GAN model to convert an image map into a realistic image | . | Style Transfer github, blog Implemented PyTorch model to transfer style between images | . | Unscramble Game Solver github Created a Python program to solve Unscramble android game | Implemented an efficient English dictionary lookup | . | Random Duty List github Built PHP/MySQL program for Chandigarh, India police department | Assign duties at various stations without repetition between days | . | . Jupyter Notebooks . Mish activation function is tested for transfer learning. Here mish is used only in the last fully-connected layers of a pretrainened Resnet50 model. I test the activation function of CIFAR10, CIFAR100 using three different learning rate values. I found that Mish gave better results than ReLU. notebook, paper . | Multi Sample Dropout is implemented and tested on CIFAR-100 using cyclic learning. My losses converged 4x faster when using num_samples=8 than using simple dropout. notebook, paper . | Data Augmentation in Computer Vision Notebook implementing single image data augmentation techniques using just Python notebook | . | Summarizing Leslie N. Smith’s research in cyclic learning and hyper-parameter setting techniques. notebook A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay paper | Super-Convergence: Very Fast Training of Neural Networks Using Learning Rates paper | Exploring loss function topology with cyclical learning rates paper | Cyclical Learning Rates for Training Neural Networks paper | . | Photorealisitc Style Transfer. Implementation of the High-Resolution Network for Photorealistic Style Transfer paper. notebook, paper . | Weight Standardization is implemented and tested using cyclic learning. I find that it does not work well with cyclic learning when using CIFAR-10. notebook, paper . | Learning Rate Finder. Implementation of learning rate finder as introduced in the paper Cyclical Learning Rates for Training Neural Networks. A general template for custom models is provided. notebook . | PyTorch computer vision tutorial. AlexNet with tips and checks on how to train CNNs. The following things are included: notebook Dataloader creation | Plotting dataloader results | Weight Initialization | Simple training loop | Overfitting a mini-batch | . | Waste Seggregation using trashnet github. Contains the code to train models for trashnet and then export them using ONNX. It was part of a bigger project where we ran these models on Rasberry Pi, which controlled wooden planks to classify the waste into different categories (code for rasberry pi not included here). | . Blogs . Complete tutorial on building images using Docker link | Data augmentation with learnable Resizer network for Image Classification link | Writing custom CUDA kernels with Triton link | Complete tutorial on how to use Hydra in Machine Learning projects link | What can neural networks reason about? link | ImageNet Dataset Advancements link | Deep Learning Model Initialization in Detail link | How to setup personal blog using Ghost and Github hosting link | Study of Mish activation function in transfer learning with code and discussion link | Reproducing Cyclic Learning papers + SuperConvergence using fastai link | How to become an expert in NLP in 2019 link | All you need for Photorealistic Style Transfer in PyTorch link | SPADE: State of the art in Image-to-Image Translation by Nvidia link | Weight Standardization: A new normalization in town link | Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch link | Theoretical Machine Learning: Probabilities and Statistical Math link | . Certifications . GCP Essentials (Qwiklabs) link | Executive Data Science Specialization (Johns Hopkins University, Coursera) A Crash Course in Data Science link | Building a Data Science Team link | Managing Data Analysis link | Data Science in Real Life link | Executive Data Science Capstone link | . | Data Science at Scale Specialization (University of Washington, Coursera) Data Manipulation at Scale: Systems and Algorithms link | Practical Predictive Analytics: Models and Methods link | . | Machine Learning (Stanford University, Coursera) link | Bayesian Statistics: From Concept to Data Analysis (University of California, Santa Cruz, Coursera) link | Neural Networks for Machine Learning (University of Toronto, Coursera) link | Deep Learning Specialization (DeepLearning.AI, Coursera) Neural Networks and Deep Learning link | Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization link | Structuring Machine Learning Projects link | Convolutional Neural Networks link | Sequence Models link | . | Recommender Systems Specialization (University of Minnesota, Coursera) Introduction to Recommender Systems: Non-Personalized and Content-Based link | Nearest Neighbor Collaborative Filtering link | Recommender Systems: Evaluation and Metrics link | Matrix Factorization and Advanced Techniques link | . | Genomic Data Science Specialization (Johns Hopkins University, Coursera) Introduction to Genomic Technologies link | Python for Genomic Data Science link | . | Algorithms Specialization (Stanford, Coursera) Divide and Conquer, Sorting and Searching, and Randomized Algorithms link | . | .",
          "url": "https://kushajveersingh.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "About",
          "content": "github, twitter, linkedin, pytorch-forums, fastai-forums . I am a Software Engineer looking to land a new job after completing my Master’s in Dec 2022, and I am open to relocating to any place in the USA. Through my current role as a Full stack developer and experience in contributing to open-source projects, I can quickly pick up an existing codebase and toolchains to start being a productive member of the team. I also understand the pain points of various tech stacks and how this plays a role in a multi-team environment. And I have a strong background in algorithms and data structures. . My main interests are full stack development, machine learning specifically computer vision, and graph machine learning. Parallel to this I am well versed with software design patterns, and I like to spend time exploring how various open-source projects implement things and try to incorporate them into my daily coding tasks. . I have a good scientific aptitude and rigor, through my preparation for the national college entrance exams JEE Main and Advanced where I specifically focused on Math and Physics, allowing me to not fall into wrong conclusions and traps, which is especially important in Machine Learning where a small mistake can result in believing total falsehoods. This also helps me in debugging and creating quality tests for the project, covering all the edge cases. . Contributing to open-source projects including PyTorch, fastai, and being an active member of the online PyTorch and fastai forums have taught me important software engineering skills like debugging software to resolve edge cases, working with a large codebase that requires design pattern skills, and carefully testing out the changes before committing them to production. It has also taught me how to collaborate well with others, communicate both positive and negative results, and the importance of empathy. . You can reach me at kushajreal@gmail.com. .",
          "url": "https://kushajveersingh.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushajveersingh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}