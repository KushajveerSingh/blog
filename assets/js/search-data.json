{
  
    
        "post0": {
            "title": "STOC 2020: Session Notes",
            "content": "Session 3A - Walking Randomly, Massively, and Efficiently . by Jakub Lacki, Slobodan Mitrovic, Krzysztofof Onak, Piotr Sankowski video link . Overview of work . How to compute Random Walks? (in &quot;a small&quot; number of parallel steps) . Random Walks in Undirected Graphs . To generate a walk of length L from vertex v to x, we can follow the below procedure . Computer a random walk of length L/2 from $v rightarrow w$ and random walk of length L/2 from $w rightarrow x$. | Stitch the two above random walks to get the random walk of length L from $v rightarrow x$. | We can repeat the above procedure recursively by computing random walks of length L/2, L/2, ... . Now the problem is how many walks will pass through w (it follows stationary distribution). . Conclusion . Compute independent random walk of length L from each vertex . in poly log L steps undirected: $O( log L)$ rounds | directed: $O( log^2 log n+ log^21/ epsilon)$ | . | using $O(m^{1+o(1)})$ memory | . Session 7C - Does Learning Require Memorization? A Short Tale about a Long Tail . by Vitaly Feldman video link . Overview of problem . For state-of-the-art deep learning algorithms on text/images, we see this pattern . Training set error: 0-1% | (Typical) test set error: 10-30% | . This means the data distribution has a large number of points that the data distribution could not classify accurately. These misclassified points are generally outliers and misclassified labels. This same thing is true for training dataset also, which means the model is memorizing the labels for some inputs, otherwise it would not be able to achieve smaller training error rates. . An example was shown where Inception model was trained on random Imagenet labels, and yet it achieved 9% training error. Which means the model was memorizing the labels, as it is not possible to learn from random labels. . Defining label memorization . $mem(A,S,i)$ where . A = learning algorithm | S = dataset | i = $i^{th}$ example in dataset | . Memorization is defined as the difference between output of softmax of the model, first when i is part of training set and second when i is part of test set i.e. $$mem(A,S,i)=Pr(i in train)-Pr(i in test)$$ . We say an example is memorized if the above value is greater than some threshold (say 0.5). . . Tip: Memorization can be thought of as the difference between training and test error (i.e. generalization gap). If this value is large, we can say the model memorized more. . Why memorization is important? . . The four pictures in the training set are not much useful for learning what is a truck. Because in real life we would not see trucks like these. But if we memorize the first example, we get better result for the corresponding test image shown and same for the third image. . So memorization is essential in this case to perform better on the test set. But the problem is our model memorizes all the four images in the training set (some of which don&#39;t even benefit on the test set). And this is the reason why we see a difference in the training set and test set error rates. The model is memorizing useless examples. . . Note: In the above example it is assumed that test set is the real representation of real life i.e. we are not arguing that images in test set are not even real life images of truck. This is a separate problem of not making good test sets . So memorization is useful in some cases and in worst case is bad. . Conclusion . Label memorization is necessary for optimal generalization on long-tailed data distributions (not algorithm-specific). . Session 7C - Efficiently Learning Structured Distributions from Unstructured Batches . by Sitan Chen, Jerry Li, Ankur Moitra video link . Overview of problem . Inspired by Robust Learning. Can we design algorithms that can tolerate a constant fraction of corruptions in the data? These corruptions arise in . adversarial examples | data poisoning attacks on recommender systems | malware classifiers | . For this session, we deal with the problem where the data came from some crowdsource fashion. Like spell check app for mobile. We want to learn the distribution over misspellings of particular word. (This distribution is a discrete probability distribution over some words). . We have a central server where we collect the data from multiple users and aggregate the data to train our model. Now what if a constant fraction of users give adversarially chosen samples, to skew the model. We cannot distinguish between these adversarial and non-adversarial users. . We can only assume that as the number of batches increase for every user (a user will be sending multiple words to the server), the added redundancy will allow you to drive this error smaller and smaller. . In practical usage every user would have access to a some subset of the main distribution (if a person is interested in music, the words entered by him would resemble close to music, which will be different from a deep learning researcher). So our model should be able to tolerate these deviations from user to user (Federated Learning). . Session 8B - How to lose at Monte Carlo: a simple dynamical system . by C.Rojas, M.Yampolsky video link . In real life, all of the systems are non-deterministic because even the simplest model exhibit chaotic behavior (weather prediction is an example of this). Even the smallest errors in the calculation will blow up very fast so deterministic predictions are not possible. . Monte Carlo method(Non-deterministic approach) . Throw random darts to select a large number of initial values | Run the simulation for the desired duration for each of them; then statistically average the outcomes. | These averages are expected to reflect the true trajectory of the system. |",
            "url": "https://kushajveersingh.github.io/blog/notes/conference/2020/06/19/post-009.html",
            "relUrl": "/notes/conference/2020/06/19/post-009.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "",
            "url": "https://kushajveersingh.github.io/blog/2020/06/17/Untitled.html",
            "relUrl": "/2020/06/17/Untitled.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "SPADE: State of the art in Image-to-Image Translation by Nvidia",
            "content": "Link to implementation code, paper . To give some motivation for this paper, see the demo released by Nvidia. . . What is Semantic Image Synthesis? . It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class. . | | . Figure 1. Caption | . hiHias .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/17/post-004.html",
            "relUrl": "/paper-implementation/2019/04/17/post-004.html",
            "date": " • Apr 17, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Weight Standardization: A new normalization in town",
            "content": "Link to jupyter notebook, paper . Recently a new normalization technique is proposed not for the activations but for the weights themselves in the paper Weight Standardization. . In short, to get new state of the art results, they combined Batch Normalization and Weight Standardization. So in this post, I discuss what is weight standardization and how it helps in the training process, and I will show my own experiments on CIFAR-10 which you can also follow along. . . For my experiments, I will use cyclic learning. As the paper discusses training with constant learning rates, I would use cyclic LR as presented by Leslie N. Smith in his report. . To make things cleaner I would use this notation:- . BN -&gt; Batch Normalization | GN -&gt; Group Normalization | WS -&gt; Weight Standardization | . What is wrong with BN and GB? . Ideally, nothing is wrong with them. But to get the most benefit out of BN we have to use a large batch size. And when we have smaller batch sizes we prefer to use GN. (By smaller I mean 1–2 images/GPU). . Why is this so? . To understand it we have to see how BN works. To make things simple, consider we have only one-channel on which we want to apply BN and we have 2 images as our batch size. . Now we would compute the mean and variance using the 2 images and then normalize the one-channel of the 2 images. So we used 2 images to compute mean and variance. This is the problem. . By increasing batch size, we are able to sample the value of mean and variance from a larger population, which means that the computed mean and variance would be closer to their real values. . GN was introduced for cases of small batch sizes but it was not able to meet the results that BN was able to achieve using larger batch sizes. . How these normalization actually help? . It is one of the leading areas of research. But it was recently shown in the paper Fixup Initialization: Residual Learning without Normalization the reason for the performance gains using BN. . In short, it helps make the loss surface smooth. . . When we make the loss surface smooth we can take longer steps, which means we can increase our learning rate. So using Batch Norm actually stabilizes our training and also makes it faster. . Weight Standardization . Unlike BN and GN that we apply on activations i.e the output of the conv layer, we apply Weight Standardization on the weights of the conv layer itself. So we are applying WS to the kernels that our conv layer uses. . How does this help? . For the theoretical justification see the original paper where they prove WS reduces the Lipschitz constants of the loss and the gradients. . But there are easier ways to understand it. . First, consider the optimizer we use. The role of the optimizer is to optimize the weights of our model, but when we apply normalization layers like BN, we do not normalize our weights, but instead, we normalize the activations which are optimizer does not even care about. . By using WS we are essentially normalizing the gradients during the backpropagation. . The authors of the paper tested WS on various computer vision tasks and they were able to achieve better results with the combination of WS+GN and WS+BN. The tasks that they tested on included: . Image Classification | Object Detection | Video Recognition | Semantic Segmentation | Point Cloud Classification | Enough talk, let&#39;s go to experiments . The code is available in the notebook. . How to implement WS? . # collapse-show class Conv2d(nn.Module): def __init__(self, in_chan, out_chan, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True): super().__init__(in_chan, out_chan, kernel_size, stride, padding, dilation, groups, bias) def forward(self, x): weight = self.weight weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True) weight = weight - weight_mean std = weight.view(weight.size(0), -1).std(dim=1).view(-1,1,1,1)+1e-5 weight = weight / std.expand_as(weight) return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups) . . First, let&#39;s try out at batch size = 64 . This will provide a baseline of what we should expect. For this, I create 2 resnet18 models: . resnet18 $ rightarrow$ It uses the nn.Conv2d layers | resnet18_ws $ rightarrow$ It uses above Conv2d layer which uses weight standardization | I change the head of resnet model, as CIFAR images are already 32 in size and I don’t want to half their size initially. The code can be found in the notebook. And for the CIFAR dataset, I use the official train and valid split. . First I plot the value of loss v/s learning rate. . . For those not familiar with loss v/s learning_rate graph. We are looking for the maximum value of lr at which the loss value starts increasing. . In this case the max_lr is around 0.0005. So let’s try to train model for some steps and see. In case you wonder in the second case the graph is flatter around 1e-2, it is because the scale of the two graphs is different. . So now let’s train our model and see what happens. I am using the fit_one_cycle to train my model. . . There is not much difference between the two as valid loss almost remains the same. . Try at bs=2 . Now I take a batch size of 2 and train the models in a similar manner. . . One thing that I should add here, is the loss diverged quickly when I used only BN, after around 40 iterations, while in the case of WS+BN the loss did not diverge. . . There is not much difference in the loss values, but the time to run each cycle increased very much. . Trying bs=256 . Also, I run some more experiments where I used a batch size of 256. Although, I could use a larger learning rate but the time taken to complete the cycle increased. The results are shown below. . . . Again, in the graph we see we can use a larger learning rate. . Conclusion . From the above experiments, I think I would prefer not to use Weight Standardization when I am using cyclic learning. For large batch sizes, it even gave worse performance and for smaller batch sizes, it gave almost similar results, but using weight standardization we added a lot of time to our computation, which we could have used to train our model with Batch Norm alone. . For constant learning rate, I think weight standardization still makes sense as there we do not change our learning rate in the training process, so we must benefit from the smoother loss function. But in the case of cyclic learning, it does not offer us a benefit. .",
            "url": "https://kushajveersingh.github.io/blog/paper-implementation/2019/04/11/post-003.html",
            "relUrl": "/paper-implementation/2019/04/11/post-003.html",
            "date": " • Apr 11, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch",
            "content": "Link to jupyter notebook . Data . To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work. . . Note: - I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do. . Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use ImageNet Object Localization Challenge to directly download all the files (warning 155GB). | Unzip the tar.gz file using tar xzvf file_name -C destination_path. | In the Data/CLS-LOC folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders. | Use this command from your terminal in the val folder wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash. It would move all the images to their respective class folders. | As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^&gt;” {}. | After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???. . . Data Loading . Steps involved:- . Create a dataset class or use a predefined class | Choose what transforms you want to perform on the data. | Create data loaders | . # collapse-show train_dir = &#39;../../../Data/ILSVRC2012/train&#39; val_dir = &#39;../../../Data/ILSVRC2012/val&#39; size = 224 batch_size = 32 num_workers = 8 data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]), &#39;val&#39;: transforms.Compose([ transforms.CenterCrop(size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) } image_datasets = { &#39;train&#39;: ImageFolder(train_dir, transform=data_transforms[&#39;train&#39;]), &#39;val&#39;: ImageFolder(val_dir, transform=data_transforms[&#39;val&#39;]), } data_loader = { x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in [&#39;train&#39;, &#39;val&#39;] } . . The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step. . Check dataloaders . After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images. . # collapse-show # As our images are normalized we have to denormalize them and # convert them to numpy arrays. def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std*img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) #Pause is necessary to display images correctly images, labels = next(iter(data_loader[&#39;train&#39;])) grid_img = make_grid(images[:4], nrow=4) imshow(grid_img, title = [labels_list[x] for x in labels[:4]]) . . One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file LOC_synset_mapping.txt. . # collapse-show f = open(&quot;../../Data/LOC_synset_mapping.txt&quot;, &quot;r&quot;) labels_dict = {} # Get class label by folder name labels_list = [] # Get class label by indexing for line in f: split = line.split(&#39; &#39;, maxsplit=1) split[1] = split[1][:-1] label_id, label = split[0], split[1] labels_dict[label_id] = label labels_list.append(split[1]) . . Model Construction . Create your model. Pretrained models covered at the end of the post. . . Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’. . # collapse-show class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.conv_base = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False), nn.BatchNorm2d(96), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc_base = nn.Sequential( nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.conv_base(x) x = x.view(x.size(0), 256*6*6) x = self.fc_base(x) return x . . See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization). . Best practices for CNN . Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always. | Weight Initialization:- Use He initialization as default with ReLU. PyTorch provides kaimingnormal for this purpose. | Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. | Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model. | Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good. | Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference. | Weight Initialization . Do not use this method as a default. After, naming the layers you can do this very easily. . # collapse-show conv_list = [0, 4, 8, 10, 12] fc_list = [1, 4, 6] for i in conv_list: torch.nn.init.kaiming_normal_(model.conv_base[i].weight) for i in fc_list: torch.nn.init.kaiming_normal_(model.fc_base[i].weight) . . Create optimizers, schedulers and loss functions. . # collapse-show device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) # Cross entropy loss takes the logits directly, so we don&#39;t need to apply softmax in our CNN criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;max&#39;, verbose=True) . . PyTorch specific discussion . You have to specify the padding yourself. Check this thread for discussion on this topic. | Create the optimizer after moving the model to GPU. | The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself. | Do not forget to zero the grads. | . How to check my model is correct? . Check 1 . The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input. . inputs, labels = next(iter(data_loader[&#39;train&#39;])) inputs = inputs.to(device) labels = labels.to(device) criterion_check1 = nn.CrossEntropyLoss() optimizer_check1 = optim.SGD(model.parameters(), lr=0.001) model.train() for epoch in range(200): optimizer_check1.zero_grad() outputs = model(inputs) loss = criterion_check1(outputs, labels) _, preds = torch.max(outputs, 1) loss.backward() optimizer_check1.step() if epoch%10 == 0: print(&#39;Epoch {}: Loss = {} Accuracy = {}&#39;.format(epoch+1, loss.item(), torch.sum(preds == labels))) . . . Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again. . Check 2 . Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems. . In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs. . Using Pretrained Models . As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset. . . Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended. . # collapse-show data = np.load(&#39;../../../Data/cifar_10.npz&#39;) alexnet = torch.load(&#39;../../../Data/Pytorch Trained Models/alexnet.pth&#39;) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) x_train = torch.from_numpy(data[&#39;train_data&#39;]) y_train = torch.from_numpy(data[&#39;train_labels&#39;]) x_test = torch.from_numpy(data[&#39;test_data&#39;]) y_test = torch.from_numpy(data[&#39;test_labels&#39;]) . . # collapse-show # Create data loader class CIFAR_Dataset(torch.utils.data.Dataset): &quot;&quot;&quot; Generally you would not load images in the __init__ as it forces the images to load into memory. Instead you should load the images in getitem function, but as CIFAR is small dataset I load all the images in memory. &quot;&quot;&quot; def __init__(self, x, y, transform=None): self.x = x self.y = y self.transform = transform def __len__(self): return self.x.size(0) def __getitem__(self, idx): image = self.x[idx] label = self.y[idx].item() if self.transform: image = self.transform(image) return (image, label) data_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) datasets = { &#39;train&#39;: CIFAR_Dataset(x_train, y_train, transform=data_transforms), &#39;test&#39;: CIFAR_Dataset(x_test, y_test, transform=data_transforms) } data_loader = { x: torch.utils.data.DataLoader(datasets[x], batch_size=64, shuffle=True, num_workers=8) for x in [&#39;train&#39;, &#39;test&#39;] } . . # collapse-show # Freeze conv layers for param in alexnet.parameters(): param.requires_grad = False # Initialize the last layer of alexnet model for out 10 class dataset alexnet.classifier[6] = nn.Linear(4096, 10) alexnet = alexnet.to(device) criterion = nn.CrossEntropyLoss() # Create list of params to learn params_to_learn = [] for name,param in alexnet.named_parameters(): if param.requires_grad == True: params_to_learn.append(param) optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True) . . Refer to this script on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from torchvision.datasets. . PyTorch has a very good tutorial on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook. . Conclusion . We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images. . References . https://github.com/soumith/imagenet-multiGPU.torch Helped in preprocessing of the Imagenet dataset. | https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html Many code references were taken from this tutorial. | https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf AlexNet paper |",
            "url": "https://kushajveersingh.github.io/blog/tutorial/2019/02/18/post-002.html",
            "relUrl": "/tutorial/2019/02/18/post-002.html",
            "date": " • Feb 18, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Theoretical Machine Learning: Probabilistic and Statistical Math",
            "content": "I will start with a quick overview of probability and then dive into details of Gaussian distribution. In the end I have provided links to some of the theoretical books that I have read for the concepts mentioned here. . . Why is there a need for uncertainty (probability) in machine learning (ML)? Generally when we are given a dataset and we fit a model on that data what we want to do actually is capture the property of dataset i.e. the underlying regularity in order to generalize well to unseen data. But the individual observations are corrupted by random noise (due to sources of variability that are themselves unobserved). This is what is termed as Polynomial Curve Fitting in ML. . In curve fitting, we usually use the maximum likelihood approach which suffers from the problem of overfitting (as we can easily increase the complexity of the model to learn the train data). . To overcome overfitting we use regularization techniques by penalizing the parameters. We do not penalize the bias term as it’s inclusion in the regularization causes the result to depend on the choice of origin. . Quick Probability Overview . Consider an event of tossing a coin. We use events to describe various possible states in our universe. So we represent the possible states as X = Event that heads come and Y = Event that tails come. Now P(X) = Probability of that particular event happening. . Takeaway, represent your probabilities as events of something happening. . In the case of ML, we have P(X = x) which means, the probability of observing the value x for our variable X. Note I changed from using the word event to a variable. . Next, we discuss expectation. One of the most important concepts. When we say E[X] (expectation of X ) we are saying, that we want to find the average of the variable X. As a quick test, what is the E[x] where x is some observed value? . Suppose x takes the value 10, the average of that value is the number itself. . . Next, we discuss variance. Probably the most frustrating part of an ML project when we have to deal with large variances. Suppose we found the expectation (which we generally call mean) of some values. Then variance tells us the average distance of each value from the mean i.e. it tells how spread a distribution is. . Another important concept in probabilistic maths is the concept of prior, posterior and likelihood. . . Prior = P(X). It is probability available before we observing an event . Posterior = P(X|Y). It is the probability of X after event Y has happened. . Likelihood = P(Y|X). It tells how probable it is for the event Y to happen given the current settings i.e. X . Now when we use maximum likelihood we are adjusting the values of X to get to maximize the likelihood function P(Y|X). . . Tip: Follow the resource list given at the end for further reading of the topic. . Gaussian Distribution . As it is by far the most commonly used distribution used in the ML literature I use it as a base case and discuss various concepts using this distribution. . . But before that let us discuss why we need to know about probability distributions in the first place? . When we are given a dataset and we want to make predictions about new data that has not been observed then what we essentially want is a formula in which we pass the input value and get the output values as output. . Let’s see how we get to that formula. Initially, we are given input data (X). Now, this data would also come from a formula, which I name probability distribution (the original distribution is although corrupted from random noises). So we set a prior for the input data. And this prior comes in the form of a probability distribution (Gaussian in most cases). . . Note: From a purely theoretical perspective we are simply following Bayesian statistics and filling in values to the Baye’s Rule. . As a quick test, if we already know a probability distribution for the input data then why we need to make complex ML models? . After assuming an input distribution we then need to assume some complexity over the model that we want to fit on the input data. Why do we want to do this? Simply because there are infinitely many figures that we draw that would pass through the given two points. It is up to us to decide whether the figure is linear, polynomial. . Now in any ML model, we have weights that we have to finetune. We can either assume constant values for these weights or we can go even a bit deeper and assume a prior for these weights also. More on this later. . . Note: In this post, I am approaching ML from a statistics point of view and not the practical way of using backpropagation to finetune the values of the weights. . Now I present a general way of approaching the problem of finding the values for the variables of a prior. . The Gaussian Equation is represented as follow . $$ mathcal{N}(x| mu, sigma^2) = frac{1}{(2 pi sigma^2)^{1/2}}exp {- frac{1}{2 sigma^2}(x- mu)^2 }$$ . Now when we assume a Gaussian prior for input data we have to deal with two variables namely the mean and variance of the distribution and it is a common hurdle when you assume some other distribution. . So how to get the value of these variables. This is where maximum likelihood comes into play. Say you observed N values as input. Now all these N values are i.i.d. (independently identically distributed), so the combined joint probability (likelihood function) can be represented as . $$p(x| mu, sigma^2)= prod_{n=1}^{N} mathcal{N}(x_n| mu, sigma^2)$$ . After getting the likelihood function, we now want to maximize this function with respect to the variables one by one. To make our life easier we usually take the log of the above value as log is a monotonically increasing function. . $$ ln p(x| mu, sigma^2)=- frac{1}{2 sigma^2} sum_{n=1}^{N}(x_n- mu)^2- frac{N}{2} ln sigma^2- frac{N}{2} ln (2 pi)$$ . Now take the derivative w.r.t the variables and get their values. . $$ mu_{ML}= frac{1}{N} prod_{n=1}^{N}x_n$$ $$ sigma^2_{ML}= frac{1}{N} prod_{n=1}{N}(x_n- mu_{ML})^2$$ . . Note: In a fully Bayesian setting the above two values represent the prior for that variables and we can update them as we observe new data. . Why was all this important? . All of you may have heard about the MSE (mean squared error) loss function, but you may not be able to use that loss function for every situation as that loss function is derived after assuming the Gaussian prior on the input data. Similarly, other loss functions like Softmax are also derived for that particular prior. . In cases where you have to take a prior like Poisson MSE would not be a good metric to consider. . Set Prior on the variables also . . Another design choice that you can make is by assuming that the variable values also follow a probability distribution. How to choose that distribution? . Ideally, you can choose any distribution. But practically you want to choose a conjugate prior for the variables. Suppose your prior for the input data is Gaussian. And now you want to select a prior for the mean. You must choose a prior such that after applying the Baye’s rule the resulting distribution for the input data is still Gaussian and same for variance also. These are called conjugate priors. . Just for a reference, conjugate prior for the mean is also Gaussian and for the variance and inverse gamma for the variance. . Congratulations if you made it to the end of the post. I rarely scratched the surface but I tried to present the material in a more interactive manner focused more on building intuition. . Here is the list of resources that I would highly recommend for learning ML:- . CS229: Machine Learning by Stanford | Pattern Recognition and Machine Learning by Christopher M. Bishop | An Introduction to Statistical Learning: with Applications in R | The Elements of Statistical Learning Data Mining, inference and Prediction |",
            "url": "https://kushajveersingh.github.io/blog/machine-learning/2018/09/14/post-001.html",
            "relUrl": "/machine-learning/2018/09/14/post-001.html",
            "date": " • Sep 14, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Check README for a summary of all my work with relevant links. Resume. I am active on fastai forums and pytorch forums. . Deep Learning Researcher with interest in Computer Vision, Natural Language Processing and Reinforcement Learning. Have worked with GANs, style transfer, image-to-image translation, colorizing segmentation maps, object detection, classification. Currently trying to expand my knowledge in the field of Computer Vision by exploring new topics in the field and experimenting with the stuff. A fan of fastai courses and an active member of fastai community. . Completed B.Tech. in Electronics and Communication Engineering from Punjab Engineering College, Chandigarh, India (2016-2020). Won Techgig Code Gladiators competition in Artificial Intelligence theme. I was also part of Skill India program where I participated in IT Software Solutions for Business in 2018 and made it to the North-Zone Nationals. . I am a national level handball player having played handball for five years from class 8 to 12. . You can reach me at kushajreal@gmail.com. .",
          "url": "https://kushajveersingh.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Notebooks",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Summary of few of my jupyter notebooks . Mish activation function is tested for transfer learning. Here mish is used only in the last fully-connected layers of a pretrainened Resnet50 model. I test the activation function of CIFAR10, CIFAR100 using three different learning rate values. I found that Mish gave better results than ReLU. notebook, paper . | Multi Sample Dropout is implemented and tested on CIFAR-100 using cyclic learning. My losses converged 4x faster when using num_samples=8 than using simple dropout. notebook, paper . | Data Augmentation in Computer Vision Notebook implementing single image data augmentation techniques using just Python notebook | . | Summarizing Leslie N. Smith’s research in cyclic learning and hyper-parameter setting techniques. notebook A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay paper | Super-Convergence: Very Fast Training of Neural Networks Using Learning Rates paper | Exploring loss function topology with cyclical learning rates paper | Cyclical Learning Rates for Training Neural Networks paper | . | Photorealisitc Style Transfer. Implementation of the High-Resolution Network for Photorealistic Style Transfer paper. notebook, paper . | Weight Standardization is implemented and tested using cyclic learning. I find that it does not work well with cyclic learning when using CIFAR-10. notebook, paper . | Learning Rate Finder. Implementation of learning rate finder as introduced in the paper Cyclical Learning Rates for Training Neural Networks. A general template for custom models is provided. notebook . | PyTorch computer vision tutorial. AlexNet with tips and checks on how to train CNNs. The following things are included: notebook Dataloader creation | Plotting dataloader results | Weight Initialization | Simple training loop | Overfitting a mini-batch | . | How to deal with outliers . | How to choose number of bins in a histogram | .",
          "url": "https://kushajveersingh.github.io/blog/notebooks/",
          "relUrl": "/notebooks/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "I use Pytorch and fastai as my main deep learning libraries. Check this repo for more details. . Unsupervised Parking Lot Detection . A complete approach to detect parking lot spaces from images and then tell which spaces are occupied or not. Here I do not use any dataset for training my model to detect parking spaces. My implementation can be divided into these three modules: . Object Detection Module :- Use COCO pretrained model, no need to do finetuning. | Label Processing Module :- As out model is not finetuned, there are some tricks that I add to overcome these limitations | Classification Module :- Use the processed labels/bounding_boxes to tell if that parking space is occupied or not. | . | SPADE by Nvidia . Unofficial implementation of SPDAE for image-to-translation from segmentation maps to the colored pictures. Due to compute limit I test it out for a simplified model on Cityscapes dataset and get descent results after 80 epochs with batch_size=2. . | Waste Seggregation using trashnet . Contains the code to train models for trashnet and then export them using ONNX. It was part of a bigger project where we ran these models on Rasberry Pi, which controlled wooden planks to classify the waste into different categories (code for rasberry pi not included here). . | Unscramble game . Python script to solve the unscramble android game. You are given 5 random letters and you have to find 3-letter, 4-letter, 5-letter english words from these 5 random letters. It is a simple brute force method with a english dictionary lookup. . | Random Duty List . A PHP and MySQL based work where the aim is to assign duties from a list to various stations and make sure the duties are not repeated and the repetition occurs only after the list is exhasuted. . | .",
          "url": "https://kushajveersingh.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushajveersingh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}