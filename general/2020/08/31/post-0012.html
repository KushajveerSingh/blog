<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ImageNet Dataset Advancements | Kushajveer Singh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="ImageNet Dataset Advancements" />
<meta name="author" content="Kushajveer Singh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A short summary of Imagenet-1k, ImageNetv2-MatchedFrequency, ImageNet-Sketch, ImageNet-A, ImageNet-O, ImageNet-C, ImageNet-P, ImageNet-“Real Labels”, ImageNet-R" />
<meta property="og:description" content="A short summary of Imagenet-1k, ImageNetv2-MatchedFrequency, ImageNet-Sketch, ImageNet-A, ImageNet-O, ImageNet-C, ImageNet-P, ImageNet-“Real Labels”, ImageNet-R" />
<link rel="canonical" href="https://kushajveersingh.github.io/blog/general/2020/08/31/post-0012.html" />
<meta property="og:url" content="https://kushajveersingh.github.io/blog/general/2020/08/31/post-0012.html" />
<meta property="og:site_name" content="Kushajveer Singh" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-31T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A short summary of Imagenet-1k, ImageNetv2-MatchedFrequency, ImageNet-Sketch, ImageNet-A, ImageNet-O, ImageNet-C, ImageNet-P, ImageNet-“Real Labels”, ImageNet-R","url":"https://kushajveersingh.github.io/blog/general/2020/08/31/post-0012.html","@type":"BlogPosting","headline":"ImageNet Dataset Advancements","dateModified":"2020-08-31T00:00:00-05:00","datePublished":"2020-08-31T00:00:00-05:00","author":{"@type":"Person","name":"Kushajveer Singh"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kushajveersingh.github.io/blog/general/2020/08/31/post-0012.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kushajveersingh.github.io/blog/feed.xml" title="Kushajveer Singh" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123402359-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123402359-3');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Kushajveer Singh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">ImageNet Dataset Advancements</h1><p class="page-description">A short summary of Imagenet-1k, ImageNetv2-MatchedFrequency, ImageNet-Sketch, ImageNet-A, ImageNet-O, ImageNet-C, ImageNet-P, ImageNet-"Real Labels", ImageNet-R</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-31T00:00:00-05:00" itemprop="datePublished">
        Aug 31, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Kushajveer Singh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#general">general</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#ImageNet-database">ImageNet database </a></li>
<li class="toc-entry toc-h2"><a href="#ImageNet-1k-dataset">ImageNet-1k dataset </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Dataset-creation-process">Dataset creation process </a></li>
<li class="toc-entry toc-h3"><a href="#Problems-with-ImageNet">Problems with ImageNet </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ImageNetv2-Matched-Frequency">ImageNetv2 Matched Frequency </a></li>
<li class="toc-entry toc-h2"><a href="#ImageNet-Sketch">ImageNet-Sketch </a></li>
<li class="toc-entry toc-h2"><a href="#ImageNet-A-/-ImageNet-O">ImageNet-A / ImageNet-O </a>
<ul>
<li class="toc-entry toc-h3"><a href="#How-ImageNet-A-is-constructed?">How ImageNet-A is constructed? </a></li>
<li class="toc-entry toc-h3"><a href="#How-ImageNet-O-is-constructed?">How ImageNet-O is constructed? </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ImageNet-C-/-ImageNet-P">ImageNet-C / ImageNet-P </a>
<ul>
<li class="toc-entry toc-h3"><a href="#How-ImageNet-C-is-constructed?">How ImageNet-C is constructed? </a></li>
<li class="toc-entry toc-h3"><a href="#How-ImageNet-P-is-constructed?">How ImageNet-P is constructed? </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ImageNet-"Real-Labels"">ImageNet-&quot;Real Labels&quot; </a></li>
<li class="toc-entry toc-h2"><a href="#ImageNet-Rendition">ImageNet-Rendition </a>
<ul>
<li class="toc-entry toc-h3"><a href="#DeepAugment">DeepAugment </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#What-to-use?">What to use? </a></li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-31-post-0012.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-database">
<a class="anchor" href="#ImageNet-database" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet database<a class="anchor-link" href="#ImageNet-database"> </a>
</h2>
<ul>
<li>Source: <a href="http://www.image-net.org/">http://www.image-net.org/</a>
</li>
<li>Paper: "ImageNet: A large-scale hierarchical image database" <a href="https://ieeexplore.ieee.org/document/5206848">https://ieeexplore.ieee.org/document/5206848</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Quoting from <a href="https://arxiv.org/abs/1409.0575">ILSVRC paper</a></p>
<blockquote>
<p>ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014).</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-1k-dataset">
<a class="anchor" href="#ImageNet-1k-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-1k dataset<a class="anchor-link" href="#ImageNet-1k-dataset"> </a>
</h2>
<ul>
<li>Source: <a href="http://image-net.org/challenges/LSVRC/2012/index">http://image-net.org/challenges/LSVRC/2012/index</a>
</li>
<li>Paper: "ImageNet Large Scale Visual Recognition Challenge" <a href="https://arxiv.org/abs/1409.0575">https://arxiv.org/abs/1409.0575</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv">results-imagenet.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>The standard 50,000 image ImageNet-1k dataset.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <strong>ILSVRC</strong> (ImageNet Large Scale Visual Recognition Challenge) ran from 2010-2017. This challenge provided the teams with a subset of ImageNet database, called ILSVRC-2012 or ImageNet-1k or ImageNet (I think ILSVRC-2012 is the correct name, but people also refer to this dataset by the later two names).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataset-creation-process">
<a class="anchor" href="#Dataset-creation-process" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset creation process<a class="anchor-link" href="#Dataset-creation-process"> </a>
</h3>
<ol>
<li>
<p><strong>Selecting categories</strong>:- The 1000 categories were manually (based on heuristics related to WordNet hierarchy). Also, to include fine-grained classification in the dataset the authors included 120 categories of dog breeds (this is why ImageNet models generally dream about dogs).</p>
</li>
<li>
<p><strong>Selecting candidate images</strong>:- Taken directly from ImageNet database. They basically did search queries for each category (synset) on several image search engines. The queries were also translated to Chinese, Spanish, Dutch and Italian to increase the diversity of the images.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>Step 2 introduces the problem of inaccurate annotations because we don’t know whether the search engines are correct or not.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>
<strong>Annotating images</strong>:- Amazon Mechanical Turk (AMT) was used to label the images. Each user was a given a set of candidate images and the definition of the target category (synset). The users were then asked to verify if the image contained the category. There was also a quality control system setup which you can read in the paper. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Problems-with-ImageNet">
<a class="anchor" href="#Problems-with-ImageNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems with ImageNet<a class="anchor-link" href="#Problems-with-ImageNet"> </a>
</h3>
<p><strong>Are we done with ImageNet?</strong> goes into the details. But the main problem is the classification task. There are a lot of images in the dataset which have multiple classes or classes with multiple meanings. This is shown in the figure below taken from the above paper.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_0012/00.jpg" alt="" title="Figure 1. Validation images in the original ImageNet dataset with labeling errors. Image taken from *Are we done with ImageNet?*"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the figure, we can see that multi-label classification would be a better option to train on the dataset. Deep learning models are generally considered to be robust to some of the noise, so maybe that is why we can still train using the classification and get the awesome results.</p>
<p>One thing that the above image shows is that <strong>blindly following the top-1 accuracy on validation dataset is not a good idea</strong>. As at that point a model is simply learning to overfit or learn which class to predict for the validation images.</p>
<p>We cannot do anything about the training dataset. As collecting new labels for the images would be a big project on its own, but we can try to test if the model really generalized or not by coming up with newer validation datasets. For some datasets we would prioritize robustness, as generalization also means that a model should be robust to unseen changes in the training dataset.</p>
<p>Another problem is we do not have access to ImageNet test data. This means people have to resort to validation results to infer which model works better (in terms of accuracy). The main problem here is the <strong>extensive hyperparameter tuning on the validation set</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNetv2-Matched-Frequency">
<a class="anchor" href="#ImageNetv2-Matched-Frequency" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNetv2 Matched Frequency<a class="anchor-link" href="#ImageNetv2-Matched-Frequency"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/modestyachts/ImageNetV2">https://github.com/modestyachts/ImageNetV2</a>
</li>
<li>Paper: "Do ImageNet Classifiers Generalize to ImageNet?" - <a href="https://arxiv.org/abs/1902.10811">https://arxiv.org/abs/1902.10811</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenetv2-matched-frequency.csv">results-imagenetv2-matched-frequency.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>An ImageNet test set of 10,000 images sampled from new images. Care was taken to replicate the original ImageNet curation/sampling process.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the paper, the authors observed a drop of 11%-14% in accuracy for the models they tested. The main reason for this is extensive hyperparameter tuning on the validation set.</p>
<p>This paper solves this problem by collecting 10,000 new images (10 for each class) from Flickr. These images are much harder than the original ImageNet validation images. There are three versions of the dataset available which you can check on source link (the difference is in the method to select the 10 images for each class). <strong>MatchedFrequency dataset</strong> is used in the <em>rwightman</em> repo.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-Sketch">
<a class="anchor" href="#ImageNet-Sketch" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-Sketch<a class="anchor-link" href="#ImageNet-Sketch"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/HaohanWang/ImageNet-Sketch">https://github.com/HaohanWang/ImageNet-Sketch</a>
</li>
<li>Paper: "Learning Robust Global Representations by Penalizing Local Predictive Power" <a href="https://arxiv.org/abs/1905.13549">https://arxiv.org/abs/1905.13549</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-sketch.csv">results-sketch.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_0012/01.jpg" alt="" title="Figure 2. ImageNet-Sketch dataset example. Figure taken from *Learning Robust Global Representations by Penalizing Local Predictive Power*"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this dataset we penalize the predictive power of the networks by discarding predictive signals such as color and texture that can be obtained from local receptive fields and rely instead on the global structure of the image.</p>
<p>This dataset basically consists of black and white sketches, doodles of the 1000 classes. This dataset focuses on model robustness by defining robustness to generalize to structure of the categories (i.e. low-frequency signal).</p>
<p>I don't think high accuracy on this dataset should be the primary goal. The reasoning being the hardware has gone pretty strong and using RGB images is not that expensive, so I don't see any point as to why we should penalize our models by taking out color and texture to check the robustness of the model (as we will never have that input during inference). Instead we should find ways to check robustness in the original RGB domain (check the next datasets for this).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-A-/-ImageNet-O">
<a class="anchor" href="#ImageNet-A-/-ImageNet-O" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-A / ImageNet-O<a class="anchor-link" href="#ImageNet-A-/-ImageNet-O"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/hendrycks/natural-adv-examples">https://github.com/hendrycks/natural-adv-examples</a>
</li>
<li>Paper: "Natural Adversarial Examples" - <a href="https://arxiv.org/abs/1907.07174">https://arxiv.org/abs/1907.07174</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet-a.csv">results-imagenet-a.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>7500 images covering 200 of the 1000 classes. The images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. Typical ResNet-50 will score 0% top-1 accuracy on this dataset.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two datasets introduced in the paper with different purposes</p>
<ol>
<li>ImageNet-Adversarial (ImageNet-A): Contains 7500 images which are naturally adversarial (200 classes out of 1000 in ImageNet). Classifiers should be able to classify the images correctly.</li>
<li>ImageNet-Out-of-Distribution-Detection (ImageNet-O): Contains 2000 images with classes that are not in ImageNet-1k dataset (out-of-distribution). Classifiers should output low-confidence predictions on the images.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-ImageNet-A-is-constructed?">
<a class="anchor" href="#How-ImageNet-A-is-constructed?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How ImageNet-A is constructed?<a class="anchor-link" href="#How-ImageNet-A-is-constructed?"> </a>
</h3>
<p>First, a lot of images for the 200 classes of ImageNet were collected from the Internet. Then all the images correctly classified by ResNet-50 are removed from the dataset (reason for 0% top-1 acc using ResNet-50). Finally, a subset of high quality images are selected for the final dataset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-ImageNet-O-is-constructed?">
<a class="anchor" href="#How-ImageNet-O-is-constructed?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How ImageNet-O is constructed?<a class="anchor-link" href="#How-ImageNet-O-is-constructed?"> </a>
</h3>
<p>ImageNet database was used to get the images (excluding the 1000 classes in ImageNet-1k). Then ResNet-50 is used to select the images for which the model predicts high-confidence predictions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ImageNet-A and ImageNet-O are good datasets to check the robustness of the models. ImageNet-A can be tested automatically. In case of ImageNet-O we have to come up with our own evaluation strategy. Manually looking at the predictions is possible. Plotting a histogram of model confidence predictions is also a possibility to get a sense of the confidence values.</p>
<p>I want to test a new thing for image classifier models, where we use <code>sigmoid</code> instead of <code>softmax</code>. The problem with <code>softmax</code> is that it forces one prediction to a large value, where as with <code>sigmoid</code> all the predictions are independent of each other. This can also help counter the problem of multiple classes in images of ImageNet dataset. I still have to explore this method, on how to train the model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-C-/-ImageNet-P">
<a class="anchor" href="#ImageNet-C-/-ImageNet-P" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-C / ImageNet-P<a class="anchor-link" href="#ImageNet-C-/-ImageNet-P"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/hendrycks/robustness">https://github.com/hendrycks/robustness</a>
</li>
<li>Paper: "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations" <a href="https://arxiv.org/abs/1903.12261">https://arxiv.org/abs/1903.12261</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Evaluates performance on common corruptions and perturbations that may happen in real-life. 3.75 million images in ImageNet-C.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-ImageNet-C-is-constructed?">
<a class="anchor" href="#How-ImageNet-C-is-constructed?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How ImageNet-C is constructed?<a class="anchor-link" href="#How-ImageNet-C-is-constructed?"> </a>
</h3>
<p>ImageNet-C consists of 15 common corruptions (Gaussian Noise, Shot Noise, Impulse Noise, Defocus Blur, Frosted Glass Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog, Brightness, Contrast, Elastic, Pixelate, JPEG) with 5 levels of severity, resulting in 75 distinct corruptions for each image. These corruptions are applied to all 50,000 validation images of the original ImageNet-1k dataset, resulting in 3.75 million images.</p>
<p>This assumes that you did not use any of these corruptions as data augmentation during the training phase.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-ImageNet-P-is-constructed?">
<a class="anchor" href="#How-ImageNet-P-is-constructed?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How ImageNet-P is constructed?<a class="anchor-link" href="#How-ImageNet-P-is-constructed?"> </a>
</h3>
<p>Perturbations hear mean applying the same corruption successively on the previous applied image. This dataset measures classifier's prediction stability, reliability and consistency in case of minor change in input image.</p>
<p>For each image we generate 30 frames of perturbation (from 10 corruptions) 5 levels of severity resulting in total of 7.5 million images. Starting with a clean ImageNet image apply brighness corruption to it and then apply brightness corruption on the current image and keep doing it for 30 times.</p>
<p>In the paper, the authors also introduce a metric for ImageNet-P. Check the paper for its details.</p>
<p>The only problem with this dataset is size. The authors of the paper have also created a new dataset ImageNet-R which we can use instead.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id='ImageNet-"Real-Labels"'>
<a class="anchor" href="#ImageNet-" real-labels aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-"Real Labels"<a class="anchor-link" href="#ImageNet-%22Real-Labels%22"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/google-research/reassessed-imagenet">https://github.com/google-research/reassessed-imagenet</a>
</li>
<li>Paper: "Are we done with ImageNet?" - <a href="https://arxiv.org/abs/2006.07159">https://arxiv.org/abs/2006.07159</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet-real.csv">results-imagenet-real.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>New labels for the original ImageNet-1k intended to improve on mistakes in the original annotation process.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This dataset can be easily summarized with Figure 3.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/post_0012/00.jpg" alt="" title="Figure 3. Image taken from *Are we done with ImageNet?*"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This dataset provides new labels for the validation set of original ImageNet-1k dataset (50,000 images).</p>
<p>In the paper, the authors propose a new metric <em>Real accuracy</em> as we cannot use top-1 accuracy for this multi-label dataset. This metric measures the precision of the model's top-1 prediction, which is deemed correct if it is included in the set of labels, and incorrect otherwise.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ImageNet-Rendition">
<a class="anchor" href="#ImageNet-Rendition" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet-Rendition<a class="anchor-link" href="#ImageNet-Rendition"> </a>
</h2>
<ul>
<li>Source: <a href="https://github.com/hendrycks/imagenet-r">https://github.com/hendrycks/imagenet-r</a>
</li>
<li>Paper: "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization" - <a href="https://arxiv.org/abs/2006.16241">https://arxiv.org/abs/2006.16241</a>
</li>
<li>rwightman/pytorch-image-models: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet-r.csv">results-imagenet-r.csv</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Renditions of 200 ImageNet classes resulting in 30,000 images for testing robustness.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset consists of artistic renditions (art, cartoons, graffiti, embroidery, graphics, origami, paintings, patterns, sculptures and more) of object classes. These kind of images are not included in the original ImageNet dataset (it consists of photos of real objects only).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="DeepAugment">
<a class="anchor" href="#DeepAugment" aria-hidden="true"><span class="octicon octicon-link"></span></a>DeepAugment<a class="anchor-link" href="#DeepAugment"> </a>
</h3>
<p>This paper also introduces a new data augmentation technique called DeepAugment. Take any <em>image-to-image network</em> (like image autoencoder or a superresolution network) and pass the images through it. Now, distort the internal weights of the network (zeroing, negating, transposing, ...) and the images you get as output can be used for training.</p>
<p>I highly recommend reading this <a href="https://arxiv.org/abs/2006.16241">paper</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-to-use?">
<a class="anchor" href="#What-to-use?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What to use?<a class="anchor-link" href="#What-to-use?"> </a>
</h2>
<p>For the training phase, I would still use the original ImageNet-1k validation dataset. The top-1 accuracy on this dataset does not matter much due to incorrect labels. Next I would use <strong>ImageNetv2 Matched Frequency</strong> dataset as a proxy of test set. In the end, when everything is done I would get the results of <strong>ImageNet Real labels</strong> dataset to get a real sense of the accuracy of the model.</p>
<p>To test the robustness of the model, I would work with <strong>ImageNet-Adversarial and ImageNet-Rendition</strong> datasets. With ImageNet-A we can get the accuracy of the model for really hard images. With ImageNet-R we can get results for out-of-distribution images. If the results on these datasets are satisfactory <strong>ImageNet-O</strong> can also be used for further testing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h2>
<ul>
<li>rwightman/pytorch-image-models <a href="https://github.com/rwightman/pytorch-image-models">link</a>
</li>
<li>WordNet: a lexical database for English <a href="https://dl.acm.org/doi/10.1145/219717.219748">link</a>
</li>
<li>ImageNet: A large-scale hierarchical image database <a href="https://ieeexplore.ieee.org/document/5206848">link</a>
</li>
<li>ImageNet Large Scale Visual Recognition Challenge <a href="https://arxiv.org/abs/1409.0575">link</a>
</li>
<li>Learning Robust Global Representations by Penalizing Local Predictive Power <a href="https://arxiv.org/abs/1905.13549">link</a>
</li>
<li>Natural Adversarial Examples <a href="https://arxiv.org/abs/1907.07174">link</a>
</li>
<li>Benchmarking Neural Network Robustness to Common Corruptions and Perturbations <a href="https://arxiv.org/abs/1903.12261">link</a>
</li>
<li>Are we done with ImageNet? <a href="https://arxiv.org/abs/2006.07159">link</a>
</li>
<li>The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization <a href="https://arxiv.org/abs/2006.16241">link</a>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="KushajveerSingh/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/general/2020/08/31/post-0012.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/KushajveerSingh" title="KushajveerSingh"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Kkushaj" title="Kkushaj"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
