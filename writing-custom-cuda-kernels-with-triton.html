<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Writing custom CUDA kernels with Triton | Kushajveer Singh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Writing custom CUDA kernels with Triton" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning." />
<meta property="og:description" content="Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning." />
<link rel="canonical" href="https://kushajveersingh.github.io/blog/writing-custom-cuda-kernels-with-triton" />
<meta property="og:url" content="https://kushajveersingh.github.io/blog/writing-custom-cuda-kernels-with-triton" />
<meta property="og:site_name" content="Kushajveer Singh" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning.","url":"https://kushajveersingh.github.io/blog/writing-custom-cuda-kernels-with-triton","@type":"BlogPosting","headline":"Writing custom CUDA kernels with Triton","dateModified":"2021-09-12T00:00:00-05:00","datePublished":"2021-09-12T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kushajveersingh.github.io/blog/writing-custom-cuda-kernels-with-triton"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kushajveersingh.github.io/blog/feed.xml" title="Kushajveer Singh" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123402359-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123402359-3');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Kushajveer Singh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Writing custom CUDA kernels with Triton</h1><p class="page-description"></p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-12T00:00:00-05:00" itemprop="datePublished">
        Sep 12, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#general">general</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#gpu-programming-basics">GPU programming basics</a></li>
<li class="toc-entry toc-h2"><a href="#cpu-program-vs-cuda-program">CPU-program vs CUDA-program</a></li>
<li class="toc-entry toc-h2"><a href="#cuda-vs-triton">CUDA vs Triton</a></li>
<li class="toc-entry toc-h2"><a href="#adding-two-arrays-using-triton">Adding two arrays using Triton</a></li>
</ul><p>With the success of deep learning and explosion of research papers, it is common to find ourselves in a situation where we come up with a new idea just to find that it is not hardware accelerated. More specifically, when we come up with new activation function, or a self-attention mechanism and have to rely on the capabilities provided by PyTorch/Tensorflow to handle the forward and backward pass through the Module.</p>

<p>PyTorch JIT is one option in these cases. But PyTorch JIT is a high-level compiler which can only optimize parts of the code but it cannot be used to write custom CUDA kernels.</p>

<p>There is another problem with writing CUDA kernels. It is incredibly hard to do. Optimizing the computations for locality and parallelism is very time consuming and error prone and it often requires experts who have spent a lot of time learning how to write CUDA code. Also, GPU architectures are rapidly evolving, like the latest edition of tensor cores which means even a bigger challenge writing code that is utilizing the maximum performance out of the hardware.</p>

<p>This is where OpenAI <a href="https://github.com/openai/triton">Triton</a> comes into picture. Triton has three main components</p>

<p><img src="/blog/images/post_0015/01.png" alt="" title="Figure 1: Overview of main components of Triton."></p>

<ol>
  <li>Triton-C: A C-like language mainly intended for programmers already familiar with CUDA.</li>
  <li>Triton-IR: An LLVM-based Intermediate Representation (IR). Triton-IR programs are directly constructed from Triton-C. In short, LLVM provided a lot of hardware specific optimizations, which means we can directly use Nvidia’s CUDA Compiler (NVCC) to optimize our code specific to the particular hardware.</li>
  <li>Triton-JIT: A Just-In-Time (JIT) compiler and code generation backend for compiling Triton-IR programs into efficient LLVM bitcode. This also includes many machine-independent optimizations, which means less work for us.</li>
</ol>

<p>Triton-JIT is the most exciting part of the Triton project for me. It allows programmers with almost no experience with CUDA programming to write highly optimized CUDA kernels in Python. Before discussing Triton, we need to understand how CUDA programs work on GPU.</p>

<p>Useful links</p>
<ul>
  <li><a href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</a></li>
  <li><a href="https://openai.com/blog/triton/">Introducing Triton: Open-Source GPU Programming for Neural Networks</a></li>
  <li><a href="https://github.com/openai/triton">triton github</a></li>
  <li><a href="https://triton-lang.org/">triton documentation</a></li>
</ul>

<h2 id="gpu-programming-basics">
<a class="anchor" href="#gpu-programming-basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>GPU programming basics</h2>
<p>Starting with CPU (host). CPU has access to RAM, storage disks and all the connected peripherals. GPU (device) on the other hand has no access to RAM or anything. A GPU has its own memory which is called VRAM and data must be copied from CPU-&gt;GPU for GPU to work on it and data must again be copied from GPU-&gt;CPU for CPU to store it in one of the storage devices or share it with the connected peripherals.</p>

<blockquote>
  <p><strong>Note:</strong> This is the reason why you should minimize data movement between CPU and GPU as much as possible. To do this you have to brainstorm how you can load the data in chunks to process it in parallel or import the data in a way that it can be reused multiple times before importing next data item.</p>
</blockquote>

<p>In CUDA, we launch many <strong>threads</strong> in groups of thread <strong>blocks</strong> that form a <strong>grid</strong>. All <em>threads</em> in a thread <em>block</em> can communicate with each other. You can launch 1024 threads per block and $2^{32}-1$ blocks in a single launch. Figure 2, shows an example of this.</p>

<p><img src="/blog/images/post_0015/02.png" alt="" title="Figure 2: Architecture of CUDA programs."></p>

<p>The idea behind using <em>blocks</em> is that, you do not need to change your code if you get a new GPU in the future. So the new GPU, can simply execute more <em>blocks</em> concurrently without change of any code.</p>

<h2 id="cpu-program-vs-cuda-program">
<a class="anchor" href="#cpu-program-vs-cuda-program" aria-hidden="true"><span class="octicon octicon-link"></span></a>CPU-program vs CUDA-program</h2>
<p>Without going into technicalities let us consider a simple example of adding two arrays of length 3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="n">arr2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div>
<p>In C++, if we want to add these arrays then we will create a for-loop that will run three times (assuming single threaded program).</p>

<p>But in CUDA, we will launch 3 threads, each of which will do addition at a single index and the for-loop is done in a single step. In reality the following would take place</p>
<ol>
  <li>Copy <code class="language-plaintext highlighter-rouge">arr1</code>, <code class="language-plaintext highlighter-rouge">arr2</code> from CPU to GPU.</li>
  <li>Create a new array of size 3 (or store the result of addition in <code class="language-plaintext highlighter-rouge">arr1</code>).</li>
  <li>Launch 3 threads to do the addition and store the result in new array.</li>
  <li>Copy the result from GPU to CPU.</li>
</ol>

<p>Because GPUs of 1000s of cores, doing simple things like addition, matrix multiplication are much faster on the GPU than the CPU (provided the speedup is more than the time spent to transfer data between the CPU and GPU).</p>

<h2 id="cuda-vs-triton">
<a class="anchor" href="#cuda-vs-triton" aria-hidden="true"><span class="octicon octicon-link"></span></a>CUDA vs Triton</h2>
<p>We saw the CUDA execution model above. Now let’s see how Triton differs from the above model.</p>

<p>In CUDA each kernel, is associated with a <em>thread-block</em> (i.e. a collection of <em>threads</em>). In Triton, each kernel is associated with a single <em>thread</em>. This execution paradigm solves the problem of memory synchronization between threads, inter-thread communication while allowing automatic parallelization.</p>

<p>Now instead of storing threads inside a thread block, a block consists of a <em>range</em> i.e. tiles of pointers to threads. The interesting thing about this is you can have as many ranges as you want. So if your input is 2D you can specify Range(10) for x-axis and Range(5) on y-axis for a total of 50 threads. Similarly, you can define ranges along as many dimensions as you want.</p>

<p><img src="/blog/images/post_0015/03.png" alt="" title="Figure 3: CUDA execution model vs Triton execution model."></p>

<h2 id="adding-two-arrays-using-triton">
<a class="anchor" href="#adding-two-arrays-using-triton" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding two arrays using Triton</h2>
<p>Now we have a basic understanding of how CUDA and Triton works, we can writing Triton programs. Use the following command to install Triton</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install triton
</code></pre></div></div>
<p>A summary of steps is given below:</p>
<ol>
  <li>Define <em>block</em>. We know <em>blocks</em> are defined by specifying a range. So for addition we only need to define range along one-dimension. Let is be 512 and we define it as a global <code class="language-plaintext highlighter-rouge">BLOCK_SIZE = 512</code>.</li>
  <li>The range of 512 actually means we are launching 512 threads to do the computation.</li>
  <li>
    <p>Next we get the index of the input data. Suppose the input array is of size 1000. Because we defined a block size of 512, we will process the input array in chunks of size 512. So the first chunk would be from <code class="language-plaintext highlighter-rouge">0:512</code> and the second chunk from <code class="language-plaintext highlighter-rouge">512:1024</code>. This is done using the code shown below</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="c1"># Addition is 1D, so we only need to get the index of axis=0
</span> <span class="n">pid</span> <span class="o">=</span> <span class="n">triton</span><span class="p">.</span><span class="n">language</span><span class="p">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

 <span class="c1"># Below offsets are a list of pointers
</span> <span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>
 <span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">triton</span><span class="p">.</span><span class="n">language</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Masking to guard memory operations. In the above example, the input array is of size <code class="language-plaintext highlighter-rouge">N=1000</code> but the offset is from <code class="language-plaintext highlighter-rouge">512:1024</code>. So we need to specify a mask that will protect us against out-of-bounds accesses. This mask needs to be specified for each axis.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">N</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Load the data. Now that we have defined the offsets and the mask, we can load the data from the RAM and mask out all the extra elements.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">add_kernel</span><span class="p">(</span><span class="n">arr1_ptr</span><span class="p">,</span> <span class="n">arr2_ptr</span><span class="p">,</span> <span class="n">output_ptr</span><span class="p">,</span> <span class="p">...):</span>
     <span class="p">...</span>
     <span class="n">arr1</span> <span class="o">=</span> <span class="n">triton</span><span class="p">.</span><span class="n">language</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">arr1_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
     <span class="n">arr2</span> <span class="o">=</span> <span class="n">triton</span><span class="p">.</span><span class="n">language</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">arr2_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Do the relevant operation. In this case, we only need to do addition.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">output</span> <span class="o">=</span> <span class="n">arr1</span> <span class="o">+</span> <span class="n">arr2</span>
</code></pre></div>    </div>
  </li>
  <li>After doing the computation, we store the result to RAM.
    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">triton</span><span class="p">.</span><span class="n">language</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>The code for entire kernel is shown below</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">triton</span>
<span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="n">tl</span>

<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">512</span>

<span class="o">@</span><span class="n">triton</span><span class="p">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">add_kernel</span><span class="p">(</span><span class="n">arr1_ptr</span><span class="p">,</span> <span class="n">arr2_ptr</span><span class="p">,</span> <span class="n">output_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="c1"># Step 1: Get range of axis
</span>    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Step 2: Define the offsets and mask
</span>    <span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">N</span>

    <span class="c1"># Step 3: Load the data from RAM
</span>    <span class="n">arr1</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">arr1_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">arr2</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">arr2_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

    <span class="c1"># Step 4: Do the computation
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">arr1</span> <span class="o">+</span> <span class="n">arr2</span>

    <span class="c1"># Step 5: Store the result in RAM
</span>    <span class="n">tl</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div></div>

<p>To use the kernel, we can define a helper function as shown below</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">arr1</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">arr2</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">arr1</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="p">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">),)</span>

    <span class="n">add_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">grid</code> is basically specifying the space over which we will work. In our case, the grid is 1D and we specify how the data is splitted along the grid. So if the input arrays is of size then we want to have the grid as following `[0:512], [512:1024]. So in this step we are basically specifying how to split the input data and pass it to the kernel.</p>

<p>This was a simple example. But in case of complex examples like matrix multiplication, how you split the data can have a huge affect on the performance. Triton docs provide a <a href="https://triton-lang.org/getting-started/tutorials/03-matrix-multiplication.html">tutorial</a> on how to write an efficient matrix multiplication kernel which goes into much more details.</p>

<p>This was a short tutorial introducing GPU programming basics and getting you started with Triton. Check the Triton project on <a href="https://github.com/openai/triton">openai/github</a> if you want to learn more.</p>

<hr>

<p><a href="https://twitter.com/Kkushaj">twitter</a>, <a href="https://www.linkedin.com/in/kushaj/">linkedin</a>, <a href="https://github.com/KushajveerSingh">github</a></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="KushajveerSingh/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/writing-custom-cuda-kernels-with-triton" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Deep Learning Researcher with interest in Computer Vision and Graph Machine Learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/KushajveerSingh" title="KushajveerSingh"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Kkushaj" title="Kkushaj"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
