{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Study of Mish activation function in transfer learning with code and discussion\"\n",
    "> Resnet50 model is tested by using Mish in its FClayers. Also, a detailed discussion of the paper is done with code implementation.\n",
    "- toc: true\n",
    "- comments: true\n",
    "- author: Kushajveer Singh\n",
    "- categories: [paper-implementation]\n",
    "- image: images/preview/post_007.png\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [jupyter notebook](https://github.com/KushajveerSingh/deep_learning/tree/master/deep_learning/paper_implementations/Study%20of%20Mish%20activation%20function%20in%20transfer%20learning%20with%20code%20and%20discussion), [paper](https://arxiv.org/abs/1908.08681), [fastai discussion thread](https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu/53299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mish activation function is proposed in [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/abs/1908.08681) paper. The experiments conducted in the paper shows it achieves better accuracy than ReLU. Also, many experiments have been conducted by the fastai community and they were also able to achieve better results than ReLU.\n",
    "\n",
    "Mish is defined as `x * tanh(softplus(x))` or by this equation $x*\\tanh (\\ln (1+e^x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "class Mish(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mish activation function is proposed in \"Mish: A Self \n",
    "    Regularized Non-Monotonic Neural Activation Function\" \n",
    "    paper, https://arxiv.org/abs/1908.08681.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot mish function\n",
    "To build upon this activation function let’s first see the plot of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "x = np.linspace(-7, 7, 700)\n",
    "y = x * np.tanh(np.log(1 + np.exp(x)))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "plt.plot(x, y, 'b')\n",
    "\n",
    "plt.savefig(fname='/home/kushaj/Desktop/Temp/SOTA/images/mish_plot.png', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_007/01.jpeg \"Figure 1. Mish activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of mish\n",
    "1. **Unbounded Above**:- Being unbounded above is a desired property of an activation function as it avoids saturation which causes training to slow down to near-zero gradients.\n",
    "2. **Bounded Below**:- Being bounded below is desired because it results in strong regularization effects.\n",
    "3. **Non-monotonic**:- This is the important factor in mish. We preserve small negative gradients and this allows the network to learn better and it also improves the gradient flow in the negative region as, unlike ReLU where all negative gradients become zero.\n",
    "4. **Continuity**:- Mish’s first derivative is continuous over the entire domain which helps in effective optimization and generalization. Unlike ReLU which is discontinuous at zero.\n",
    "\n",
    "To compute the first derivative expand the `tanh(softplus(x))` term and you will get the following term and then do product rule of the derivative.\n",
    "\n",
    "$$y=x*\\frac{e^{2x}+2e^x}{e^{2x}+2e^x+2}$$\n",
    "\n",
    "When using Mish against ReLU use a lower learning rate in the case of Mish. Range of around 1e-5 to 1e-1 showed good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mish against ReLU\n",
    "Rather than training from scratch which is already done in the paper, I would test for transfer learning. When we use pretrained models for our own dataset we keep the CNN filter weights the same (we update them during finetuning) but we initialize the last fully-connected layers randomly (head of the model). So I would test for using ReLU and Mish in these fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: I would be using OneCycle training. In case you are unfamiliar with the training technique that I would use here, I have written a complete notebook summarizing them in fastai. You can check the notebook [here](https://github.com/KushajveerSingh/deep_learning/blob/master/deep_learning/paper_implementations/Leslie%20N.%20Smith%20papers%20notebook/main.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use CIFAR10 and CIFAR100 dataset to test a pretrained Resnet50 model. I would run the model for 10 epochs and then compare the results at the fifth and tenth epoch. Also, the results would be averaged across 3 runs using different learning rates (1e-2, 5e-3, 1e-3). The weighs of the CNN filters would not be updated, only the fully connected layers would be updated/trained.\n",
    "\n",
    "For the fully connected layers, I would use the following architecture. In case of Mish, replace the ReLU with Mish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# AdaptiveConcatPool2d is just combining AdaptiveAvgPool and AdaptiveMaxPool.\n",
    "head = nn.Sequential(\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=4096, out_features=512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=512, out_features=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results are shown below. It was observed that Mish required training with a smaller learning rate otherwise it overfits quickly, thus suggesting that it requires stronger regularization than ReLU. It was consistent across multiple runs. Generally, you can get away with using a higher learning rate in the case of ReLU but when using Mish a higher learning rate always lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://gist.github.com/KushajveerSingh/461424a6fb0c59d776d5ad2266737c5f.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the results are quite similar but by using Mish we can see some marginal improvements. This is a very limited test as only one Mish activation is used in the entire network and also the network has been run for only 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of output landscape\n",
    "We would use a 5 layer randomly initialized fully connected neural network to visualize the output landscape of ReLU and Mish. The code and the results are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "\n",
    "# The following code has been taken from \n",
    "# https://github.com/digantamisra98/Mish/blob/master/output_landscape.py\n",
    "def get_model(act_fn='relu'):\n",
    "    if act_fn is 'relu':\n",
    "        fn = nn.ReLU(inplace=True)\n",
    "    if act_fn is 'mish':\n",
    "        fn = Mish()\n",
    "        \n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, 64),\n",
    "        fn,\n",
    "        nn.Linear(64, 32),\n",
    "        fn,\n",
    "        nn.Linear(32, 16),\n",
    "        fn,\n",
    "        nn.Linear(16, 1),\n",
    "        fn\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main code\n",
    "relu_model = get_model('relu')\n",
    "mish_model = get_model('mish')\n",
    "\n",
    "x = np.linspace(0., 10., 100)\n",
    "y = np.linspace(0., 10., 100)\n",
    "\n",
    "grid = [torch.tensor([xi, yi]) for xi in x for yi in y]\n",
    "\n",
    "np_img_relu = np.array([relu_model(point).detach().numpy() for point in grid]).reshape(100, 100)\n",
    "np_img_mish = np.array([mish_model(point).detach().numpy() for point in grid]).reshape(100, 100)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "np_img_relu = scaler.fit_transform(np_img_relu)\n",
    "np_img_mish = scaler.fit_transform(np_img_mish)\n",
    "\n",
    "plt.imsave('relu_land.png', np_img_relu)\n",
    "plt.imsave('mish_land.png', np_img_mish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output landscapes, we can observe that the mish produces a smoother output landscape thus resulting is smoother loss functions which are easier to optimize and thus the network generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "25",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}