{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"SPADE: State of the art in Image-to-Image Translation by Nvidia\"\n",
    "> New state of the art method for generating colored images from segmentation masks. It uses a GAN to learn to produce photorealistic images.\n",
    "- toc: true\n",
    "- comments: true\n",
    "- author: Kushajveer Singh\n",
    "- categories: [paper-implementation]\n",
    "- image: images/preview/post_004.png\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [implementation code](https://github.com/KushajveerSingh/SPADE-PyTorch), [paper](https://arxiv.org/abs/1903.07291)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give motivation for this paper, see the demo released by Nvidia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/MXWm6w4E5q0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Semantic Image Synthesis?\n",
    "It is the opposite of image segmentation. Here we take a segmentation map (seg map)and our aim is to produce a colored picture for that segmentation map. In segmentation tasks, each color value in the seg map corresponds to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/01.jpeg \"Figure 1. [left] segmentation map. [right] corresponding colored image of the segmentation map. Image taken from CityScapes Dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New things in the paper\n",
    "SPADE paper introduces a new normalization technique called **spatially-adaptive normalization**. Earlier models used the seg map only at the input layer but as seg map was only available in one layer the information contained in the seg map washed away in the deeper layers. SPADE solves this problem. In SPADE, we give seg map as input to all the intermediate layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train the model?\n",
    "Before getting into the details of the model, I would discuss how models are trained for a task like Semantic Image Synthesis.\n",
    "\n",
    "The core idea behind the model training is a GAN. Why GAN is needed? Because whenever we want to generate something that looks photorealistic or more technically closer to the output images, we have to use GANs.\n",
    "\n",
    "So for GAN we need three things 1) Generator 2) Discriminator 3) Loss Function. For the Generator, we need to input some random values. Now you can either take random normal values. But if you want your output image to resemble some other image i.e. take the style of some image and add it your output image, you will also need an image encoder which would provide the mean and variance values for the random Gaussian distribution.\n",
    "\n",
    "For the loss function, we would use the loss function used in pix2pixHD paper with some modifications. Also, I would discuss this technique where we extract features from the VGG model and then compute loss function (perceptual loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPADE\n",
    "This is the basic block that we would use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/02.jpeg \"Figure 2. [left] shows the architecture of the model. [right] a 3D view of the model. Figure taken from paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to resize segmentation map?\n",
    "Every pixel value in your seg map corresponds to a class and you cannot introduce new pixel values. When we use the defaults in various libraries for resizing, we do some form of interpolation like linear, which can change up the pixel values and result in values that were not there before. To solve this problem, whenever you have to resize your segmentation map **use ‘nearest’ as the upsampling or downsampling method**.\n",
    "\n",
    "How we use it? Consider some layer in your model, you want to add the information from the segmentation map to the output of that layer. That will be done using SPADE.\n",
    "\n",
    "SPADE first resizes your seg map to match the size of the features and then we apply a conv layer to the resized seg map to extract the features. To normalize our feature map, we first normalize our feature map using BatchNorm and then denormalize using the values we get from the seg map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(Module):\n",
    "    def __init__(self, args, k):\n",
    "        super().__init__()\n",
    "        num_filters = args.spade_filter\n",
    "        kernel_size = args.spade_kernel\n",
    "        self.conv = spectral_norm(Conv2d(1, num_filters, kernel_size=(kernel_size, kernel_size), padding=1))\n",
    "        self.conv_gamma = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1))\n",
    "        self.conv_beta = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1))\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        N, C, H, W = x.size()\n",
    "\n",
    "        sum_channel = torch.sum(x.reshape(N, C, H*W), dim=-1)\n",
    "        mean = sum_channel / (N*H*W)\n",
    "        std = torch.sqrt((sum_channel**2 - mean**2) / (N*H*W))\n",
    "\n",
    "        mean = torch.unsqueeze(torch.unsqueeze(mean, -1), -1)\n",
    "        std = torch.unsqueeze(torch.unsqueeze(std, -1), -1)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        seg = F.interpolate(seg, size=(H,W), mode='nearest')\n",
    "        seg = relu(self.conv(seg))\n",
    "        seg_gamma = self.conv_gamma(seg)\n",
    "        seg_beta = self.conv_beta(seg)\n",
    "\n",
    "        x = torch.matmul(seg_gamma, x) + seg_beta\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPADERes Block\n",
    "Just like Resnet where we combine conv layers into a ResNet Block, we combine SPADE into a SPADEResBlk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/03.jpeg \"Figure 3. Architecture of SPADERes Block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is simple we are just extending the ResNet block. The skip-connection is important as it allows for training of deeper networks and we do not have to suffer from problems of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADEResBlk(Module):\n",
    "    def __init__(self, args, k, skip=False):\n",
    "        super().__init__()\n",
    "        kernel_size = args.spade_resblk_kernel\n",
    "        self.skip = skip\n",
    "        \n",
    "        if self.skip:\n",
    "            self.spade1 = SPADE(args, 2*k)\n",
    "            self.conv1 = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n",
    "            self.spade_skip = SPADE(args, 2*k)\n",
    "            self.conv_skip = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n",
    "        else:\n",
    "            self.spade1 = SPADE(args, k)\n",
    "            self.conv1 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n",
    "\n",
    "        self.spade2 = SPADE(args, k)\n",
    "        self.conv2 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x, seg):\n",
    "        x_skip = x\n",
    "        x = relu(self.spade1(x, seg))\n",
    "        x = self.conv1(x)\n",
    "        x = relu(self.spade2(x, seg))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.skip:\n",
    "            x_skip = relu(self.spade_skip(x_skip, seg))\n",
    "            x_skip = self.conv_skip(x_skip)\n",
    "        \n",
    "        return x_skip + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our basic blocks, we start coding up our GAN. Again, the three things that we need for GAN are:\n",
    "1. Generator\n",
    "2. Discriminator\n",
    "3. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/04.jpeg \"Figure 4. Architecture of SPADE Generator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADEGenerator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(args.gen_input_size, args.gen_hidden_size)\n",
    "        self.spade_resblk1 = SPADEResBlk(args, 1024)\n",
    "        self.spade_resblk2 = SPADEResBlk(args, 1024)\n",
    "        self.spade_resblk3 = SPADEResBlk(args, 1024)\n",
    "        self.spade_resblk4 = SPADEResBlk(args, 512)\n",
    "        self.spade_resblk5 = SPADEResBlk(args, 256)\n",
    "        self.spade_resblk6 = SPADEResBlk(args, 128)\n",
    "        self.spade_resblk7 = SPADEResBlk(args, 64)\n",
    "        self.conv = spectral_norm(Conv2d(64, 3, kernel_size=(3,3), padding=1))\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        b, c, h, w = seg.size()\n",
    "        x = self.linear(x)\n",
    "        x = x.view(b, -1, 4, 4)\n",
    "\n",
    "        x = interpolate(self.spade_resblk1(x, seg), size=(2*h, 2*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk2(x, seg), size=(4*h, 4*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk3(x, seg), size=(8*h, 8*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk4(x, seg), size=(16*h, 16*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk5(x, seg), size=(32*h, 32*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk6(x, seg), size=(64*h, 64*w), mode='nearest')\n",
    "        x = interpolate(self.spade_resblk7(x, seg), size=(128*h, 128*w), mode='nearest')\n",
    "        \n",
    "        x = tanh(self.conv(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/05.jpeg \"Figure 5. Architecture of SPADE Discriminator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model1(in_chan, out_chan):\n",
    "    return nn.Sequential(\n",
    "        spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=2, padding=1)),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def custom_model2(in_chan, out_chan, stride=2):\n",
    "    return nn.Sequential(\n",
    "        spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(4,4), stride=stride, padding=1)),\n",
    "        nn.InstanceNorm2d(out_chan),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class SPADEDiscriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.layer1 = custom_model1(4, 64)\n",
    "        self.layer2 = custom_model2(64, 128)\n",
    "        self.layer3 = custom_model2(128, 256)\n",
    "        self.layer4 = custom_model2(256, 512, stride=1)\n",
    "        self.inst_norm = nn.InstanceNorm2d(512)\n",
    "        self.conv = spectral_norm(nn.Conv2d(512, 1, kernel_size=(4,4), padding=1))\n",
    "\n",
    "    def forward(self, img, seg):\n",
    "        x = torch.cat((seg, img.detach()), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = leaky_relu(self.inst_norm(x))\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "The most important piece for training a GAN. We are all familiar with the loss function of minimizing the Generator and maximizing the discriminator, where the objective function looks something like this.\n",
    "\n",
    "$$\\mathbb{E}_{(\\boldsymbol{\\mathrm{s}},\\boldsymbol{\\mathrm{x}})}[\\log D(\\boldsymbol{\\mathrm{s}},\\boldsymbol{\\mathrm{x}})]+\\mathbb{E}_{\\boldsymbol{\\mathrm{s}}}[\\log (1-D(\\boldsymbol{\\mathrm{s}},G(\\boldsymbol{\\mathrm{s}})$$\n",
    "\n",
    "Now we extend this loss function to a feature matching loss. What do I mean? When we compute this loss function we are only computing the values on a fixed size of the image, but what if we compute the losses at different sizes of the image and then sum them all.\n",
    "\n",
    "This loss would stabilize training as the generator has to produce natural statistics at multiple scales. To do so, we extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized images. This is done by taking features out of a pretrained VGG model. This is called perceptual loss. The code makes it easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = VGG19().cuda()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we take the two images, real and synthesized and pass it through VGG network. We compare the intermediate feature maps to compute the loss. We can also use ResNet, but VGG works pretty good and earlier layers of VGG are generally good at extracting the features of an image.\n",
    "\n",
    "This is not the complete loss function. Below I show my implementation without the perceptual loss. I strongly recommend seeing the loss function implementation used by Nvidia themselves for this project as it combines the above loss also and it would also provide a general guideline on how to train GANs in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.FloatTensor):\n",
    "        super().__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.L1Loss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = torch.tensor(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):        \n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor.to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Init\n",
    "In the paper, they used Glorot Initialization (another name of Xavier initialization). I prefer to use He. Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder\n",
    "This is the final part of our model. It is used if you want to transfer style from one image to the output of SPADE. It works by outputting the mean and variance values from which we compute the random gaussian noise that we input to the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/06.jpeg \"Figure 6. Architecture of Encoder model used in SPADE,optional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_inst_lrelu(in_chan, out_chan):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=2, bias=False, padding=1),\n",
    "        nn.InstanceNorm2d(out_chan),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class SPADEEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.layer1 = conv_inst_lrelu(3, 64)\n",
    "        self.layer2 = conv_inst_lrelu(64, 128)\n",
    "        self.layer3 = conv_inst_lrelu(128, 256)\n",
    "        self.layer4 = conv_inst_lrelu(256, 512)\n",
    "        self.layer5 = conv_inst_lrelu(512, 512)\n",
    "        self.layer6 = conv_inst_lrelu(512, 512)\n",
    "        self.linear_mean = nn.Linear(8192, args.gen_input_size)\n",
    "        self.linear_var = nn.Linear(8192, args.gen_input_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return self.linear_mean(x), self.linear_var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spectral Normalization?\n",
    "[Spectral Normalization Explained](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html) by Christian Cosgrove. This article discusses spectral norm in detail with all the maths behind it. Ian Goodfellow even commented on spectral normalization and considers it to be an important tool.\n",
    "\n",
    "The reason we need spectral norm is that when we are generating images, it can become a problem to train our model to generate images of say 1000 categories on ImageNet. Spectral Norm helps by stabilizing the training of discriminator. There are theoretical justifications behind this, on why this should be done, but all that is beautifully explained in the above blog post that I linked to.\n",
    "\n",
    "To use spectral norm in your model, just apply `spectral_norm` to all convolutional layers in your generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Discussion on Instance normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_004/07.jpeg \"Figure 7.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization uses the complete batch to compute the mean and std and then normalizes the complete batch with a single value of mean and std. This is good when we are doing classification, but when we are generating images, we want to keep the normalization of these images independent.\n",
    "\n",
    "One simple reason for that is if in my batch one image is being generated for blue sky and in another image, generating a road then clearly normalizing these with the same mean and std would add extra noise to the images, which would make training worse. So instance norm is used instead of batch normalization here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* SPADE Paper (link)\n",
    "Official Implementation (link)\n",
    "My Implementation (link)\n",
    "pix2pixHD (link)\n",
    "Spectral Normalization paper (link)\n",
    "Spectral Norm blog (link)\n",
    "Instance Normalization paper (link)\n",
    "Instance norm other resources, blog, stack overflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "25",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
