{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch\"\n",
    "> Overfitting small batch, manually checking loss. Creating data pipelines. I give a complete and detailed introduction on how to create AlexNet model in PyTorch with code.\n",
    "- toc: true\n",
    "- comments: true\n",
    "- author: Kushajveer Singh\n",
    "- categories: [tutorial]\n",
    "- image: images/preview/post_002.png\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [jupyter notebook](https://github.com/KushajveerSingh/deep_learning/blob/master/deep_learning/paper_implementations/Training%20AlexNet%20with%20tips%20and%20checks%20on%20how%20to%20train%20CNNs/training_alexnet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "To train CNNs we want data. The options available to you are MNIST, CIFAR, Imagenet with these being the most common. You can use any dataset. I use Imagenet as it requires some preprocessing to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:- I use the validation data provided by Imagenet i.e. 50000 images as my train data and take 10 images from each class from the train dataset as my val dataset(script to do so in my jupyter notebook). The choice of the dataset is up to you. Below is the processing that you have to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download Imagenet. You can refer to the Imagenet site to download the data. If you have limited internet, then this option is good, as you can download fewer images. Or use [ImageNet Object Localization Challenge](https://www.kaggle.com/c/imagenet-object-localization-challenge) to directly download all the files (warning 155GB).\n",
    "2. Unzip the tar.gz file using `tar xzvf file_name -C destination_path`.\n",
    "3. In the *Data/CLS-LOC* folder you have the train, val and test images folders. The train images are already in their class folders i.e. the images of dogs are in a folder called dog and images of cats are in cat folder. But the val images are not classified in their class folders.\n",
    "4. Use this command from your terminal in the val folder `wget -qO- [https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) | bash`. It would move all the images to their respective class folders.\n",
    "5. As a general preprocessing step, we rescale all images to 256x??? on thir shorter side. As this operation repeats everytime I store the rescaled version of the images on disk. Using `find . -name “*.JPEG” | xargs -I {} convert {} -resize “256^>” {}`.\n",
    "\n",
    "After doing the above steps you would have your folder with all the images in their class folders and the dimension of all images would be 256x???."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_002/01.jpeg \"Figure 1. File structure for my data folder. Each folder contains subfolder like ‘n01440764’ and the images for that class are placed in that folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Steps involved:-\n",
    "1. Create a dataset class or use a predefined class\n",
    "2. Choose what transforms you want to perform on the data.\n",
    "3. Create data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://gist.github.com/KushajveerSingh/7705c90dded96c8993306311e7d8dc40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "train_dir = '../../../Data/ILSVRC2012/train'\n",
    "val_dir = '../../../Data/ILSVRC2012/val'\n",
    "size = 224\n",
    "batch_size = 32\n",
    "num_workers = 8 \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "image_datasets = {\n",
    "    'train': ImageFolder(train_dir,   transform=data_transforms['train']),\n",
    "    'val': ImageFolder(val_dir, transform=data_transforms['val']),\n",
    "}\n",
    "data_loader = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=num_workers) for x in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization values are precalculated for the Imagenet dataset so we use those values for normalization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataloaders\n",
    "After creating the input data pipeline, you should do a sanity check to see everything is working as expected. Plot some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# As our images are normalized we have to denormalize them and \n",
    "# convert them to numpy arrays.\n",
    "def imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) #Pause is necessary to display images correctly\n",
    "    \n",
    "images, labels = next(iter(data_loader['train']))\n",
    "grid_img = make_grid(images[:4], nrow=4)\n",
    "imshow(grid_img, title = [labels_list[x] for x in labels[:4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that you will face with Imagenet data is with getting the class names. The class names are contained in the file **LOC_synset_mapping.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "f = open(\"../../Data/LOC_synset_mapping.txt\", \"r\")\n",
    "labels_dict = {} # Get class label by folder name\n",
    "labels_list = [] # Get class label by indexing\n",
    "for line in f:\n",
    "    split = line.split(' ', maxsplit=1)\n",
    "    split[1] = split[1][:-1]\n",
    "    label_id, label = split[0], split[1]\n",
    "    labels_dict[label_id] = label  \n",
    "    labels_list.append(split[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "Create your model. Pretrained models covered at the end of the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Changes from the original AlexNet. BatchNorm is used instead of ‘brightness normalization’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv_base = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.fc_base = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = x.view(x.size(0), 256*6*6)\n",
    "        x = self.fc_base(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the division of the conv_base and fc_base in the model. This is a general scheme that you would see in most implementations i.e. dividing the model into smaller models. We use 0-indexing to access the layers for now, but in future posts, I would use names for layers (as it would help for weight initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices for CNN\n",
    "1. **Activation function**:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always.\n",
    "2. **Weight Initialization**:- Use He initialization as default with ReLU. PyTorch provides kaiming_normal_ for this purpose. \n",
    "3. **Preprocess data**:- There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other.\n",
    "4. **Batch Normalization**:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. *Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don’t need batchnorm layers in your model.*\n",
    "5. **Pooling layers**:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is good.\n",
    "6. **Optimizer**:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new optimizer AdamW. Choice of optimizer comes to experimentation and the task at hand. Look at benchmarks using different optimizers as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "Do not use this method as a default. After, naming the layers you can do this very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "conv_list = [0, 4, 8, 10, 12]\n",
    "fc_list = [1, 4, 6]\n",
    "for i in conv_list:\n",
    "    torch.nn.init.kaiming_normal_(model.conv_base[i].weight)\n",
    "for i in fc_list:\n",
    "    torch.nn.init.kaiming_normal_(model.fc_base[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizers, schedulers and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Cross entropy loss takes the logits directly, so we don't need to apply softmax in our CNN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch specific discussion\n",
    "* You have to specify the padding yourself. Check this thread for discussion on this topic.\n",
    "* Create the optimizer after moving the model to GPU.\n",
    "* The decision to add softmax layer in your model depends on your loss function. In case of CrossEntropyLoss, we do not need to add softmax layer in our model as that is handled by loss function itself.\n",
    "* Do not forget to zero the grads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to check my model is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 1\n",
    "The first technique is to overfit a mini-batch. If the model is not able to overfit small mini-batch then your model lacks the power to generalize over the dataset. Below I overfit 32-batch input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "inputs, labels = next(iter(data_loader['train']))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "criterion_check1 = nn.CrossEntropyLoss()\n",
    "optimizer_check1 = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer_check1.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion_check1(outputs, labels)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    loss.backward()\n",
    "    optimizer_check1.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch {}: Loss = {} Accuracy = {}'.format(epoch+1, loss.item(), torch.sum(preds == labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_002/02.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: Turn off regularization like Dropout, BatchNorm although results don’t vary much in other case. Don’t use L2 regularization i.e. make weight_decay=0 in optimizer. Remember to reinitialize your weights again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 2\n",
    "Double check loss value. If you are doing a binary classification and are getting a loss of 2.3 on the first iter then it is ok, but if you are getting a loss of 100 then there are some problems.\n",
    "\n",
    "In the above figure, you can see we got a loss value of 10.85 which is ok considering the fact we have 1000 classes. In case you get weird loss values try checking for negative signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Models\n",
    "As we are using AlexNet, we download AlexNet from torchvision.models and try to fit it on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Just doing for fun. Rescaling images from 32x32 to 224x224 is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "data = np.load('../../../Data/cifar_10.npz')\n",
    "alexnet = torch.load('../../../Data/Pytorch Trained Models/alexnet.pth')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x_train = torch.from_numpy(data['train_data'])\n",
    "y_train = torch.from_numpy(data['train_labels'])\n",
    "x_test = torch.from_numpy(data['test_data'])\n",
    "y_test = torch.from_numpy(data['test_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# Create data loader\n",
    "class CIFAR_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Generally you would not load images in the __init__ as it forces the images\n",
    "    to load into memory. Instead you should load the images in getitem function,\n",
    "    but as CIFAR is small dataset I load all the images in memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image =  self.x[idx]\n",
    "        label = self.y[idx].item()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)\n",
    "            \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "datasets = {\n",
    "    'train': CIFAR_Dataset(x_train, y_train, transform=data_transforms),\n",
    "    'test': CIFAR_Dataset(x_test, y_test, transform=data_transforms)\n",
    "}\n",
    "\n",
    "data_loader = {\n",
    "    x: torch.utils.data.DataLoader(datasets[x],\n",
    "                                   batch_size=64,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=8) for x in ['train', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# Freeze conv layers\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Initialize the last layer of alexnet model for out 10 class dataset\n",
    "alexnet.classifier[6] = nn.Linear(4096, 10)\n",
    "alexnet = alexnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create list of params to learn\n",
    "params_to_learn = []\n",
    "for name,param in alexnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_learn.append(param)\n",
    "        \n",
    "optimizer = optim.SGD(params_to_learn, lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this [script](https://github.com/KushajveerSingh/Deep-Learning-Notebooks/blob/master/Extra/Scrips/cifar10_data_script.py) on how I processed CIFAR data after downloading from the official site. You can also download CIFAR from [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar).\n",
    "\n",
    "PyTorch has a very good [tutorial](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) on fine-tuning torchvision models. I give a short implementation with the rest of the code being in the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We discussed how to create dataloaders, plot images to check data loaders are correct. Then we implemented AlexNet in PyTorch and then discussed some important choices while working with CNNs like activations functions, pooling functions, weight initialization (code for He. initialization was also shared). Some checks like overfitting small dataset and manually checking the loss function were then discussed. We concluded by using a pre-trained AlenNet to classify CIFAR-10 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. <https://github.com/soumith/imagenet-multiGPU.torch> Helped in preprocessing of the Imagenet dataset.\n",
    "2. <https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html> Many code references were taken from this tutorial.\n",
    "3. <https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf> AlexNet paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "25",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
