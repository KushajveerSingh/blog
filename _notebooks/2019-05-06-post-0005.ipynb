{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"All you need for Photorealistic Style Transfer in PyTorch\"\n",
    "> A quick summary of probabilistic math used in machine learning.\n",
    "- toc: true\n",
    "- comments: true\n",
    "- author: Kushajveer Singh\n",
    "- categories: [paper-implementation]\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [jupyter notebook](https://github.com/KushajveerSingh/Photorealistic-Style-Transfer/tree/master), [paper](https://arxiv.org/abs/1904.11617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is style transfer?\n",
    "We have two images as input one is content image and the other is style image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/01.jpeg \"Figure 1. [left] content image, [right] style image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to transfer the style from style image to the content image. This looks something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/02.jpeg \"Figure 2. Output image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why another paper?\n",
    "Earlier work on style transfer although successful was not able to maintain the structure of the content image. For instance, see Fig2 and then see the original content image in Fig1. As you can see the curves and structure of the content image are not distorted and the output image has the same structure as content image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/03.jpeg \"Figure 3. The results from this paper are shown in [e] and [j] and  a comparison is done with other methods as well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram Matrix\n",
    "The main idea behind the paper is using Gram Matrix for style transfer. It was shown in these 2 papers that Gram Matrix in feature map of convolutional neural network (CNN) can represent the style of an image and propose the neural style transfer algorithm for image stylization.\n",
    "1. [Texture Synthesis Using Convolution Neural Networks](https://arxiv.org/abs/1505.07376) by Gatys et al. 2015\n",
    "2. [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) by Gatys et al. 2016\n",
    "\n",
    "Details about gram matrix can be found on wikipedia. Mathematically, given a vector V gram matrix is computed as\n",
    "$$G=V^TV$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Resolution Models\n",
    "It is a recent research paper accepted at CVPR 2019 paper. So generally what happens in CNNs is we first decrease the image size while increasing the number of filters and then increase the size of the image back to the original size.\n",
    "\n",
    "Now this forces our model to generate output images from a very small resolution and this results in loss of finer details and structure. To counter this fact High-Res model was introduced.\n",
    "\n",
    "High-resolution network is designed to maintain high-resolution representations through the whole process and continuously receive information from low-resolution networks. So we train our models on the original resolution.\n",
    "\n",
    "Example of this model would be covered below. You can refer to the original [papers](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch) for more details on this. I will cover this topic in detail in my next week blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style transfer details\n",
    "The general architecture of modern deep learning style transfer algorithms looks something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/04.jpeg \"Figure 4. Model architecture for style transfer in the deep learning era.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three things that style transfer model needs\n",
    "1. **Generating model**:- It would generate the output images. In Fig4 this is ‘Hi-Res Generation Network’\n",
    "2. **Loss function**:- Correct choice of loss functions is very important in case you want to achieve good results.\n",
    "3. **Loss Network**:- You need a CNN model that is pretrained and can extract good features from the images. In our case, it is VGG19 pretrained on ImageNet.\n",
    "\n",
    "So we load VGG model. The complete code is available at my [GitHub repo](https://github.com/KushajveerSingh/Photorealistic-Style-Transfer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise Exception('GPU is not available')\n",
    "    \n",
    "# Load VGG19 features. We do not need the last linear layers,\n",
    "# only CNN layers are needed\n",
    "vgg = vgg19(pretrained=True).features\n",
    "vgg = vgg.to(device)\n",
    "# We don't want to train VGG\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load our images from disk. My images are stored as *src/imgs/content.png and src/imgs/style.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "content_img = load_image(os.path.join(args.img_root, args.content_img), size=500)\n",
    "content_img = content_img.to(device)\n",
    "\n",
    "style_img = load_image(os.path.join(args.img_root, args.style_img))\n",
    "style_img = style_img.to(device)\n",
    "\n",
    "# Show content and style image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(im_convert(content_img))\n",
    "ax2.imshow(im_convert(style_img))\n",
    "plt.show()\n",
    "\n",
    "# Utility functions\n",
    "def im_convert(img):\n",
    "    \"\"\"\n",
    "    Convert img from pytorch tensor to numpy array, so we can plot it.\n",
    "    It follows the standard method of denormalizing the img and clipping\n",
    "    the outputs\n",
    "    \n",
    "    Input:\n",
    "        img :- (batch, channel, height, width)\n",
    "    Output:\n",
    "        img :- (height, width, channel)\n",
    "    \"\"\"\n",
    "    img = img.to('cpu').clone().detach()\n",
    "    img = img.numpy().squeeze(0)\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    img = img.clip(0, 1)\n",
    "    return img\n",
    "\n",
    "def load_image(path, size=None):\n",
    "    \"\"\"\n",
    "    Resize img to size, size should be int and also normalize the\n",
    "    image using imagenet_stats\n",
    "    \"\"\"\n",
    "    img = Image.open(path)\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detail**:- When we load our images, what sizes should we use? Your content image size should be divisible by 4, as our model would downsample images 2 times. For style images, do not resize them. Use their original resolution. Size of content image is (500x500x3) and size of style image is (800x800x3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hi-Res Generation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/05.jpeg \"Figure 5. The structure of high-resolution generation network. When we fuse feature maps with different resolution, we directly concatenate these feature images like the inception module, for example, the feature map 4, is concatenated by the feature map 2 and the feature map 3. We use bottleneck residual to ensure that our network can be trained well and speedup the training while preserving good visual effects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is quite simple we start with 500x500x3 images and maintain this resolution for the complete model. We downsample to 250x250 and 125x125 and then fuse these back together with 500x500 images.\n",
    "\n",
    "**Details**:-\n",
    "1. No pooling is used (as pooling causes loss of information). Instead strided convolution (i.e. stride=2) are used.\n",
    "2. No dropout is used. But if you need regularization you can use weight decay.\n",
    "3. 3x3 conv kernels are used everywhere with padding=1.\n",
    "4. Zero padding is only used. Reflex padding was tested but the results were not good.\n",
    "5. For upsampling,’bilinear’ mode is used.\n",
    "6. For downsampling, conv layers are used.\n",
    "7. InstanceNorm is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# Downsampling function\n",
    "def conv_down(in_c, out_c, stride=2):\n",
    "    return nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1)\n",
    "\n",
    "# Upsampling function\n",
    "def upsample(input, scale_factor):\n",
    "    return F.interpolate(input=input, scale_factor=scale_factor, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation code\n",
    "Residual connections are used between every block. We use BottleNeck layer from the ResNet architecture. (In Fig5 all the horizontal arrows are bottleneck layers).\n",
    "\n",
    "Refresher on bottleneck layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/06.jpeg \"Figure 6. Architecture of BottleneckModule.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "# Helper class for BottleneckBlock\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        # We have to keep the size of images same, so choose padding accordingly\n",
    "        num_pad = int(np.floor(kernel_size / 2))\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=num_pad)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used\n",
    "    instead of BatchNorm because when we want to generate images, we normalize\n",
    "    all the images independently. \n",
    "    \n",
    "    (In batch norm you compute mean and std over complete batch, while in instance \n",
    "    norm you compute mean and std for each image channel independently). The reason for \n",
    "    doing this is, the generated images are independent of each other, so we should\n",
    "    not normalize them using a common statistic.\n",
    "    \n",
    "    If you confused about the bottleneck architecture refer to the official pytorch\n",
    "    resnet implementation and paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        self.in_c = in_channels\n",
    "        self.out_c = out_channels\n",
    "        \n",
    "        self.identity_block = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels//4, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels//4),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels//4, out_channels//4, kernel_size, stride=stride),\n",
    "            nn.InstanceNorm2d(out_channels//4),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels//4, out_channels, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels, 1, stride),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.identity_block(x)\n",
    "        if self.in_c == self.out_c:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = self.shortcut(x)\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our style_transfer model, which we call HRNet (based on the paper). Use the Fig5 as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "class HRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf.\n",
    "    \n",
    "    Naming convention used.\n",
    "    I refer to vertical layers as a single layer, so from left to right we have 8 layers\n",
    "    excluding the input image.\n",
    "    E.g. layer 1 contains the 500x500x16 block\n",
    "         layer 2 contains 500x500x32 and 250x250x32 blocks and so on\n",
    "    \n",
    "    self.layer{x}_{y}:\n",
    "        x :- the layer number, as explained above\n",
    "        y :- the index number for that function starting from 1. So if layer 3 has two\n",
    "             downsample functions I write them as `downsample3_1`, `downsample3_2`\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1_1 = BottleneckBlock(3, 16)\n",
    "        \n",
    "        self.layer2_1 = BottleneckBlock(16, 32)\n",
    "        self.downsample2_1 = conv_down(16, 32)\n",
    "        \n",
    "        self.layer3_1 = BottleneckBlock(32, 32)\n",
    "        self.layer3_2 = BottleneckBlock(32, 32)\n",
    "        self.downsample3_1 = conv_down(32, 32)\n",
    "        self.downsample3_2 = conv_down(32, 32, stride=4)\n",
    "        self.downsample3_3 = conv_down(32, 32)\n",
    "        \n",
    "        self.layer4_1 = BottleneckBlock(64, 64)\n",
    "        self.layer5_1 = BottleneckBlock(192, 64)\n",
    "        self.layer6_1 = BottleneckBlock(64, 32)\n",
    "        self.layer7_1 = BottleneckBlock(32, 16)\n",
    "        self.layer8_1 = conv_down(16, 3, stride=1) # Needed conv layer so reused conv_down function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        map1_1 = self.layer1_1(x)\n",
    "        \n",
    "        map2_1 = self.layer2_1(map1_1)\n",
    "        map2_2 = self.downsample2_1(map1_1)\n",
    "        \n",
    "        map3_1 = torch.cat((self.layer3_1(map2_1), upsample(map2_2, 2)), 1)\n",
    "        map3_2 = torch.cat((self.downsample3_1(map2_1), self.layer3_2(map2_2)), 1)\n",
    "        map3_3 = torch.cat((self.downsample3_2(map2_1), self.downsample3_3(map2_2)), 1)\n",
    "        \n",
    "        map4_1 = torch.cat((self.layer4_1(map3_1), upsample(map3_2, 2), upsample(map3_3, 4)), 1)\n",
    "        \n",
    "        out = self.layer5_1(map4_1)\n",
    "        out = self.layer6_1(out)\n",
    "        out = self.layer7_1(out)\n",
    "        out = self.layer8_1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "In style transfer we use feature extraction, to calculate the value of losses. Feature extraction put in simple terms, means you take a pretrained imagenet model and pass your images through it and store the intermediate layer outputs. Generally, VGG model is used for such tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/post_005/07.jpeg \"Figure 7. Architecture of VGG model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you take the outputs from the conv layers. Like for the above fig, you can take the output from the second 3x3 conv 64 layer and then 3x3 conv 128.\n",
    "\n",
    "To extract features from VGG we use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "def get_features(img, model, layers=None):\n",
    "    \"\"\"\n",
    "    Use VGG19 to extract features from the intermediate layers.\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        layers = {\n",
    "            '0' : 'conv1_1',  # style layer\n",
    "            '5' : 'conv2_1',  # style layer\n",
    "            '10': 'conv3_1',  # style layer\n",
    "            '19': 'conv4_1',  # style layer\n",
    "            '28': 'conv5_1',  # style layer\n",
    "            \n",
    "            '21': 'conv4_2'   # content layer\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    x = img\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 5 layers in total for feature extraction. Only conv4_2 is used as layer for content loss.\n",
    "\n",
    "Refer to Fig4, we pass our output image from HRNet and the original content and style image through VGG.\n",
    "\n",
    "There are two losses\n",
    "1. Content Loss\n",
    "2. Style Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss \n",
    "Content image and the output image should have a similar feature representation as computed by loss network VGG. Because we are only changing the style without any changes to the structure of the image. For the content loss, we use Euclidean distance as shown by the formula\n",
    "\n",
    "$$l_{content}^{\\phi,j}(y,\\hat{y})=\\frac{1}{C_jJ_jW_j}\\left\\|\\phi_j(\\hat{y}=\\phi_j(y)\\right\\|^2$$\n",
    "\n",
    "$\\phi_j$ means we are referring to the activations of the j-th layer of loss network. In code it looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "style_net = HRNet().to(device)\n",
    "\n",
    "target = style_net(content_img).to(device)\n",
    "target.requires_grad_(True)\n",
    "\n",
    "target_features = get_features(target, vgg)\n",
    "content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss\n",
    "We use gram matrix for this. So style of an image is given by its gram matrix. Our aim is to make style of two images close, so we compute the difference of gram matrix of style image and output image and then take their Frobenius norm.\n",
    "\n",
    "$$l_{style}^{\\phi,j}(y,\\hat{y})=\\left\\|G_j^{\\phi}(y)-G_j^{\\phi}(\\hat{y})\\right\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "def get_gram_matrix(img):\n",
    "    \"\"\"\n",
    "    Compute the gram matrix by converting to 2D tensor and doing dot product\n",
    "    img: (batch, channel, height, width)\n",
    "    \"\"\"\n",
    "    b, c, h, w = img.size()\n",
    "    img = img.view(b*c, h*w)\n",
    "    gram = torch.mm(img, img.t())\n",
    "    return gram\n",
    "    \n",
    "\n",
    "# There are 5 layers, and we compute style loss for each layer and sum them up\n",
    "style_loss = 0\n",
    "for layer in layers:\n",
    "    target_gram_matrix = get_gram_matrix(target_feature)\n",
    "    # we already computed gram matrix for our style image\n",
    "    style_gram_matrix = style_gram_matrixs[layer]\n",
    "\n",
    "    layer_style_loss = style_weights[layer] * torch.mean((target_gram_matrix - style_gram_matrix) ** 2)\n",
    "    b, c, h, w = target_feature.shape\n",
    "    style_loss += layer_style_loss / (c*h*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult part\n",
    "To compute our final losses, we multiply them with some weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss = content_weight * content_loss  \n",
    "style_loss = style_weight * style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty comes in setting these values. If you want some desired output, then you would have to test different values before you get your desired result.\n",
    "\n",
    "To build your own intuitions you can choose two images and try different range of values. I am working on providing like a summary of this. It will be available in my repo README.\n",
    "\n",
    "Paper recommends content_weight = [50, 100] and style_weight = [1, 10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Well, congratulation made it to the end. You can now implement style transfer. Now read the paper for more details on style transfer.\n",
    "\n",
    "Check out my repo [README](https://github.com/KushajveerSingh/Photorealistic-Style-Transfer), it will contain the complete instructions on how to use the code in the repo, along with complete steps on how to train your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "25",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
